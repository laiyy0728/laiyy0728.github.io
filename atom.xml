<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Laiyy 的个人小站</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.laiyy.top/"/>
  <updated>2019-12-16T07:33:32.000Z</updated>
  <id>https://www.laiyy.top/</id>
  
  <author>
    <name>Laiyy</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>hadoop（18） YARN &lt;BR /&gt; 资源调度器、Hadoop 优化</title>
    <link href="https://www.laiyy.top/hadoop/yarn/hadoop-18.html"/>
    <id>https://www.laiyy.top/hadoop/yarn/hadoop-18.html</id>
    <published>2019-12-16T07:33:32.000Z</published>
    <updated>2019-12-16T07:33:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>Yarn 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的 <code>操作系统</code>，而 MapReduce 等运算程序相当于 <code>操作系统上的应用程序</code></p><a id="more"></a><h1 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h1><p>Yarn 由 <code>ResourceManager</code>、<code>NodeManager</code>、<code>ApplicationMaster</code>、<code>Container</code> 等组件构成。</p><blockquote><p>ResourceManager</p></blockquote><ol><li>处理客户端请求</li><li>监控 NodeManager</li><li>启动或监控 ApplicationMaster</li><li>资源的分配和调度</li></ol><blockquote><p>NodeManager</p></blockquote><ol><li>管理单个节点上的资源</li><li>处理来自 ResourceManager 的命令</li><li>处理来自 ApplicationMaster 的命令</li></ol><blockquote><p>ApplicationMaster</p></blockquote><ol><li>负责数据的切分</li><li>为应用程序申请资源，并分配给内部的任务</li><li>任务的监控、容错</li></ol><blockquote><p>Container</p></blockquote><p>是 YARN 资源的抽象，封装了某个节点上的多维度资源，如：内存、CPU、磁盘、网络等。</p><hr><h1 id="工作机制"><a href="#工作机制" class="headerlink" title="工作机制"></a>工作机制</h1><ol><li>MapReduce 程序提交任务到客户端所在的节点</li><li>申请一个 Application 到 ResourceManager</li><li>ResourceManager 返回 <code>资源提交路径</code> 和 <code>application_id</code></li><li>将任务所需资源提交到 <code>资源提交路径</code> 上，包括切片信息(Job.Split)，任务信息（Job.xml）、所需 jar 包</li><li>资源提交完成后，申请运行 MrAppMaster</li><li>ResourceManager 将用户的请求初始化为一个 Task，进入任务队列</li><li>NodeManager 到 ResourceManager 领取 Task 任务</li><li>NodeManager 创建 Container 容器，分配 CPU、内存等资源，启动对应的 MrAppMaster</li><li>NodeManager 下载 job 资源到本地，MrAppMaster 读取切片信息，决定开启多少个 MapTask</li><li>Container 向 ResourceManager 申请运行 MapTask 容器</li><li>其余 NodeManager 重复 7-10 步骤，等待任务领取完成</li><li>任务领取完成后，MrAppMaster 统一发送程序运行脚本，启动 MapTask</li><li>所有 MapTask 执行完成后，由 MrAppMaster 向 ResourceManager 申请对应切片数量的 ReduceTask，进行 reduce 工作</li><li>ReduceTask 从 MapTask 中获取相应的数据</li><li>ReduceTask 执行完成后，MrAppMaster 想 ResourceManager 申请注销自己</li></ol><p><img src="/images/hadoop/yarn/yarn.png" alt="yarn"></p><hr><h1 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h1><p>资源调度器对应上图中的 <strong><em>FIFO调度队列</em></strong>。</p><p>Hadoop 资源调度器主要有三种：<code>FIFO(队列)</code>、<code>Capacity Scheduler(容量调度器)</code>、<code>Fair Scheduler(公平调度器)</code>， 默认资源调度器为 <code>Capacity Scheduler</code></p><p><img src="/images/hadoop/yarn/yarn-scheduler-class.png" alt="默认资源调度器"></p><h2 id="FIFO"><a href="#FIFO" class="headerlink" title="FIFO"></a>FIFO</h2><p>按照到达时间，先到先服务；单项执行</p><p>当有新的服务器节点资源时，从队列中获取一个任务，从任务重分配一个 Task 给节点进行服务</p><p><img src="/images/hadoop/yarn/fifo.png" alt="FIFO"></p><h2 id="Capacity-Scheduler"><a href="#Capacity-Scheduler" class="headerlink" title="Capacity Scheduler"></a>Capacity Scheduler</h2><p>Hadoop 默认调度器，按照到达时间，先到先服务；并发执行</p><blockquote><p>支持多个队列，每个队列可配置一定的资源量，每个队列采用 FIFO 调度策略<br>为防止同一个用户的作业独占队列中的资源，调度器会对同一个用户提交的作业所占资源量进行限制。<br>如果调度器中有三个队列，可以从三个队列的头部取出三个任务并发执行，相比 FIFO 提高了任务的执行速度。</p></blockquote><p>计算方式：<br>首先，计算每个队列中正在执行的任务数与其应该分得的资源之间的比值，选择一个最小（最闲）的队列；<br>其次，按照作业优先级和提交时间顺序，同时考虑用户资源量限制和内存限制对队列内的任务进项排序</p><h2 id="Fair-Scheduler"><a href="#Fair-Scheduler" class="headerlink" title="Fair Scheduler"></a>Fair Scheduler</h2><p>按照缺额排序，缺额越大越优先；并发度最高</p><blockquote><p>支持多队列、多用户<br>每个队列中的资源量可以配置<br>同一个队列中的作业公平共享队列的所有资源</p></blockquote><p>分配方式：<br>假设有三个队列：QA、QB、QC，每个队列中的任务按照优先级分配资源，优先级越高分配的资源越多。但是每个任务都会分配到资源，以确保 <code>公平</code>。<br>在资源有限的情况下，每个任务理想情况下获得的资源与实际获得的资源可能存在一定的差距，这个差距就称为 <code>缺额</code>。<br>通一个队列中，任务的资源缺额越大，越先获得资源优先执行。作业是按照缺额的高低来先后执行的，且多个作业同时运行。</p><h1 id="任务的推测执行"><a href="#任务的推测执行" class="headerlink" title="任务的推测执行"></a>任务的推测执行</h1><p><strong><em>作业完成时间取决于最慢任务的完成时间</em></strong></p><p>一个作业由若干个 Map 任务和 Reduce 任务构成，因硬件老化、软件 bug 等，某些任务可能运行的非常慢（如：99% 的Map 都完成了，少数的 Mpa 进度很慢完不成）</p><p>解决方案：</p><p>为慢的任务启动一个 <code>备份任务</code>，同时运行，谁先运行完就采用谁的结果。</p><blockquote><p>执行推测任务的前台条件</p></blockquote><ol><li>每个 Task 只能有一个备份任务</li><li>当前 Job 已完成的 Task 必须小于 5%</li><li>在 mapred-site.xml 中开启推测执行（默认是打开的）</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>不能启动推测任务的情况</p></blockquote><ol><li>任务间存在严重的负载倾斜</li><li>特殊任务（如向数据库中写数据）</li></ol><h2 id="推测方法"><a href="#推测方法" class="headerlink" title="推测方法"></a>推测方法</h2><p>假设某一时刻，任务 T 的执行进度为 progress，则可通过一定的算法来推测出该任务最终完成的时刻 <code>endTime</code>；另一方面，如果此刻为该任务开启一个备份任务，则可以推断出备份任务可能的完成时刻 <code>endTime1</code>。则：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">runTime = (currentTimestamp - taskStartTime) / progress</span><br><span class="line">推测运行时间 = (当前时刻 - 任务启动时刻) / 任务运行比例</span><br><span class="line"></span><br><span class="line">endTime = runTIme + taskStartTime</span><br><span class="line">推测结束时刻 = 运行时间 + 任务启动时刻</span><br><span class="line"></span><br><span class="line">entTime1 = currentTimestamp + avgRunTime</span><br><span class="line">备份任务推测完成时刻 = 当前时刻 + 运行完成任务的平均时间</span><br></pre></td></tr></table></figure><blockquote><p>MR 总是选择 entTime - endTime1 差值最大的任务，并为之启动备份任务<br>为防止大量任务同时启动备份任务造成资源浪费，MR 为每个作业设置了同时启动备份任务数目的上限<br>推测执行机制实际上采用了经典的优化方案：<code>以空间换时间</code>。同时启动多个相同的任务处理相同数据，并让这些任务竞争，以缩短数据处理时间。显然这种方法占用更多的计算资源。在集群资源紧缺的情况下，应合理使用，争取在多用少量资源的情况下，减少作业的计算时间</p></blockquote><hr><h1 id="Hadoop-优化"><a href="#Hadoop-优化" class="headerlink" title="Hadoop 优化"></a>Hadoop 优化</h1><h2 id="MapReduce-速度慢的原因"><a href="#MapReduce-速度慢的原因" class="headerlink" title="MapReduce 速度慢的原因"></a>MapReduce 速度慢的原因</h2><p>MapReduce 效率的瓶颈在于：计算机性能、IO 操作优化</p><p>计算机性能包括：</p><blockquote><p>CPU、内存、磁盘监控、网络</p></blockquote><p>IO 操作优化包括：</p><blockquote><p>数据倾斜<br>Map 和 Reduce 数设置不合理<br>Map 运行时间太长，导致 Reduce 等待太久<br>小文件过多<br>大量不可分块的超大文件<br>溢写次数过多<br>归并次数过多</p></blockquote><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p>可以从六个方面考虑优化：数据输入、Map、Reduce、IO、数据倾斜、参数调优</p><h3 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h3><blockquote><p>合并小文件</p></blockquote><p>在执行 MR 任务之前，将小文件进行合并，大量小文件会产生大量的 Map 任务，增加 Map 任务装载数，而任务的装载比较耗时，从而导致 MR 运行慢。</p><p>解决办法：采用 CombinerTextInputFormat 作为输入，解决输入端大量小文件的问题。</p><h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><blockquote><p>减小溢写次数</p></blockquote><p>通过调整 <strong><em>mapred-site.xml</em></strong> 文件中的 <code>mapreduce.task.io.sort.mb</code> 和 <code>mapreduce.map.sort.spill.percent</code> 参数，增大触发溢写的内存上限，减少溢写次数，从而减小磁盘 IO</p><blockquote><p>减少合并次数</p></blockquote><p>通过调整 <strong><em>mapred-site.xml</em></strong> 文件中的 <code>mapreduce.task.io.sort.factor</code> 参数，增大合并的文件数目，减少合并次数，从而缩短 MR 处理时间</p><blockquote><p>在 Map 之后，不影响业务逻辑的前台下，先进行 Combine 处理，减少 IO（适用于汇总）</p></blockquote><h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h3><blockquote><p>合理设置 Map、Reduce 数</p></blockquote><p>影响 Map 个数的是 <code>切片</code>，影响 reduce 个数的是 <code>setNumReduceTasks</code> 方法。<br>这两个数值都不能设置太小，也不能太大。<br>太小会导致 Task 等待，处理时间长；太大会导致 Map、Reduce 任务间竞争资源，造成处理超时等错误</p><blockquote><p>设置 Map、Reduce 共存</p></blockquote><p>调整 <strong><em>mapred-site.xml</em></strong> 文件中的 <code>mapreduce.job.reduce.slowstart.completedmaps</code> 参数，使 Map 运行到一定程度后，Reduce 也开始运行，减少 Reduce 等待时间。</p><blockquote><p>规避使用 Reduce</p></blockquote><p>由于 Reduce 在用于连接数据集的时候会产生大量的网络消耗，如果不需要使用 Reduce，则可以进行规避，减少大量的 shuffle 时间</p><blockquote><p>合理设置 Reduce 的 buffer</p></blockquote><p>默认情况下，数据达到一个阈值的时候，Buffer 中的数据就会写入磁盘，然后 Reduce 会从磁盘中获得所有数据。<br>Buffer 和 Reduce 是没有直接关联的，中间有多次 <code>写磁盘 -&gt; 读磁盘</code> 的过程，可以通过调整参数来规避，使得 Buffer 中的一部分数据可以直接输送到 Reduce，减少 IO 开销。</p><p>通过调整 <strong><em>mapred-site.xml</em></strong> 文件中的 <code>mapreduce.reduce.input.buffer.percent</code> 配置，默认为 0.0。 当数值大于 0 时，会保留指定比例的内存读 Buffer 中的数据直接交给 Reduce，这样一来，设置 Buffer 需要内存、读取数据需要内存、Reduce 计算也需要内存，如果调整的不合理可能会撑爆服务器，因此需要根据作业运行情况去进行调整。</p><h3 id="IO"><a href="#IO" class="headerlink" title="IO"></a>IO</h3><blockquote><p>数据压缩</p></blockquote><p>安装 Snappy 或 LZO，开启数据压缩</p><blockquote><p>使用 SequenceFile 二进制文件</p></blockquote><h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><p>数据倾斜包含：数据频率倾斜（某一区域内的数据量远远大于其他区域）、数据大小倾斜（部分记录的大小远远大于平均值）</p><p><strong><em>解决方案</em></strong></p><blockquote><p>抽样和范围分区</p></blockquote><p>通过对原始数据进行抽样得到的结果集来预设分区边界值。</p><blockquote><p>自定义分区</p></blockquote><p>使用自定义分区，将某些 key 发送给固定的 Reduce 实例，将剩余 key 发送给剩余的 Reduce 实例</p><blockquote><p>Combine</p></blockquote><p>使用 Combine 可以大量较小数据倾斜，在可能的情况下，Combine 的目的就是聚合并精简数据</p><blockquote><p>采用 MapJoin，避免 ReduceJoin</p></blockquote><h3 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h3><blockquote><p>mapred-default.xml</p></blockquote><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">mapreduce.map.memory.mb</td><td style="text-align:center">一个 MapTask 可以使用的资源上线，默认 1024M。如果 MapTask 实际使用的资源超过该值，将被强制杀死</td></tr><tr><td style="text-align:center">mapreduce.reduce.memory.mb</td><td style="text-align:center">一个 ReduceTask 可以使用的资源上线，默认 1024M。如果 ReduceTask 实际使用的资源超过该值，将被强制杀死</td></tr><tr><td style="text-align:center">mapreduce.map.cpu.vcores</td><td style="text-align:center">一个 MapTask 可使用的最大 CPU 数目，默认 1</td></tr><tr><td style="text-align:center">mapreduce.reduce.cpu.vcores</td><td style="text-align:center">一个 ReduceTask 可使用的最大 CPU 数目，默认 1</td></tr><tr><td style="text-align:center">mapreduce.reduce.shuffle.parallelcopies</td><td style="text-align:center">每个 Reduce 去 Map 中获取数据的并行数，默认 5</td></tr><tr><td style="text-align:center">mapreduce.reduce.shuffle.merge.percent</td><td style="text-align:center">Buffer 中的数据达到多少比例开始写入磁盘，默认 0.66</td></tr><tr><td style="text-align:center">mapreduce.reduce.shuffle.input.buffer.percent</td><td style="text-align:center">Buffer 大小占 Reduce 可用内存的比例，默认 0.7</td></tr><tr><td style="text-align:center">mapreduce.reduce.input.buffer.percent</td><td style="text-align:center">指定多少比例的内存用来存放 Buffer 中的数据，默认 0.0</td></tr><tr><td style="text-align:center">mapreduce.task.io.sort.mb</td><td style="text-align:center">Shuffle 的环形缓冲区大小，默认 100M</td></tr><tr><td style="text-align:center">mapreduce.map.sort.spill.percent</td><td style="text-align:center">环形缓冲区溢写的阈值，默认 0.8</td></tr><tr><td style="text-align:center">mapreduce.map.maxattempts</td><td style="text-align:center">每个 MapTask 最大重试次数，超过该值认为 MapTask 失败，默认 4</td></tr><tr><td style="text-align:center">mapreduce.reduce.maxattempts</td><td style="text-align:center">每个 ReduceTask 最大重试次数，超过该值认为 ReduceTask 失败，默认 4</td></tr><tr><td style="text-align:center">mapreduce.task.timeout</td><td style="text-align:center">Task 超时时间，如果 Task 在一定时间内没有读取新数据，也没有输出数据，则认为 Task 处于 Block 状态，为防止因为用户程序永远 Block 不退出，则强制设置一个超时时间，默认为 10 分钟</td></tr></tbody></table><blockquote><p>yarn-default.xml</p></blockquote><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">yarn.scheduler.minimum-allocation-mb</td><td style="text-align:center">应用程序 Container 分配的最小内存，默认 1024</td></tr><tr><td style="text-align:center">yarn.scheduler.maximum-allocation-mb</td><td style="text-align:center">应用程序给 Container 份分配的最大内存，默认 8192</td></tr><tr><td style="text-align:center">yarn.scheduler.minimum-allocation-vcores</td><td style="text-align:center">每个 Container 申请最小 CPU 核数，默认 1</td></tr><tr><td style="text-align:center">yarn.scheduler.maximum-allocation-vcores</td><td style="text-align:center">每个 Container 申请的最大 CPU 核数，默认 4</td></tr><tr><td style="text-align:center">yarn.nodemanager.resource.memory-mb</td><td style="text-align:center">给 Container 分配的最大物理内存，默认 8192</td></tr></tbody></table><h2 id="小文件优化"><a href="#小文件优化" class="headerlink" title="小文件优化"></a>小文件优化</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Yarn 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的 &lt;code&gt;操作系统&lt;/code&gt;，而 MapReduce 等运算程序相当于 &lt;code&gt;操作系统上的应用程序&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="yarn" scheme="https://www.laiyy.top/categories/hadoop/yarn/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="yarn" scheme="https://www.laiyy.top/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（17） Map Reduce &lt;BR /&gt; 计数器、压缩</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/hadoop-17.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/hadoop-17.html</id>
    <published>2019-12-13T06:45:52.000Z</published>
    <updated>2019-12-13T06:45:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop 为每个作业维护若干个内置计数器，以描述多项指标。<br>如：记录已处理的字节数和记录数，使用户可以监控已处理的输入数据量和已产生的输出数据量</p><a id="more"></a><h1 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h1><p>计数器 API</p><blockquote><p>枚举方式计数<br>采用计数器组、计数器名称的方式计数<br>计数器结果在程序运行后，在控制台上查看</p></blockquote><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>在运行核心业务 MapReduce 之前，往往要先进行数据清洗，清理掉不符合用户要求的数据。清理过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序</p><p>使用一个项目中的日志信息作为输入数据，去除字段长度小于 11 的日志信息</p><p><strong><em>数据信息敏感，不公开</em></strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> result = parseLog(line, context);</span><br><span class="line">        <span class="keyword">if</span> (result)&#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">parseLog</span><span class="params">(String line, Context context)</span> </span>&#123;</span><br><span class="line">        String[] fields = line.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>)&#123;</span><br><span class="line">            context.getCounter(<span class="string">"map"</span>, <span class="string">"true"</span>).increment(<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        context.getCounter(<span class="string">"map"</span>, <span class="string">"false"</span>).increment(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取 job 对象</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">    Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置 jar 存放路径</span></span><br><span class="line">    job.setJarByClass(LogDriver.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关联 Mapper、Reducer 业务类</span></span><br><span class="line">    job.setMapperClass(LogMapper.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定最终输出的数据 KV 类型</span></span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 job 的输入文件所在目录</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\dev\\log\\*"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 job 的输出结果所在目录</span></span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\dev\\log\\output"</span>));</span><br><span class="line"></span><br><span class="line">    job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提交 job</span></span><br><span class="line">    <span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    System.exit(succeed ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>查看结果</p></blockquote><p>数据清洗前：<br><img src="/images/hadoop/shuffle/log-before.png" alt="数据清洗前"></p><p>数据清洗后：<br><img src="/images/hadoop/shuffle/log-after.png" alt="数据清洗后"></p><p><strong><em>具体更复杂的数据清洗，根据输入数据的不同进行高度定制化，此例只做基础概念思想</em></strong></p><hr><h1 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h1><p>压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行 MapReduce 程序时，IO 操作、网络传输、shuffle 和 merge 要花费大量的时间，尤其是数据规模很大、工作负载密集的情况下，因此使用数据压缩非常重要。</p><p>磁盘IO 和网络带宽是 hadoop 的宝贵资源，数据压缩对于节省资源、最小化磁盘 IO、网络传输很有帮助；可以在 <strong><code>任意</code></strong> MapReduce 阶段启用压缩。压缩也是有代价的。</p><p>压缩是提高 hadoop 运行效率的一种优化策略，通过对 Mapper、Reduce 程序运行过程的数据进行压缩，以减少磁盘 IO，提高 MapReduce 程序运行速度。</p><p><strong><em>注意：采用压缩技术，减少了磁盘 IO，但同时增加了 CPU 运算负担，所以，压缩特性运用得当，能提高性能，但是运用不得当也可能降低性能</em></strong></p><blockquote><p>压缩基本原则</p></blockquote><ol><li>运算密集型的任务，少用压缩</li><li>IO 密集型的任务，多用压缩</li></ol><h2 id="压缩编码"><a href="#压缩编码" class="headerlink" title="压缩编码"></a>压缩编码</h2><table><thead><tr><th style="text-align:center">压缩格式</th><th style="text-align:center">是否 hadoop 自带</th><th style="text-align:center">算法</th><th style="text-align:center">文件扩展名</th><th style="text-align:center">是否可切分</th><th style="text-align:center">是否需要修改程序</th><th>对应编解码器</th></tr></thead><tbody><tr><td style="text-align:center">DEFLATE</td><td style="text-align:center">是</td><td style="text-align:center">DEFLATE</td><td style="text-align:center">.deflate</td><td style="text-align:center">否</td><td style="text-align:center">否</td><td>DefaultCodec</td></tr><tr><td style="text-align:center">GZIP</td><td style="text-align:center">是</td><td style="text-align:center">DEFLATE</td><td style="text-align:center">.gz</td><td style="text-align:center">否</td><td style="text-align:center">否</td><td>GzipCodec</td></tr><tr><td style="text-align:center">bzip2</td><td style="text-align:center">是</td><td style="text-align:center">bzip2</td><td style="text-align:center">.bz2</td><td style="text-align:center"><strong><em>是</em></strong></td><td style="text-align:center">否</td><td>BZip2Codec</td></tr><tr><td style="text-align:center">LZO</td><td style="text-align:center"><strong><em>否</em></strong></td><td style="text-align:center">LZO</td><td style="text-align:center">.lzo</td><td style="text-align:center"><strong><em>是</em></strong></td><td style="text-align:center"><strong><em>是，需要建索引、指定输入格式</em></strong></td><td>LzopCodec</td></tr><tr><td style="text-align:center">Snappy</td><td style="text-align:center"><strong><em>否</em></strong></td><td style="text-align:center">Snappy</td><td style="text-align:center">.snappy</td><td style="text-align:center">否</td><td style="text-align:center">否</td><td>SnappyCodec</td></tr></tbody></table><blockquote><p>性能比较</p></blockquote><table><thead><tr><th style="text-align:center">压缩算法</th><th style="text-align:center">原始文件大小</th><th style="text-align:center">压缩文件大小</th><th style="text-align:center">压缩速度</th><th style="text-align:center">解压速度</th></tr></thead><tbody><tr><td style="text-align:center">gzip</td><td style="text-align:center">8.3G</td><td style="text-align:center">1.8G</td><td style="text-align:center">17.5M/S</td><td style="text-align:center">58M/S</td></tr><tr><td style="text-align:center">bzip2</td><td style="text-align:center">8.3G</td><td style="text-align:center">1.1G</td><td style="text-align:center">2.4M/S</td><td style="text-align:center">9.5M/S</td></tr><tr><td style="text-align:center">LZO</td><td style="text-align:center">8.3G</td><td style="text-align:center">2.9G</td><td style="text-align:center">49.3M/S</td><td style="text-align:center">74.6M/S</td></tr></tbody></table><p>Snappy 在单核 i7 64 位处理器上，压缩速度可以达到 250M/S 以上，解压速度可以达到 500M/S 以上；但是压缩完的文件大小还是很大，且不可以切分。</p><h2 id="压缩方式对比"><a href="#压缩方式对比" class="headerlink" title="压缩方式对比"></a>压缩方式对比</h2><h3 id="GZip-压缩"><a href="#GZip-压缩" class="headerlink" title="GZip 压缩"></a>GZip 压缩</h3><blockquote><p>优点：</p></blockquote><ol><li>压缩率较高，压缩、解压速度较快</li><li>hadoop 本身支持，在应用中处理 gzip 文件和直接处理文本一样</li><li>大部分 linux 系统都自带 gzip 命令，使用方便</li></ol><blockquote><p>缺点：</p></blockquote><p>不支持切分</p><blockquote><p>应用场景</p></blockquote><p>当每个文件压缩后，文件大小在 130M 以内（即一个块大小的 1.1 倍以内），都可以考虑用 gzip 压缩。</p><h3 id="BZip2"><a href="#BZip2" class="headerlink" title="BZip2"></a>BZip2</h3><blockquote><p>优点</p></blockquote><ol><li>支持切分</li><li>压缩率高</li><li>hadoop 自带，使用方便</li></ol><blockquote><p>缺点</p></blockquote><p>压缩、解压速度慢</p><blockquote><p>应用场景</p></blockquote><ol><li>对速度要求不高，但是要求较高压缩率；</li><li>输出后的数据比较大，处理后的速度需要压缩存档减少磁盘空间并且以后数据用的比较少</li><li>对单个很大的文本文件向压缩以较少存储空间，同时需要支持切分，而且兼容之前的应用程序</li></ol><h3 id="LZO"><a href="#LZO" class="headerlink" title="LZO"></a>LZO</h3><blockquote><p>优点</p></blockquote><ol><li>压缩、解压速度快</li><li>压缩率合理</li><li>支持切分，hadoop 中最流行的压缩格式</li><li>可以在 linux 中安装 lzop 命令，使用方便</li></ol><blockquote><p>缺点</p></blockquote><ol><li>压缩率比 gzip 低</li><li>hadoop 本身不支持，需要安装</li><li>在应用中需要对 lzo 格式的文件做特殊处理（建索引、指定文件格式）</li></ol><blockquote><p>应用场景</p></blockquote><p>一个很大的文本文件，压缩后还大于 200M 以上的可以考虑。单个文件越大，lzo 优点越明显</p><h3 id="Snappy"><a href="#Snappy" class="headerlink" title="Snappy"></a>Snappy</h3><blockquote><p>优点</p></blockquote><ol><li>高速压缩、解压</li><li>压缩率合理 </li></ol><blockquote><p>缺点</p></blockquote><ol><li>不支持切分</li><li>压缩率比 gzip 低</li><li>hadoop 本身不支持，需要安装</li></ol><blockquote><p>应用场景</p></blockquote><ol><li>当 MapReduce 的 Map 输出数据比较大，作为 Map 到 Reduce 的中间数据压缩格式</li><li>作为 MapReduce 作业的输出和另外一个 MapReduce 作业的输入</li></ol><h2 id="压缩位置的选择"><a href="#压缩位置的选择" class="headerlink" title="压缩位置的选择"></a>压缩位置的选择</h2><blockquote><p>Map 输入之前</p></blockquote><p>在有大量数据并计划重复处理的情况下，应该考虑对输入进行压缩。此时无需指定使用的压缩方式，hadoop 可以检查文件的扩展名，如果扩展名能够匹配，就会使用恰当的编解码方式对文件进行压缩，否则，hadoop 不会使用压缩</p><blockquote><p>Map 处理之后、Reduce 之前</p></blockquote><p>当 Mapper 任务输出的中间数据量很大时，应考虑再此阶段采用压缩技术，能够显著改善内部数据的 shuffle 过程；而 shuffle 过程在 hadoop 处理过程中是消耗资源最多的环节。<br>如果发现数据量大造成网络传输缓慢，应该考虑使用压缩技术，可以用于 Mapper 输出压缩的格式为：LZO、Snappy。</p><p>需要注意：<br>LZO 是提供 hadoop 压缩数据的通用压缩编解码器。设计目的是达到与硬盘读写速度相当的压缩速度，因此速度是优先考虑的因素，而不是压缩率。<br>与 gzip 编解码器相比，它的压缩速度是 gzip 的 5 倍，解压速度是 gzip 的 2 倍。<br>同一个文件用 LZO 压缩后，比用 gzip 压缩后大 50%，但是比压缩前小 25%~50%，这对于改善性能非常有利，Map 阶段完成时间快大概 4 倍。</p><blockquote><p>Reducer 输出</p></blockquote><p>此阶段启用压缩，能够减少要存储的数据量，因此降低所需的磁盘空间。<br>当 MapReduce 作业形成链条时，第二个作业的输入也以压缩，所以启用压缩同样有效。</p><h2 id="压缩参数的配置"><a href="#压缩参数的配置" class="headerlink" title="压缩参数的配置"></a>压缩参数的配置</h2><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">默认值</th><th style="text-align:center">阶段</th><th style="text-align:center">备注</th></tr></thead><tbody><tr><td style="text-align:center"><code>io.compression.codecs</code>（core-site.xml）</td><td style="text-align:center">DefaultCodec、GzipCodec、BZipCodec</td><td style="text-align:center">输入压缩</td><td style="text-align:center">hadoop 使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td style="text-align:center"><code>mapreduce.map.output.compress</code>（mapred-site.xml）</td><td style="text-align:center">false</td><td style="text-align:center">mapper 输出</td><td style="text-align:center">为 true 时启用压缩</td></tr><tr><td style="text-align:center"><code>mapreduce.map.output.compress.codec</code>（mapred-site.xml）</td><td style="text-align:center">DefaultCodec</td><td style="text-align:center">mapper 输出</td><td style="text-align:center">多用 LZO 或 Snappy</td></tr><tr><td style="text-align:center"><code>mapreduce.output.fileoutputformat.compress</code>（mapred-site.xml）</td><td style="text-align:center">false</td><td style="text-align:center">reduce 输出</td><td style="text-align:center">为 true 时启用压缩</td></tr><tr><td style="text-align:center"><code>mapreduce.output.fileoutputformat.compress.type</code>（mapred-site.xml）</td><td style="text-align:center">RECORD</td><td style="text-align:center">reduce 输出</td><td style="text-align:center">SequenceFile 输出使用的压缩类型（BLOCK、NONE、RECORD）</td></tr><tr><td style="text-align:center"><code>mapreduce.output.fileoutputformat.compress.codec</code>（mapred-site.xml）</td><td style="text-align:center">DefaultCodec</td><td style="text-align:center">reduce 输出</td><td style="text-align:center">具体的压缩编码</td></tr></tbody></table><h2 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h2><p>CompressionCodec 有两个方法可以用于轻松的压缩、解压数据。</p><p>想要对正在被写入一个输出流的数据进行压缩，可以使用 <code>createOutputStream</code> 方法创建一个 CompressionOutputStream，将其以压缩格式写入底层的流。<br>想要对输入数据进行嘉业，可以使用 <code>createInputStream</code> 方法床架一个 CompressionInputStream，从而从底层的流读取未压缩的数据</p><h3 id="压缩测试"><a href="#压缩测试" class="headerlink" title="压缩测试"></a>压缩测试</h3><p>输入数据：使用之前 <a href="/hadoop/map-reduce/input-format/hadoop-12.html#案例实操">CombinerTextInputFormat 示例</a>  中的 <a href="/file/hadoop/input-format/d.txt">输入数据 d.txt</a> 作为压缩测试的输入数据，测试不同压缩方式下的压缩率、耗时等。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    String filePath = <span class="string">"d:/dev/wordcount/d.txt"</span>;</span><br><span class="line"><span class="comment">//        String type = "org.apache.hadoop.io.compress.BZip2Codec";</span></span><br><span class="line"><span class="comment">//        String type = "org.apache.hadoop.io.compress.DefaultCodec";</span></span><br><span class="line">    String type = <span class="string">"org.apache.hadoop.io.compress.GzipCodec"</span>;</span><br><span class="line">    compress(filePath, type);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 测试压缩</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@param</span> path 待压缩文件路径</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@param</span> type 压缩方式</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">(String path, String type)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    <span class="comment">// 1. 获取输入流</span></span><br><span class="line">    FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(path));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 设置压缩方式</span></span><br><span class="line">    Class&lt;?&gt; compressClass = Class.forName(type);</span><br><span class="line">    <span class="comment">// 3. 反射获取编解码实例</span></span><br><span class="line">    CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(compressClass, <span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 获取输出流，通过编解码实例获取后缀</span></span><br><span class="line">    FileOutputStream outputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(path + codec.getDefaultExtension()));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5.创建压缩输出流</span></span><br><span class="line">    CompressionOutputStream codecOutputStream = codec.createOutputStream(outputStream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6. 流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(inputStream, codecOutputStream, <span class="number">1024</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7. 关闭资源</span></span><br><span class="line">    IOUtils.closeStream(codecOutputStream);</span><br><span class="line">    IOUtils.closeStream(outputStream);</span><br><span class="line">    IOUtils.closeStream(inputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>测试结果</p></blockquote><p>可见压缩前后数据大小的区别非常明显<br><img src="/images/hadoop/map-reduce/compress.png" alt="压缩结果"></p><h3 id="解压测试"><a href="#解压测试" class="headerlink" title="解压测试"></a>解压测试</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"><span class="comment">//        String path = "d:/dev/wordcount/d.txt.bz2";</span></span><br><span class="line"><span class="comment">//        String path = "d:/dev/wordcount/d.txt.gz";</span></span><br><span class="line">    String path = <span class="string">"d:/dev/wordcount/d.txt.deflate"</span>;</span><br><span class="line">    decompress(path);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">decompress</span><span class="params">(String path)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    <span class="comment">// 1. 校验文件是否可以解压</span></span><br><span class="line">    CompressionCodecFactory factory = <span class="keyword">new</span> CompressionCodecFactory(<span class="keyword">new</span> Configuration());</span><br><span class="line">    CompressionCodec codec = factory.getCodec(<span class="keyword">new</span> Path(path));</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> == codec)&#123;</span><br><span class="line">        System.out.println(<span class="string">"不支持压缩"</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 获取输入流</span></span><br><span class="line">    FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(path);</span><br><span class="line">    CompressionInputStream codecInputStream = codec.createInputStream(inputStream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 获取输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(path + <span class="string">".txt"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 流对拷</span></span><br><span class="line">    IOUtils.copyBytes(codecInputStream, fileOutputStream, <span class="number">1024</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 关闭流</span></span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    IOUtils.closeStream(codecInputStream);</span><br><span class="line">    IOUtils.closeStream(inputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>测试结果</p></blockquote><p><img src="/images/hadoop/map-reduce/decompress.png" alt="解压测试"></p><h3 id="Map-输出端采用压缩"><a href="#Map-输出端采用压缩" class="headerlink" title="Map 输出端采用压缩"></a>Map 输出端采用压缩</h3><p>即是 MapReduce 的输入、输出文件都是未压缩的文件，仍然可以对 Map 任务的中间结果输出做压缩（原因是 Map 结束后要将文件写入磁盘，并通过网络传输到 Reduce 节点，对其压缩可以提高很多性能）</p><p>Hadoop 源码支持的压缩格式有：<code>BZip2Codec</code>、<code>DefaultCodec</code></p><blockquote><p>基于 <a href="/hadoop/map-reduce/input-format/hadoop-12.html#案例实操">CombinerTextInputFormat 示例</a> 进行操作。</p></blockquote><p>首先执行 wordcount 实例，查看控制台 MapReduce 过程的字节数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Map-Reduce Framework</span><br><span class="line">    Map input records=3792092</span><br><span class="line">    Map output records=5056134</span><br><span class="line">    Map output bytes=50561388</span><br><span class="line">    Map output materialized bytes=60673704</span><br><span class="line">    Input split bytes=778</span><br><span class="line">    Combine input records=0</span><br><span class="line">    Combine output records=0</span><br><span class="line">    Reduce input groups=10</span><br><span class="line">    Reduce shuffle bytes=60673704</span><br><span class="line">    Reduce input records=5056134</span><br><span class="line">    Reduce output records=10</span><br><span class="line">    Spilled Records=10112268</span><br><span class="line">    Shuffled Maps =8</span><br><span class="line">    Failed Shuffles=0</span><br><span class="line">    Merged Map outputs=8</span><br><span class="line">    GC time elapsed (ms)=259</span><br><span class="line">    Total committed heap usage (bytes)=8830582784</span><br><span class="line"></span><br><span class="line">压缩前总耗时：9090 ms</span><br></pre></td></tr></table></figure><p>修改 WordCountDriver，增加压缩配置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启压缩</span></span><br><span class="line">configuration.set(<span class="string">"mapreduce.map.output.compress"</span>, <span class="string">"true"</span>);</span><br><span class="line"><span class="comment">// 指定压缩格式</span></span><br><span class="line">configuration.setClass(<span class="string">"mapreduce.map.output.compress.codec"</span>, BZip2Codec.class, CompressionCodec.class);</span><br></pre></td></tr></table></figure><p>再次运行测试，查看控制台 MapReduce 过程字节数（开启压缩后的程序会比开启压缩前要慢很多）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Map-Reduce Framework</span><br><span class="line">    Map input records=3792092</span><br><span class="line">    Map output records=5056134</span><br><span class="line">    Map output bytes=50561388</span><br><span class="line">    Map output materialized bytes=207808</span><br><span class="line">    Input split bytes=778</span><br><span class="line">    Combine input records=0</span><br><span class="line">    Combine output records=0</span><br><span class="line">    Reduce input groups=10</span><br><span class="line">    Reduce shuffle bytes=207808</span><br><span class="line">    Reduce input records=5056134</span><br><span class="line">    Reduce output records=10</span><br><span class="line">    Spilled Records=10112268</span><br><span class="line">    Shuffled Maps =8</span><br><span class="line">    Failed Shuffles=0</span><br><span class="line">    Merged Map outputs=8</span><br><span class="line">    GC time elapsed (ms)=286</span><br><span class="line">    Total committed heap usage (bytes)=7362576384</span><br><span class="line"></span><br><span class="line">压缩总耗时：312134 ms</span><br></pre></td></tr></table></figure><p><img src="/images/hadoop/map-reduce/map-compress.png" alt="压缩前后字节数对比"></p><h3 id="Reduce-输出压缩"><a href="#Reduce-输出压缩" class="headerlink" title="Reduce 输出压缩"></a>Reduce 输出压缩</h3><p>还是以上例为例，将 map 压缩注释掉（为了快速测试 reduce 压缩，省去 map 压缩的等待时间），增加如下配置，开启 reduce 压缩<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启 reduce 压缩</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 采用 bzip 压缩</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span><br><span class="line"><span class="comment">// 采用gzip压缩</span></span><br><span class="line"><span class="comment">//FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);</span></span><br><span class="line"><span class="comment">// 采用默认压缩</span></span><br><span class="line"><span class="comment">//FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);</span></span><br></pre></td></tr></table></figure></p><p>测试三种压缩方式的执行耗时，由于文件过小，实际总耗时相差无几。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reduce bz2 总耗时：9132</span><br><span class="line">reduce gz 总耗时：9166</span><br><span class="line">reduce default 总耗时：9038</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop 为每个作业维护若干个内置计数器，以描述多项指标。&lt;br&gt;如：记录已处理的字节数和记录数，使用户可以监控已处理的输入数据量和已产生的输出数据量&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（16） Map Reduce &lt;BR /&gt; ReduceJoin、MapJoin</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/reduce-join/hadoop-16.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/reduce-join/hadoop-16.html</id>
    <published>2019-12-13T01:43:17.000Z</published>
    <updated>2019-12-13T01:43:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>ReduceJoin 的工作：</p><p>Map 端的主要工作：为来自不同表或者文件的 KV 对，打标签以区别不同来源的记录，然后用连接字段作为 key，其余部分和新加的标志位作为 value，最后进行输出。<br>Reduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，只需要在每个分组中，将那么来源于不同文件的记录分开，最后完成合并即可。</p><a id="more"></a><h1 id="ReduceJoin"><a href="#ReduceJoin" class="headerlink" title="ReduceJoin"></a>ReduceJoin</h1><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>需求：输入数据为两个表：<a href="/file/hadoop/join/order.txt">订单</a>、<a href="/file/hadoop/join/pd.txt">商品信息</a>，将商品信息中的数据，根据商品的 pid，合并到订单数据中。</p><blockquote><p>TableBean</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 订单 id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String id;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 产品 id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String pid;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 数量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> amount;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 产品名称</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String pname;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 标记位</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String flag;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(id);</span><br><span class="line">        dataOutput.writeUTF(pid);</span><br><span class="line">        dataOutput.writeInt(amount);</span><br><span class="line">        dataOutput.writeUTF(pname);</span><br><span class="line">        dataOutput.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        id = dataInput.readUTF();</span><br><span class="line">        pid = dataInput.readUTF();</span><br><span class="line">        amount = dataInput.readInt();</span><br><span class="line">        pname = dataInput.readUTF();</span><br><span class="line">        flag = dataInput.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id + <span class="string">"\t"</span> + pname + <span class="string">"\t"</span> + amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略 get、set</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">TableBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> TableBean table = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String fileName;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 在 map 执行前，获取文件名称，来判断当前处理的是哪个文件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        FileSplit inputSplit = ((FileSplit) context.getInputSplit());</span><br><span class="line">        fileName = inputSplit.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="keyword">if</span> (fileName.contains(<span class="string">"order"</span>)) &#123;</span><br><span class="line">            <span class="comment">// order.txt</span></span><br><span class="line">            table.setId(fields[<span class="number">0</span>]);</span><br><span class="line">            table.setPid(fields[<span class="number">1</span>]);</span><br><span class="line">            table.setAmount(Integer.parseInt(fields[<span class="number">2</span>]));</span><br><span class="line">            table.setFlag(<span class="string">"order"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// pd.txt</span></span><br><span class="line">            table.setFlag(<span class="string">"pd"</span>);</span><br><span class="line">            table.setPid(fields[<span class="number">0</span>]);</span><br><span class="line">            table.setPname(fields[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        outKey.set(table.getPid());</span><br><span class="line"></span><br><span class="line">        context.write(outKey, table);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">TableBean</span>, <span class="title">TableBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        List&lt;TableBean&gt; tableBeans = Lists.newArrayList();</span><br><span class="line">        <span class="comment">// 注意！！！！！</span></span><br><span class="line">        <span class="comment">// 一定要遍历中重新创建对象，拷贝数据到新创建的对象中，并把新创建的对象放入一个新的 list 中！！</span></span><br><span class="line">        <span class="comment">// 否则 tableBeans 里面的对象都是同一个对象！！！</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean value : values) &#123;</span><br><span class="line">            TableBean tableBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// 示例使用的是 commons 包的对象拷贝，阿里规范禁止使用，如果在 spring 项目中使用 hadoop，尽量使用 spring util 的对象拷贝</span></span><br><span class="line">                BeanUtils.copyProperties(tableBean, value);</span><br><span class="line">                tableBeans.add(tableBean);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IllegalAccessException | InvocationTargetException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 获取 orders</span></span><br><span class="line">        List&lt;TableBean&gt; orders = tableBeans.stream().filter(tableBean -&gt; tableBean.getFlag().equals(<span class="string">"order"</span>))</span><br><span class="line">                .collect(Collectors.toList());</span><br><span class="line">        <span class="comment">// 2. 获取商品，并转换为 map，key 为商品id，value 为商品名称</span></span><br><span class="line">        Map&lt;String, String&gt; pd = tableBeans.stream().filter(tableBean -&gt; tableBean.getFlag().equals(<span class="string">"pd"</span>))</span><br><span class="line">                .collect(Collectors.toMap(TableBean::getPid, TableBean::getPname));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 遍历 orders，赋值 pname，并输出</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (TableBean order : orders) &#123;</span><br><span class="line">            order.setPname(pd.get(order.getPid()));</span><br><span class="line"></span><br><span class="line">            context.write(order, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>省略 Driver，查看运行结果</p></blockquote><p><img src="/images/hadoop/map-reduce/reduce-join.png" alt="reduce join "></p><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>合并的操作在 Reduce 阶段完成，Reduce 端的处理压力太大，Map 节点的运算负载很低，资源利用率不高，而且在 Reduce 阶段极易产生数据倾斜。 推荐使用 MapJoin</p><hr><h1 id="MapJoin"><a href="#MapJoin" class="headerlink" title="MapJoin"></a>MapJoin</h1><p>适用场景： 一个张表十分小，一个表十分大</p><blockquote><p>优点</p></blockquote><p>在 Map 端缓存多张表，提前处理业务逻辑，增加了 Map 业务，减少 Reduce 数据压力，尽可能减少数据倾斜</p><blockquote><p>方法</p></blockquote><p>1、在 Mapper 的 setup 阶段，将文件读取到缓存集合中<br>2、在驱动函数中加载缓存</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>依然使用上例中的输入数据，输入结果也应该与上例一致。</p><blockquote><p>Driver</p></blockquote><p><strong><em>由于 MapJoin 不需要 Reduce 端，此时可以将 Driver 中的 MapperKeyClass、MapperValueClass、ReduceClass 去掉</em></strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取 job 对象</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">    Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置 jar 存放路径</span></span><br><span class="line">    job.setJarByClass(MapJoinDriver.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关联 Mapper、Reducer 业务类</span></span><br><span class="line">    job.setMapperClass(MapJoinMapper.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定最终输出的数据 KV 类型</span></span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 job 的输入文件所在目录</span></span><br><span class="line">    <span class="comment">// 只读入 order</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\dev\\join\\order.txt"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 job 的输出结果所在目录</span></span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\dev\\join\\output1"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载缓存数据</span></span><br><span class="line">    job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"file:///d:/dev/join/pd.txt"</span>));</span><br><span class="line">    <span class="comment">// MapJoin 的逻辑不需要 Reduce，设置 ReduceTask 为 0</span></span><br><span class="line">    job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提交 job</span></span><br><span class="line">    <span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    System.exit(succeed ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, String&gt; fieldMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String path = context.getCacheFiles()[<span class="number">0</span>].getPath();</span><br><span class="line">        BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(path), StandardCharsets.UTF_8));</span><br><span class="line">        String line;</span><br><span class="line">        <span class="keyword">while</span> (StringUtils.isNotBlank(line = reader.readLine()))&#123;</span><br><span class="line">            String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">            fieldMap.put(fields[<span class="number">0</span>], fields[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        IOUtils.closeStream(reader);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">        String pid = fields[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        String pname = fieldMap.get(pid);</span><br><span class="line"></span><br><span class="line">        outKey.set(fields[<span class="number">0</span>] + <span class="string">"\t"</span> + pname + <span class="string">"\t"</span> + fields[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">        context.write(outKey, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>查看结果</p></blockquote><p><img src="/images/hadoop/map-reduce/map-join.png" alt="MapJoin"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ReduceJoin 的工作：&lt;/p&gt;
&lt;p&gt;Map 端的主要工作：为来自不同表或者文件的 KV 对，打标签以区别不同来源的记录，然后用连接字段作为 key，其余部分和新加的标志位作为 value，最后进行输出。&lt;br&gt;Reduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，只需要在每个分组中，将那么来源于不同文件的记录分开，最后完成合并即可。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
      <category term="reduce-join" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/reduce-join/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
      <category term="reduce-join" scheme="https://www.laiyy.top/tags/reduce-join/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（15） Map Reduce &lt;BR /&gt; 工作流程、OutputFormat</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/hadoop-15.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/hadoop-15.html</id>
    <published>2019-12-10T06:32:03.000Z</published>
    <updated>2019-12-10T06:32:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>MapTask 流程分为：Read 阶段、Map 阶段、Collect 阶段、溢写阶段、Combine 阶段<br>ReduceTask 流程分为：Copy 阶段、Merge 阶段、Sort 阶段、Reduce 阶段</p><a id="more"></a><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><h2 id="MapTask-流程"><a href="#MapTask-流程" class="headerlink" title="MapTask 流程"></a>MapTask 流程</h2><blockquote><p>Read 阶段</p></blockquote><p>客户端获取待处理文本，提交之前，获取待处理数据的信息，然后根据参数设置，形成一个任务分配的规划；<br>提交信息（Job.split、jar、Job.xml）；<br>计算出 MapTask 数量、开启 MapTask，TextInputFormat 开始读取数据</p><blockquote><p>Map 阶段</p></blockquote><p>读取后，返回对应的 KV 数据，并把数据写入到 Mapper 中</p><blockquote><p>Collect 阶段</p></blockquote><p>Mapper 将数据写入环形缓冲区，并进行分区、排序</p><blockquote><p>溢写阶段</p></blockquote><p>将分区、排序后的数据，溢写到文件（分区且区内有序）</p><blockquote><p>Combine 阶段</p></blockquote><p>将溢写后的数据进行归并排序</p><p><img src="/images/hadoop/shuffle/map-task.png" alt="Map Task"></p><hr><h2 id="ReduceTask-流程"><a href="#ReduceTask-流程" class="headerlink" title="ReduceTask 流程"></a>ReduceTask 流程</h2><blockquote><p>Copy 阶段</p></blockquote><p>将 MapTask 执行结束后，将对应分区的数据，拷贝到 ReduceTask 中</p><blockquote><p>Merge 阶段</p></blockquote><p>将数据进行归并排序</p><blockquote><p>Sort 阶段</p></blockquote><p>合并 ReduceTask 中的文件，进行归并排序</p><blockquote><p>Reduce 阶段</p></blockquote><p>进行数据处理逻辑，通过 TextOutputFormat 输出到指定位置</p><p><img src="/images/hadoop/shuffle/reduce-task.png" alt="reduce-task"></p><h3 id="ReduceTask-数量"><a href="#ReduceTask-数量" class="headerlink" title="ReduceTask 数量"></a>ReduceTask 数量</h3><p>ReduceTask 的并行度，影响着整个 Job 的并发度和执行效率，但是与 <code>MapTask 的并发度，由切片数决定</code> 不同，ReduceTask 的数量是可以手动设置的。<code>job.setNumReduceTasks(2);</code></p><p>ReduceTask 的数量设置，需要根据集群性能去测试调节，并不是一成不变的。</p><blockquote><p>注意事项</p></blockquote><ol><li>ReduceTask=0，表示没有 Reduce 阶段，输出文件的个数和 Map 个数一致。</li><li>ReduceTask=1，输出文件个数为 1 个（默认）</li><li>如果数据分布不均匀，就有可能在 Reduce 阶段产生数据倾斜</li><li>ReduceTask 数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有一个 ReduceTask</li><li>具体需要多少个 ReduceTask，需要根据集群性能而定</li><li>如果分数区不是 1，但是 ReduceTask 为 1，不会执行分区过程！理由：在 MapTask 源码中，执行分区的前提，是判断 ReduceNum 格式是否大于 1，不大于 1 不执行分区</li></ol><hr><h2 id="Shuffle-流程"><a href="#Shuffle-流程" class="headerlink" title="Shuffle 流程"></a>Shuffle 流程</h2><p>Shuffle 运行在 Map 方法之后，Reduce 方法之前；部分与 Map、Reduce 重合。</p><h3 id="Map-阶段"><a href="#Map-阶段" class="headerlink" title="Map 阶段"></a>Map 阶段</h3><p>Map 接收之后，数据写入环形缓冲区，对数据进行分区、排序，数据量到达缓冲区 80% 后，开始溢写数据（可选 Combiner 合并）；<br>溢写到磁盘，生成 2 个文件：spill.index、spill.out；<br>对数据进行归并排序（可选 Combiner 合并）、数据压缩；<br>将文件写入磁盘，分区输出</p><h3 id="Reduce-阶段"><a href="#Reduce-阶段" class="headerlink" title="Reduce 阶段"></a>Reduce 阶段</h3><p>从 MapTask 拷贝数据到内存缓冲区（内存不够时溢写到磁盘）；<br>对每个 Map 来的数据进行归并排序；<br>按照相同的 key 进行分组；<br>执行 reduce 方法</p><p><img src="/images/hadoop/shuffle/shuffle-1.png" alt="shuffle 流程"></p><hr><h1 id="OutputFormat"><a href="#OutputFormat" class="headerlink" title="OutputFormat"></a>OutputFormat</h1><p>OutputFormat 是 MapReduce 输出的基类，所有实现了 MapReduce 输出都实现了 OutputFormat 接口。常见的几个实现类：</p><blockquote><p>TextOutputFormat</p></blockquote><p>默认输出格式，把每条记录斜纹文本行。KV 可以是任意类型，toString 方法可以把它们转为字符串</p><blockquote><p>SequenceFileOutputFormat</p></blockquote><p>可以将将 SequenceFileOutputFormat 的输出，作为后续 MapReduce 任务的输入；格式紧凑，容易压缩</p><blockquote><p>自定义 OutputFormat</p></blockquote><p>自定义实现输出，可定制</p><h2 id="自定义实现"><a href="#自定义实现" class="headerlink" title="自定义实现"></a>自定义实现</h2><p>需求，根据 <a href="/file/hadoop/output-format/log.txt">日志文件</a>，将包含 <code>baidu.com</code> 的日志输出到 <code>baidu.log</code>，其他的输出到 <code>other.log</code></p><blockquote><p>Mapper、Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(key, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>RecordeWriter、OutputFormat</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> LogRecordWriter(job);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FSDataOutputStream baidu, other;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LogRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            FileSystem fileSystem = FileSystem.get(job.getConfiguration());</span><br><span class="line">            baidu = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"d:/dev/opf/baidu.log"</span>));</span><br><span class="line">            other = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"d:/dev/opf/other.log"</span>));</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取域名</span></span><br><span class="line">        String domain = key.toString();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (domain.contains(<span class="string">"baidu.com"</span>))&#123;</span><br><span class="line">            baidu.write((domain+<span class="string">"\n"</span>).getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            other.write((domain+<span class="string">"\n"</span>).getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        IOUtils.closeStream(baidu);</span><br><span class="line">        IOUtils.closeStream(other);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Driver</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setOutputFormatClass(CustomerOutputFormat.class);</span><br></pre></td></tr></table></figure><blockquote><p>测试结果</p></blockquote><p><img src="/images/hadoop/shuffle/opf-result.png" alt="自定义outputformat 结果"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MapTask 流程分为：Read 阶段、Map 阶段、Collect 阶段、溢写阶段、Combine 阶段&lt;br&gt;ReduceTask 流程分为：Copy 阶段、Merge 阶段、Sort 阶段、Reduce 阶段&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（14） Map Reduce &lt;BR /&gt; 排序、合并</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/sort/hadoop-14.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/sort/hadoop-14.html</id>
    <published>2019-12-09T08:02:42.000Z</published>
    <updated>2019-12-09T08:02:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>排序，是 MapReduce 中最重要的操作之一。默认的排序方式是 <code>字典排序</code>，且实现此排序的方式是 <code>快速排序</code>。</p><p>MapTask 和 MapReduce 均会对数据按照 <code>key</code> 进行排序，该操作属于 Hadoop 的默认操作。任何程序中的数据均会被排序，不论逻辑上是否需要。</p><a id="more"></a><h1 id="排序概述"><a href="#排序概述" class="headerlink" title="排序概述"></a>排序概述</h1><blockquote><p>对于 MapTask</p></blockquote><p>它会将处理的结果暂时放到 <code>环形缓冲区</code> 中，当缓冲区的使用率达到一定的阈值之后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上的文件进行 <code>归并排序</code>。</p><blockquote><p>对于 ReduceTask</p></blockquote><p>它从 <code>每个 MapTask</code> 上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写到磁盘上，否则存储到内存。<br>如果磁盘上的文件数据达到一定阈值，则进行一次 <code>归并排序</code>，以生成一个更大的文件。<br>如果内存中文件大小或数目，达到一定阈值，则进行一次 <code>合并</code>，将数据溢写到磁盘上。<br>当所有数据拷贝完成后，ReduceTask 统一对内存和磁盘上的数据进行一次 <code>归并排序</code>。</p><h1 id="排序的分类"><a href="#排序的分类" class="headerlink" title="排序的分类"></a>排序的分类</h1><blockquote><p>部分排序</p></blockquote><p>MapReduce 根据输入记录的键，对数据集排序，保证输出的每个文件，内部有序</p><blockquote><p>全排序</p></blockquote><p>最终输出结果只有一个文件，且文件内部有序。实现方式是：只设置一个 ReduceTask。但是该犯法在处理大型文件是，效率极低，完全丧失了 MapReduce 所提供的并行架构。</p><blockquote><p>辅助排序（GroupingComparator 分组）</p></blockquote><p>在 Reduce 端对 key 进行分组。应用于：在接收的 key 为 bean 对象时，想让一个或几个字段相同（并不是全部字段相同）的 key 进入到同一个 Reduce 方法时，可以使用分组排序</p><blockquote><p>二次排序</p></blockquote><p>在自定义排序过程中，如果 compareTo 中的条件为两个，即为二次排序。</p><h2 id="自定义排序（全排序）"><a href="#自定义排序（全排序）" class="headerlink" title="自定义排序（全排序）"></a>自定义排序（全排序）</h2><p>bean 对象作为 key 传输，需要实现 WritableComparable 接口，重写 compareTo 方法。</p><p>需求：使用之前 <a href="/hadoop/map-reduce/hadoop-10.html#序列化-Demo">序列化实例</a> 的结果作为 <a href="/file/hadoop/shuffle/sort.txt">输入数据</a>，期望输出：按照总流量倒序排序。</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><blockquote><p>Bean</p></blockquote><p><a href="/hadoop/map-reduce/hadoop-10.html#序列化-Demo">序列化实例</a> 中的 FlowBean 类，实现 <code>WritableComparable</code> 接口，重写 <code>compareTo</code> 方法<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean flowBean)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 比较</span></span><br><span class="line">    <span class="keyword">if</span> (sumFlow &gt; flowBean.getSumFlow())&#123;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (sumFlow &lt; flowBean.getSumFlow())&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    String line = value.toString();</span><br><span class="line"></span><br><span class="line">    String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">    String phone = fields[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> upFlow = Long.parseLong(fields[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">long</span> downFlow = Long.parseLong(fields[<span class="number">2</span>]);</span><br><span class="line">    flowBean.set(upFlow, downFlow);</span><br><span class="line"></span><br><span class="line">    Text text = <span class="keyword">new</span> Text();</span><br><span class="line">    text.set(phone);</span><br><span class="line">    context.write(flowBean, text);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">        context.write(value, key);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Driver 省略，测试结果</p></blockquote><p><img src="/images/hadoop/shuffle/sort.png" alt="排序结果"></p><h2 id="区内排序"><a href="#区内排序" class="headerlink" title="区内排序"></a>区内排序</h2><p>需求：使用 <a href="/file/hadoop/shuffle/sort_1.txt">输入数据</a>，期望输出：根据手机号的前三位不同，分成不同的文件，并在每个文件中，按照总流量倒序输出。</p><p>在上一例全排序的基础上，增加一个分区操作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBeanPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(FlowBean flowBean, Text text, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> partitioner = <span class="number">4</span>;</span><br><span class="line">        String phonePrefix = text.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"134"</span>.equals(phonePrefix))&#123;</span><br><span class="line">            partitioner = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"135"</span>.equals(phonePrefix))&#123;</span><br><span class="line">            partitioner = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"136"</span>.equals(phonePrefix))&#123;</span><br><span class="line">            partitioner = <span class="number">2</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(phonePrefix))&#123;</span><br><span class="line">            partitioner = <span class="number">3</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> partitioner;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>查看结果</p></blockquote><p><img src="/images/hadoop/shuffle/shuffle-sort.png" alt="分区排序"></p><hr><h1 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h1><blockquote><p>Combiner 是 MapReduce 程序中 Maper 和 Reducer 之外的一种组件<br>Combiner 是父类是 Reducer<br>Combiner 与 Reducer 的区别在于运行的位置：Combiner 是在每个 MapTask 所在的节点运行；Reducer 是接收全局所有 Mapper 的输出结果<br>Combiner 的意义是对每个 MapTask 的数据进行局部汇总，以减小网络的传输量<br>应用前提：不能影响最终的业务逻辑（Combiner 输出的 KV，应该与 Reducer 的输入 KV 类型对应）</p></blockquote><h2 id="自定义-Combiner"><a href="#自定义-Combiner" class="headerlink" title="自定义 Combiner"></a>自定义 Combiner</h2><p>需求： 使用 <a href="/file/hadoop/combiner/combiner.txt">输入数据</a>，进行 <code>局部汇总</code>，以减小网络传输量。</p><p>期望输出：分隔单词，局部汇总每个单词的数量</p><p><strong><em>以 WordCount 实例为例</em></strong></p><p>原始的 WordCount 的控制台输出为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Map-Reduce Framework</span><br><span class="line">    Map input records=8</span><br><span class="line">    Map output records=18</span><br><span class="line">    Map output bytes=168</span><br><span class="line">    Map output materialized bytes=210</span><br><span class="line">    Input split bytes=100</span><br><span class="line">    Combine input records=0</span><br><span class="line">    Combine output records=0</span><br><span class="line">    Reduce input groups=7</span><br><span class="line">    Reduce shuffle bytes=210</span><br><span class="line">    Reduce input records=18</span><br><span class="line">    Reduce output records=7</span><br><span class="line">    Spilled Records=36</span><br><span class="line">    Shuffled Maps =1</span><br><span class="line">    Failed Shuffles=0</span><br><span class="line">    Merged Map outputs=1</span><br><span class="line">    GC time elapsed (ms)=7</span><br><span class="line">    Total committed heap usage (bytes)=514850816</span><br></pre></td></tr></table></figure></p><blockquote><p>方案 1</p></blockquote><p>自实现一个 Combiner，并在 Driver 中注册。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable();</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 1. 累加求和</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        outValue.set(sum);</span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordCountCombiner.class);</span><br></pre></td></tr></table></figure><p>测试运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Map-Reduce Framework</span><br><span class="line">    Map input records=8</span><br><span class="line">    Map output records=18</span><br><span class="line">    Map output bytes=168</span><br><span class="line">    Map output materialized bytes=85  //缩小了</span><br><span class="line">    Input split bytes=100</span><br><span class="line">    Combine input records=18    // 相比之前 Combine 有变化</span><br><span class="line">    Combine output records=7</span><br><span class="line">    Reduce input groups=7</span><br><span class="line">    Reduce shuffle bytes=85      // 缩小了</span><br><span class="line">    Reduce input records=7      // 缩小了</span><br><span class="line">    Reduce output records=7</span><br><span class="line">    Spilled Records=14          // 缩小了</span><br><span class="line">    Shuffled Maps =1</span><br><span class="line">    Failed Shuffles=0</span><br><span class="line">    Merged Map outputs=1</span><br><span class="line">    GC time elapsed (ms)=11      // 缩小了</span><br><span class="line">    Total committed heap usage (bytes)=514850816</span><br></pre></td></tr></table></figure></p><blockquote><p>方案 2</p></blockquote><p>直接将之前的 Reducer 作为 Combiner 即可。<code>job.setCombinerClass(WordCountReducer.class);</code></p><hr><h1 id="辅助排序"><a href="#辅助排序" class="headerlink" title="辅助排序"></a>辅助排序</h1><p>GroupingComparator，对 Reducer 阶段的数据根据某一个或几个字段进行分组。</p><p>分组排序步骤：</p><blockquote><p>自定义排序类，继承 WritableComparator<br>从学 compare 方法<br>创建一个构造，将比较对象传给父类</p></blockquote><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p>根据订单 <a href="/file/hadoop/grouping/grouping.txt">输入数据</a>，进行分组，并找出每笔订单中最贵的商品</p><p>实现步骤：</p><ol><li>利用 “订单 id、成交金额” 作为 key，可以将 Map 阶段读取到的订单数据按照 id 进行排序。如果 id 相同，再根据金额降序，然后发送到 Reducer</li><li>在 Reducer 利用 GroupingComparator 将订单相同的 KV 聚合成组，然后取第一个即可。</li></ol><blockquote><p>创建一个 Bean，用于存储订单信息</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">OrderBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> orderId;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span> price;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(OrderBean order)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 先按照 id 升序，id 相同的按照价格降序</span></span><br><span class="line">        <span class="keyword">if</span> (orderId &gt; order.getOrderId())&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (orderId &lt; order.getOrderId())&#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> Double.compare(order.getPrice(), price);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeInt(orderId);</span><br><span class="line">        dataOutput.writeDouble(price);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        orderId = dataInput.readInt();</span><br><span class="line">        price = dataInput.readDouble();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> OrderBean order = <span class="keyword">new</span> OrderBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String[] fields = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="keyword">int</span> orderId = Integer.parseInt(fields[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">double</span> price = Double.parseDouble(fields[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">        order.setOrderId(orderId);</span><br><span class="line">        order.setPrice(price);</span><br><span class="line"></span><br><span class="line">        context.write(order, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">OrderBean</span>, <span class="title">NullWritable</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(key, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Driver 省略</p></blockquote><blockquote><p>结果</p></blockquote><p><img src="/images/hadoop/shuffle/grouping.png" alt="grouping1"></p><h2 id="开始分组排序"><a href="#开始分组排序" class="headerlink" title="开始分组排序"></a>开始分组排序</h2><p>在得到了结果后，可以看到，现在的结果确实是不同订单的，在一起显示，且是按照倒序排列的。只不过，没有进行分组，没有完成只输出第一条。在此基础上，开始进行辅助分组排序。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个构造，调用父方法的构造，第二个参数必须传为 true</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(OrderBean.class, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注意，一定要重写两个参数均为 WritableComparable 的比较方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">        OrderBean order1 = ((OrderBean) a);</span><br><span class="line">        OrderBean order2 = ((OrderBean) b);</span><br><span class="line">        <span class="keyword">int</span> result;</span><br><span class="line">        <span class="keyword">if</span> (order1.getOrderId() &gt; order2.getOrderId())&#123;</span><br><span class="line">            result = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order1.getOrderId() &lt; order2.getOrderId())&#123;</span><br><span class="line">            result = -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            result = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// WritableComparator 源码</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">WritableComparator</span><span class="params">(Class&lt;? extends WritableComparable&gt; keyClass, <span class="keyword">boolean</span> createInstances)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(keyClass, (Configuration)<span class="keyword">null</span>, createInstances);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">WritableComparator</span><span class="params">(Class&lt;? extends WritableComparable&gt; keyClass, Configuration conf, <span class="keyword">boolean</span> createInstances)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.keyClass = keyClass;</span><br><span class="line">    <span class="keyword">this</span>.conf = conf != <span class="keyword">null</span> ? conf : <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 由此可见，如果 OrderGroupingComparator 的构造参数，在调用父类的构造时，</span></span><br><span class="line">    <span class="comment">// 如果传入的是 false，或者不传入第二个参数，则 key1、buffer 均为 null，此时可能出现空指针异常。</span></span><br><span class="line">    <span class="keyword">if</span> (createInstances) &#123;</span><br><span class="line">        <span class="keyword">this</span>.key1 = <span class="keyword">this</span>.newKey();</span><br><span class="line">        <span class="keyword">this</span>.key2 = <span class="keyword">this</span>.newKey();</span><br><span class="line">        <span class="keyword">this</span>.buffer = <span class="keyword">new</span> DataInputBuffer();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">this</span>.key1 = <span class="keyword">this</span>.key2 = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">this</span>.buffer = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Driver</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setGroupingComparatorClass(OrderGroupingComparator.class);</span><br></pre></td></tr></table></figure><blockquote><p>运行测试</p></blockquote><p><img src="/images/hadoop/shuffle/grouping-result.png" alt="grouping result"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;排序，是 MapReduce 中最重要的操作之一。默认的排序方式是 &lt;code&gt;字典排序&lt;/code&gt;，且实现此排序的方式是 &lt;code&gt;快速排序&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;MapTask 和 MapReduce 均会对数据按照 &lt;code&gt;key&lt;/code&gt; 进行排序，该操作属于 Hadoop 的默认操作。任何程序中的数据均会被排序，不论逻辑上是否需要。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
      <category term="sort" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/sort/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
      <category term="sort" scheme="https://www.laiyy.top/tags/sort/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（13） Map Reduce &lt;BR /&gt; 工作流程、Shuffle</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/shuffle/hadoop-13.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/shuffle/hadoop-13.html</id>
    <published>2019-12-05T07:46:39.000Z</published>
    <updated>2019-12-05T07:46:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>在了解了常见的 InputFormat，及其处理分片的方式后，通过集成 FileInputFormat 自实现了一个自定义的 InputFormat，并通过自实现的 InputFormat，完成了一个对小文件的汇总合并工作。</p><p>那么此时，就需要深入了解一下 MapReduce 的具体工作流程</p><a id="more"></a><h1 id="MapReduce-工作流程"><a href="#MapReduce-工作流程" class="headerlink" title="MapReduce 工作流程"></a>MapReduce 工作流程</h1><ol><li>准备待处理文件</li><li>客户端进行 submit 之前，获取待处理数据的信息，根据参数配置，形成一份任务分配的规划（即 切片信息）</li><li>提交切片信息(job.split) 和 jar(集群模式下提交)、Job.xml</li><li>计算 MapTask 的数量（Yarn 中会先创建一个 MrAppMaster，根据 job.split 决定分配 MapTask 的数量）</li><li>执行 InputFormat 的 initialize 方法，获取文件、分片信息</li><li>执行 Mapper 操作</li><li>向环形缓冲区(默认 100M)中写入 KV 数据（日志中会打印 0%、50% 等信息）；缓冲区右侧是数据，左侧是数据的元数据（索引、位置、k-v 的起始位置等）。当缓冲区达到 80% 时，数据溢写到磁盘，且左右两侧数据清空，并反向写入数据。</li><li>进行分区、排序</li><li>溢出到文件（分区、且区内有序）</li><li>归并排序并合并文件（Reducer）</li><li>所有的 MapTask 任务完成后，启动 ReduceTask（数量由 MapTask 分区数量决定），并告知 ReduceTask 处理数据的范围（数据分区）</li><li>将 MapTask 处理后的数据下载到 ReduceTask 本地磁盘</li><li>将 ReduceTask 的文件进行归并排序，合并为一个文件后，进行 Reduce 操作</li><li>通过 OutputFormat 写出文件</li></ol><p><img src="/images/hadoop/map-reduce/mr-work-1.png" alt="MapReduce 工作流程"><br><img src="/images/hadoop/map-reduce/mr-work-2.png" alt="MapReduce 工作流程"></p><hr><h1 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h1><p>Map 方法之后，Reduce 方法之前的数据处理，称之为 Shuffle。此操作涉及：分区、排序、归并排序、数据压缩等。</p><p><img src="/images/hadoop/shuffle/shuffle.png" alt="shuffle"></p><h2 id="Partition-分区"><a href="#Partition-分区" class="headerlink" title="Partition 分区"></a>Partition 分区</h2><p>分区：将统计的结果，按照不同的条件，输出到不同的文件中。默认分区实现：<code>HashPartitioner</code>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由源码可知，默认的分区是根据 key 的HashCode 对 ReduceTasks 个数取模得到的。用户没有办法控制哪个 key 存储到哪个分区。</p><p>修改 WordCount 实例，增加配置： <code>job.setNumReduceTasks(2);</code>，再次运行。当 Mapper 进入 <code>context.write(key, value);</code> 时，将进行分区操作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// MapTask.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// collector 收集器；此处调用的是 MapOutputBuffer 的 collect 方法</span></span><br><span class="line">    collector.collect(key, value,</span><br><span class="line">                    <span class="comment">// 此处调用的 Partitioner 即为 HashPartitioner</span></span><br><span class="line">                    partitioner.getPartition(key, value, partitions));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行后查看输出目录，发现有 2 个文件，证明分区成功。<br><img src="/images/hadoop/shuffle/2-split.png" alt="2 split"><br><img src="/images/hadoop/shuffle/2-split-result.png" alt="2 split"></p><h2 id="自定义分区"><a href="#自定义分区" class="headerlink" title="自定义分区"></a>自定义分区</h2><p>实现步骤：</p><ol><li>自定义类，继承 <code>Partitioner</code>，重写 <code>getPartition</code> 方法</li><li>在 Job 驱动中，设置自定义的 Partitioner</li><li>根据自定义的 Partitioner 逻辑，设置相应数量的 ReduceTask</li></ol><p>需求：使用之前 <a href="/hadoop/map-reduce/hadoop-10.html#序列化-Demo">序列化实例</a> 的输入数据，实现 <code>按照手机号归属地不同，输出的不同的文件中</code>。</p><p>期望输出：如果手机号以 <code>偶数</code> 结尾，输入的一个文件，否则输出到不同文件。 根据文件内容，手机号以 <code>0、3、5、7、8</code> 结尾，则 0、8 输出到一个文件，其余的每个手机号一个文件。</p><hr><p><strong><em>注意：在使用自定义的 Partitioner 时，必须要指定 ReduceTask 的数量（setNumReduceTasks），否则只会输出一个文件，且所有数据都在这一个文件中！</em></strong><br><strong><em>如果指定的 ReduceTask 数量，小于 Partitioner 中的数量，则会出现 IO 异常，原因：无法确定输出结果用哪个 ReduceTask 输出。</em></strong><br><strong><em>如果指定的 ReduceTask 数量，大于 Partitioner 中的数量，不会报错，但是会出现几个空文件</em></strong><br><strong><em>分区号必须从0开始，逐一累加</em></strong></p><hr><blockquote><p>在之前<code>序列化实例</code>的基础上，进行修改</p></blockquote><ol><li>自定义 Partitioner</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBanPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// 注意：getPartition 只能从 0 开始。</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 如果手机号以 `偶数` 结尾，输入的一个文件，否则输出到不同文件</span></span><br><span class="line">        String phone = text.toString();</span><br><span class="line">        String key = phone.substring(<span class="number">9</span>, <span class="number">12</span>);</span><br><span class="line">        <span class="keyword">if</span> (key.equals(<span class="string">"885"</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (key.equals(<span class="string">"889"</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (key.equals(<span class="string">"883"</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (key.equals(<span class="string">"887"</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>修改 Driver</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改 Partitioner</span></span><br><span class="line">job.setPartitionerClass(FlowBeanPartitioner.class);</span><br><span class="line"><span class="comment">// 根据 Partitioner，设置 ReduceTask</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><ol><li>测试运行</li></ol><p><img src="/images/hadoop/shuffle/customer-split.png" alt="自定义分区"><br><img src="/images/hadoop/shuffle/customer-split-result.png" alt="自定义分区"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在了解了常见的 InputFormat，及其处理分片的方式后，通过集成 FileInputFormat 自实现了一个自定义的 InputFormat，并通过自实现的 InputFormat，完成了一个对小文件的汇总合并工作。&lt;/p&gt;
&lt;p&gt;那么此时，就需要深入了解一下 MapReduce 的具体工作流程&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
      <category term="shuffle" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/shuffle/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
      <category term="shuffle" scheme="https://www.laiyy.top/tags/shuffle/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（12） Map Reduce &lt;BR /&gt; MapReduce 框架原理：InputFormat（二）</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/input-format/hadoop-12.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/input-format/hadoop-12.html</id>
    <published>2019-12-05T02:29:37.000Z</published>
    <updated>2019-12-05T02:29:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>此前了解了 InputFormat 运行时，需要参考的 MapTask 并行度决定机制，以及任务提交的流程，那么接下来就需要深入分析 InputFormat 机制。</p><a id="more"></a><h1 id="FileInputFormat-切片机制"><a href="#FileInputFormat-切片机制" class="headerlink" title="FileInputFormat 切片机制"></a>FileInputFormat 切片机制</h1><h2 id="切片机制"><a href="#切片机制" class="headerlink" title="切片机制"></a>切片机制</h2><ol><li>简单的按照文件的内容长度进行切片</li><li>切片大小默认为 Block 大小</li><li>切片时不考虑数据集整体，而是针对每一个文件单独切片</li></ol><h2 id="源码中计算切片大小"><a href="#源码中计算切片大小" class="headerlink" title="源码中计算切片大小"></a>源码中计算切片大小</h2><p>Math.max(minSize, Math.min(maxSize, blockSize));</p><p>mapreduce.input.fileinputformat.split.minsize=1<br>mapreduce.input.fileinputformat.split.maxsize=Long.MAX_VALUE</p><p>基于此，默认情况下，切片大小 等于 blockSize</p><h2 id="切片大小的设置"><a href="#切片大小的设置" class="headerlink" title="切片大小的设置"></a>切片大小的设置</h2><p>maxsize：切片最大值，参数如果调得比 blockSize 小，则会让切片变小，而且就等于配置的这个参数的值。<br>minsize：切片最小值，如果调的比 blockSize 大，则可以让切片变得比 blockSize 还大</p><h2 id="获取切片信息的-API"><a href="#获取切片信息的-API" class="headerlink" title="获取切片信息的 API"></a>获取切片信息的 API</h2><p>inputSplit.gePath.getName()：获取切片的文件名称<br>(FileSplit)content.getInputSplit(); 根据文件类型获取切片信息</p><hr><h1 id="CombineTextInputFormat-切片机制"><a href="#CombineTextInputFormat-切片机制" class="headerlink" title="CombineTextInputFormat 切片机制"></a>CombineTextInputFormat 切片机制</h1><p>Hadoop 默认的 TextInputFormat 切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个 MapTask，这样如果有大量小文件，就会产生大量的 MapTask，处理效率极其低下。</p><p>CombineTextInputFormat 用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样多个小文件就可以交给一个 MapTask 处理。</p><h2 id="最大值设置"><a href="#最大值设置" class="headerlink" title="最大值设置"></a>最大值设置</h2><p>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); // 4 M</p><p>这个最大值的设置最好按照实际的小文件大小情况来设置。</p><h2 id="切片机制-1"><a href="#切片机制-1" class="headerlink" title="切片机制"></a>切片机制</h2><p>生成切片的过程包括：虚拟存储过程、切片过程 两步。</p><p>如：虚拟切片最大值为 4 M，现在有 4 个文件，分别为：a.txt(1.7M)、b.txt(5.1M)、c.txt(3.4M)、d.txt(6.8M)。</p><p>虚拟存储过程：</p><blockquote><p>a：1.7 &lt; 4，划分为 1 块（1.7M）<br>b：4 &lt; 5.1 &lt; 2<em>4；则划分为 2 块（2.55M，2.55M）<br>c：3.4 &lt; 4：划分为 1块（3.4M）<br>d：4 &lt; 6.8 &lt; 2</em>4：划分为 2块（3.4M，3.4M）</p></blockquote><p>切片过程：</p><blockquote><p>如果虚拟存储的文件大小大于设置好的最大值，则单独形成一个切片<br>否则跟下一个虚拟存储文件进行合并，共同形成一个切片</p></blockquote><p>所以最终会形成 3 个切片：(1.7 + 2.55)M、(2.55 + 3.4)M、(3.4 + 3.4)M</p><h2 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h2><p>准备 4 个测试文件：<a href="/file/hadoop/input-format/a.txt">a.txt(1.7M)</a>、<a href="/file/hadoop/input-format/b.txt">b.txt(5.1M)</a>、<a href="/file/hadoop/input-format/c.txt">c.txt(3.4M)</a>、<a href="/file/hadoop/input-format/d.txt">d.txt(6.8M)</a>。</p><p>期望：一个切片，处理4个文件。</p><h3 id="默认处理"><a href="#默认处理" class="headerlink" title="默认处理"></a>默认处理</h3><p>利用这个 4 个文件，运行 WordCount 实例，查看切片个数。</p><p><img src="/images/hadoop/map-reduce/default-split.png" alt="default split"></p><h3 id="设置虚拟存储"><a href="#设置虚拟存储" class="headerlink" title="设置虚拟存储"></a>设置虚拟存储</h3><blockquote><p>设置虚拟存储切片最大值为 4M</p></blockquote><p>在任务提交之前，增加配置：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置任务使用虚拟存储切片</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置虚拟存储切片最大值为 4 M</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br><span class="line"><span class="comment">// 提交 job</span></span><br><span class="line"><span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure></p><p><img src="/images/hadoop/map-reduce/3-split.png" alt="三片"></p><blockquote><p>设置虚拟存储切片最大值为 20M</p></blockquote><p>在任务提交之前，增加配置：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置任务使用虚拟存储切片</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置虚拟存储切片最大值为 20 M</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">20971520</span>);</span><br><span class="line"><span class="comment">// 提交 job</span></span><br><span class="line"><span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure></p><p><img src="/images/hadoop/map-reduce/1-split.png" alt="一片"></p><hr><h1 id="FileInputFormat-实现类"><a href="#FileInputFormat-实现类" class="headerlink" title="FileInputFormat 实现类"></a>FileInputFormat 实现类</h1><p>在运行 MapReduce 程序时，输入的文件包括：基于行的日志文件、二进制格式文件、数据库表等 。针对不同的数据类型，MapReduce 如何读取数据？</p><p>FileInputFormat 常见实现：TextInputFormat(文本文件)，KeyValueTextInputFormat（基于 KV 的文本文件），NLineInputFormat（按行处理）、CombineTextInputFormat（小文件处理）、自定义 InputFormat。</p><p><img src="/images/hadoop/map-reduce/first-sub-class.png" alt="实现类"></p><h2 id="TextInputFormat"><a href="#TextInputFormat" class="headerlink" title="TextInputFormat"></a>TextInputFormat</h2><p>TextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。key 是该行在整个文件中的字节偏移量（LongWritable 类型）；value 是读取到的该行内容（不包括任何终止符，Text 类型）。</p><h2 id="KeyValueTextInputFormat"><a href="#KeyValueTextInputFormat" class="headerlink" title="KeyValueTextInputFormat"></a>KeyValueTextInputFormat</h2><p>每一行为一条记录，被分隔符分隔为 key、value。可以通过 <code>configuration.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, &quot;\t&quot;);</code> 来设定分隔符，默认分隔符是 <code>\t</code>。<br>此时的 key 是每行排在分隔符之前的 Text 序列。</p><h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><p>需求：统计输入文件中，每一行的第一个单词相同的行数。参考文件：<a href="/file/hadoop/input-format/key-value.txt">key-value.txt</a></p><p>如：输入数据格式为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">laiyy#dahe.cn like</span><br><span class="line">liyl#dahe.cn like</span><br><span class="line">laiyy#sina.com.cn hate</span><br><span class="line">laiyy#study hadoop</span><br><span class="line">liyl#study hadoop</span><br></pre></td></tr></table></figure></p><p>期望输出为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">laiyy 3</span><br><span class="line">liyl 2</span><br></pre></td></tr></table></figure></p><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"当前行的 key ："</span> + key + <span class="string">" ---&gt; 当前行的 value："</span> + value);</span><br><span class="line">    context.write(key, outValue);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">        sum += value.get();</span><br><span class="line">    &#125;</span><br><span class="line">    outValue.set(sum);</span><br><span class="line">    context.write(key, outValue);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Driver</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取 job 对象</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置分隔符</span></span><br><span class="line">    configuration.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, <span class="string">"#"</span>);</span><br><span class="line"></span><br><span class="line">    Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置使用 KeyValue 形式的 InputFormat</span></span><br><span class="line">    job.setInputFormatClass(KeyValueTextInputFormat.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>检查输出结果</p></blockquote><p><img src="/images/hadoop/map-reduce/kv-result.png" alt="kv 输出结果"></p><h2 id="NLineInputFormat"><a href="#NLineInputFormat" class="headerlink" title="NLineInputFormat"></a>NLineInputFormat</h2><p>如果使用 NLineInputFormat，代表每个 map 进程处理的 InputSplit 不再按照 Block 去划分，而是按照 NLineInputFormat 指定的行数来划分。<br>即：<code>输入文件的总行数/N=切片数</code>。如果不能整除，切片数为 <code>商+1</code>。</p><p>如：使用 hadoop 的 <a href="/file/hadoop/map-reduce/README.txt">README.txt</a> 文件作为输入，如果 N 为 5，则每个输入分片包括 5 行；文件总共 32 行，则应该有 7 个分片。</p><h3 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h3><p>使用 WordCount 实例作为测试代码，对每个单词进行个数统计。根据输入文件的行数来规定输出几个切片。此案例要求每 5 行放入一个切片。</p><p>只需要在 WordCount 的 Driver 中，Job 提交之前，加入下列代码即可。Mapper、Reducer 都不需要变动。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置为 NLineInputFormat</span></span><br><span class="line">job.setInputFormatClass(NLineInputFormat.class);</span><br><span class="line"><span class="comment">// 5 行一个切片</span></span><br><span class="line">NLineInputFormat.setNumLinesPerSplit(job, <span class="number">5</span>);</span><br></pre></td></tr></table></figure></p><p><img src="/images/hadoop/map-reduce/nline.png" alt="NLineInputFormat split"></p><hr><h1 id="自定义-InputFormat"><a href="#自定义-InputFormat" class="headerlink" title="自定义 InputFormat"></a>自定义 InputFormat</h1><p>步骤：</p><blockquote><p>自定义一个类，集成 FileInputFormat<br>改写 RecordReader，实现一次读取一个完整文件，封装为 KV<br>在输出的时候，使用 SequenceFileOutputFormat 输出合并文件</p></blockquote><p>无论是 HDFS 还是 MapReduce，在处理小文件时，效率都非常低，但是又难免面临大量小文件的场景。此时，可以使用自定义 InputFormat 实现小文件的合并。</p><blockquote><p>需求</p></blockquote><p>将多个小文件，合并为一个 SequenceFile 文件（Hadoop 用来存储二进制形式的 k-v 对的文件格式），SequenceFile 中存储着多着文件，存储形式为 <code>文件路径 + 名称 为 key，内容为 value</code></p><p>准备三个小文件：<a href="/file/hadoop/input-format/sf_1.txt">sf_1.txt</a>，<a href="/file/hadoop/input-format/sf_2.txt">sf_2.txt</a>，<a href="/file/hadoop/input-format/sf_3.txt">sf_3.txt</a></p><h2 id="自定义-InputFormat-1"><a href="#自定义-InputFormat-1" class="headerlink" title="自定义 InputFormat"></a>自定义 InputFormat</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Text：key，存储的是文件路径+名称</span></span><br><span class="line"><span class="comment"> * BytesWritable：value，存储的是整个文件的字节流</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerInputFormat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordReader&lt;Text, BytesWritable&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        CustomerRecordReader recordReader = <span class="keyword">new</span> CustomerRecordReader();</span><br><span class="line">        <span class="comment">// 初始化</span></span><br><span class="line">        recordReader.initialize(split, context);</span><br><span class="line">        <span class="keyword">return</span> recordReader;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="实现-RecordReader"><a href="#实现-RecordReader" class="headerlink" title="实现 RecordReader"></a>实现 RecordReader</h2><blockquote><p>自定义实现 RecordReader，实现一次读取一个完整文件，封装为 KV</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 切片</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> FileSplit fileSplit;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 配置信息</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Configuration configuration;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 输出的 key（路径 + 名称）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Text key = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 输出的 value（整个文件内容）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> BytesWritable value = <span class="keyword">new</span> BytesWritable();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 标识是否正在读取</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> progressing = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> split 切片</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context 上下文</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.fileSplit = (FileSplit) split;</span><br><span class="line">        configuration = context.getConfiguration();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 核心业务逻辑</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (progressing) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取 fs 对象</span></span><br><span class="line">            Path path = fileSplit.getPath();</span><br><span class="line">            FileSystem fileSystem = path.getFileSystem(configuration);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取输入流</span></span><br><span class="line">            FSDataInputStream inputStream = fileSystem.open(path);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 封装 key</span></span><br><span class="line">            key.set(path.toString());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 拷贝，将文件内容拷贝到 buffer 中</span></span><br><span class="line">            <span class="keyword">byte</span>[] buffer = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>) fileSplit.getLength()];</span><br><span class="line">            IOUtils.readFully(inputStream, buffer, <span class="number">0</span>, buffer.length);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 封装 value</span></span><br><span class="line">            value.set(buffer, <span class="number">0</span>, buffer.length);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 关闭资源</span></span><br><span class="line">            IOUtils.closeStream(inputStream);</span><br><span class="line"></span><br><span class="line">            progressing = <span class="keyword">false</span>;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取当前的 key</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> key;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> BytesWritable <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取处理进度</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 关闭资源</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, BytesWritable value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(key, value);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;BytesWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 循环写出</span></span><br><span class="line">        <span class="keyword">for</span> (BytesWritable value : values) &#123;</span><br><span class="line">            context.write(key, value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Driver</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 设置 InputFormat、OutputFormat</span></span><br><span class="line">    job.setInputFormatClass(CustomerInputFormat.class);</span><br><span class="line">    job.setOutputFormatClass(SequenceFileOutputFormat.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="执行结果"><a href="#执行结果" class="headerlink" title="执行结果"></a>执行结果</h2><p><img src="/images/hadoop/map-reduce/sf-result.png" alt="执行结果"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此前了解了 InputFormat 运行时，需要参考的 MapTask 并行度决定机制，以及任务提交的流程，那么接下来就需要深入分析 InputFormat 机制。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
      <category term="input-format" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/input-format/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
      <category term="input-format" scheme="https://www.laiyy.top/tags/input-format/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（11） Map Reduce &lt;BR /&gt; MapReduce 框架原理：InputFormat（一）&lt;BR/&gt; MapTask 并行度决定机制</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/input-format/hadoop-11.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/input-format/hadoop-11.html</id>
    <published>2019-12-04T08:17:08.000Z</published>
    <updated>2019-12-04T08:17:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>在了解了 Hadoop 的序列化操作，实现了基本的 Bean 序列化的一个 demo，接下来分析一下 MapReduce 的框架原理。</p><a id="more"></a><h1 id="切片与MapTask-并行度决定机制"><a href="#切片与MapTask-并行度决定机制" class="headerlink" title="切片与MapTask 并行度决定机制"></a>切片与MapTask 并行度决定机制</h1><p>MapTask 的并行度决定 Map 阶段的任务处理并发度，进而影响整个 Job 的处理速度。</p><p>问题：</p><blockquote><p>一个 1G 的数据，启动 8 个MapTask，可以提高集群的并发处理能力。但是如果是一个 1K 的数据，也启动 8 个MapTask，会提高性能吗？<br>MapTask 是否越多越好？<br>什么因素会影响到 MapTask 的并行度？</p></blockquote><hr><h1 id="MapTask并行度决定机制"><a href="#MapTask并行度决定机制" class="headerlink" title="MapTask并行度决定机制"></a>MapTask并行度决定机制</h1><p>前置概念：</p><blockquote><p>数据块：Block 在 HDFS 物理上把数据分成一块一块的。<br>数据切片：在逻辑上对输入进行分片，并不会在磁盘上将其分片存储。</p></blockquote><p>现在，假设有一个 300M 的数据，分别存放在 DataNode 1、2、3 上，DataNode1 上存储 0~128M 数据，DataNode2 上存储 128~256M 数据，DataNode3 上存储 256~300M 数据。<br>如果数据切片大小为 100M，则读取第一个切片没有问题，当读取第2、3个切片时，需要将DataNode1 上的剩余的数据拷贝到 MapTask2 上，将 DataNode2 上剩余的数据拷贝到 MapTask3 上，这样会存在大量的数据 IO，严重影响性能。</p><p><img src="/images/hadoop/map-reduce/100-split.png" alt="切片大小为 100M"></p><p>如果数据切片大小为 128M（即与 Block 大小一致），此时，每个 MapTask 都读取 128M 数据，则可以分别运行在三台 DataNode 上，没有数据拷贝，此时性能最高。</p><blockquote><p>MapTask 并行度决定机制</p></blockquote><ol><li>一个 Job 的 Map 阶段并行度由客户端在提交 Job 时的切片数决定</li><li>每个切片分配一个 MapTask 并行实例处理</li><li>默认情况下，切片大小等于 BlockSize</li><li>切片时不考虑数据集整体，而是逐个针对每个文件单独切片</li></ol><hr><h1 id="Job-提交流程、切片源码"><a href="#Job-提交流程、切片源码" class="headerlink" title="Job 提交流程、切片源码"></a>Job 提交流程、切片源码</h1><p>在 Job 调用 <code>job.waitForCompletion</code> 时，进行任务提交。此方法会调用 <code>submit()</code> 方法进行真正的提交。</p><h2 id="任务提交流程"><a href="#任务提交流程" class="headerlink" title="任务提交流程"></a>任务提交流程</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">waitForCompletion</span><span class="params">(<span class="keyword">boolean</span> verbose</span></span></span><br><span class="line"><span class="function"><span class="params">                                )</span> <span class="keyword">throws</span> IOException, InterruptedException,</span></span><br><span class="line"><span class="function">                                        ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">        <span class="comment">// 调用真正的提交</span></span><br><span class="line">        submit();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (verbose) &#123;</span><br><span class="line">        <span class="comment">// 打印日志</span></span><br><span class="line">        monitorAndPrintJob();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 忽略</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> isSuccessful();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">submit</span><span class="params">()</span> </span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="comment">// 判断任务状态</span></span><br><span class="line">    ensureState(JobState.DEFINE);</span><br><span class="line">    <span class="comment">// 将老旧的 API 转换为新的 API，为兼容性考虑</span></span><br><span class="line">    setUseNewAPI();</span><br><span class="line">    <span class="comment">// 连接</span></span><br><span class="line">    connect();</span><br><span class="line">    <span class="keyword">final</span> JobSubmitter submitter = </span><br><span class="line">        getJobSubmitter(cluster.getFileSystem(), cluster.getClient());</span><br><span class="line">    status = ugi.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> JobStatus <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, </span></span><br><span class="line"><span class="function">        ClassNotFoundException </span>&#123;</span><br><span class="line">            <span class="comment">// 提交任务的详细信息</span></span><br><span class="line">            <span class="keyword">return</span> submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    state = JobState.RUNNING;</span><br><span class="line">    LOG.info(<span class="string">"The url to track the job: "</span> + getTrackingURL());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="connect-连接流程"><a href="#connect-连接流程" class="headerlink" title="connect 连接流程"></a>connect 连接流程</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">connect</span><span class="params">()</span></span></span><br><span class="line"><span class="function">          <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (cluster == <span class="keyword">null</span>) &#123;</span><br><span class="line">      cluster = </span><br><span class="line">        ugi.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;Cluster&gt;() &#123;</span><br><span class="line">                   <span class="function"><span class="keyword">public</span> Cluster <span class="title">run</span><span class="params">()</span></span></span><br><span class="line"><span class="function">                          <span class="keyword">throws</span> IOException, InterruptedException, </span></span><br><span class="line"><span class="function">                                 ClassNotFoundException </span>&#123;</span><br><span class="line">                     <span class="comment">// 创建一个新的 Cluster</span></span><br><span class="line">                     <span class="keyword">return</span> <span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">                   &#125;</span><br><span class="line">                 &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Cluster</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="keyword">null</span>, conf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Cluster</span><span class="params">(InetSocketAddress jobTrackAddr, Configuration conf)</span> </span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.conf = conf;</span><br><span class="line">    <span class="keyword">this</span>.ugi = UserGroupInformation.getCurrentUser();</span><br><span class="line">    <span class="comment">// Cluster 初始化</span></span><br><span class="line">    initialize(jobTrackAddr, conf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InetSocketAddress jobTrackAddr, Configuration conf)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">synchronized</span> (frameworkLoader) &#123;</span><br><span class="line">        <span class="keyword">for</span> (ClientProtocolProvider provider : frameworkLoader) &#123;</span><br><span class="line">        ClientProtocol clientProtocol = <span class="keyword">null</span>; </span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (jobTrackAddr == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// 创建一个 LocalJobRunner（在本地运行时）</span></span><br><span class="line">                clientProtocol = provider.create(conf);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 创建一个 YARNRunner（在集群运行时）</span></span><br><span class="line">                clientProtocol = provider.create(jobTrackAddr, conf);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 省略</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="实际提交流程"><a href="#实际提交流程" class="headerlink" title="实际提交流程"></a>实际提交流程</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">JobStatus <span class="title">submitJobInternal</span><span class="params">(Job job, Cluster cluster)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> ClassNotFoundException, InterruptedException, IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 校验输出路径</span></span><br><span class="line">    checkSpecs(job);</span><br><span class="line"></span><br><span class="line">    Configuration conf = job.getConfiguration();</span><br><span class="line">    <span class="comment">// 缓存处理</span></span><br><span class="line">    addMRFrameworkToDistributedCache(conf);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 任务临时目录， tmp/hadoop-Administrator\mapred\staging，每次运行任务都会在这个目录下创建一个文件夹，将所需数据都保存在内</span></span><br><span class="line">    <span class="comment">// 当任务执行结束后，会删除文件夹内的所有数据</span></span><br><span class="line">    Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 获取网络 ip</span></span><br><span class="line">    InetAddress ip = InetAddress.getLocalHost();</span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成一个唯一的 jobId</span></span><br><span class="line">    JobID jobId = submitClient.getNewJobID();</span><br><span class="line">    job.setJobID(jobId);</span><br><span class="line">    Path submitJobDir = <span class="keyword">new</span> Path(jobStagingArea, jobId.toString());</span><br><span class="line">    JobStatus status = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交文件的信息到之前创建的文件夹下（本机下不会提交）</span></span><br><span class="line">        copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line"></span><br><span class="line">        Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 写入切片信息到文件夹</span></span><br><span class="line">        <span class="keyword">int</span> maps = writeSplits(job, submitJobDir);</span><br><span class="line">        conf.setInt(MRJobConfig.NUM_MAPS, maps);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写入任务信息到文件夹</span></span><br><span class="line">        writeConf(conf, submitJobFile);</span><br><span class="line">        </span><br><span class="line">        printTokens(jobId, job.getCredentials());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交完成后，删除文件夹内信息</span></span><br><span class="line">        status = submitClient.submitJob(</span><br><span class="line">            jobId, submitJobDir.toString(), job.getCredentials());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 省略</span></span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="comment">// 省略</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/images/hadoop/map-reduce/job-staging.png" alt="Hadoop 任务临时路径"><br><img src="/images/hadoop/map-reduce/split-file.png" alt="hadoop 临时切片文件"><br><img src="/images/hadoop/map-reduce/job-submit.png" alt="hadoop Job 提交流程"></p><h2 id="切片流程"><a href="#切片流程" class="headerlink" title="切片流程"></a>切片流程</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">writeSplits</span><span class="params">(org.apache.hadoop.mapreduce.JobContext job,</span></span></span><br><span class="line"><span class="function"><span class="params">    Path jobSubmitDir)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">    JobConf jConf = (JobConf)job.getConfiguration();</span><br><span class="line">    <span class="keyword">int</span> maps;</span><br><span class="line">    <span class="keyword">if</span> (jConf.getUseNewMapper()) &#123;</span><br><span class="line">        <span class="comment">// 使用新的切片规则</span></span><br><span class="line">        maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 使用旧切片规则</span></span><br><span class="line">        maps = writeOldSplits(jConf, jobSubmitDir);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> maps;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;T extends InputSplit&gt; <span class="function"><span class="keyword">int</span> <span class="title">writeNewSplits</span><span class="params">(JobContext job, Path jobSubmitDir)</span> <span class="keyword">throws</span> IOException,</span></span><br><span class="line"><span class="function">        InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取配置信息</span></span><br><span class="line">    Configuration conf = job.getConfiguration();</span><br><span class="line">    InputFormat&lt;?, ?&gt; input =</span><br><span class="line">        ReflectionUtils.newInstance(job.getInputFormatClass(), conf);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取切片信息</span></span><br><span class="line">    List&lt;InputSplit&gt; splits = input.getSplits(job);</span><br><span class="line">    T[] array = (T[]) splits.toArray(<span class="keyword">new</span> InputSplit[splits.size()]);</span><br><span class="line"></span><br><span class="line">    Arrays.sort(array, <span class="keyword">new</span> SplitComparator());</span><br><span class="line">    JobSplitWriter.createSplitFiles(jobSubmitDir, conf, </span><br><span class="line">        jobSubmitDir.getFileSystem(conf), array);</span><br><span class="line">    <span class="keyword">return</span> array.length;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 此处调用的是 FileInputFormat 中的 getSplits</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    StopWatch sw = <span class="keyword">new</span> StopWatch().start();</span><br><span class="line">    <span class="keyword">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">    <span class="keyword">long</span> maxSize = getMaxSplitSize(job);</span><br><span class="line"></span><br><span class="line">    List&lt;InputSplit&gt; splits = <span class="keyword">new</span> ArrayList&lt;InputSplit&gt;();</span><br><span class="line">    <span class="comment">// 文件信息</span></span><br><span class="line">    List&lt;FileStatus&gt; files = listStatus(job);</span><br><span class="line">    <span class="comment">// 按照文件，一个一个切片</span></span><br><span class="line">    <span class="keyword">for</span> (FileStatus file: files) &#123;</span><br><span class="line">        Path path = file.getPath();</span><br><span class="line">        <span class="keyword">long</span> length = file.getLen();</span><br><span class="line">        <span class="keyword">if</span> (length != <span class="number">0</span>) &#123;</span><br><span class="line">        BlockLocation[] blkLocations;</span><br><span class="line">        <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span><br><span class="line">            blkLocations = ((LocatedFileStatus) file).getBlockLocations();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            FileSystem fs = path.getFileSystem(job.getConfiguration());</span><br><span class="line">            blkLocations = fs.getFileBlockLocations(file, <span class="number">0</span>, length);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 判断是否可切割</span></span><br><span class="line">        <span class="keyword">if</span> (isSplitable(job, path)) &#123;</span><br><span class="line">            <span class="comment">// 获取块大小（如果是 local 运行：2.x 32 M，1.x 64 M，yarn 集群：128M，）</span></span><br><span class="line">            <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">            <span class="comment">// 获取切片大小</span></span><br><span class="line">            <span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">long</span> bytesRemaining = length;</span><br><span class="line">            <span class="comment">// 如果当前文件大小 / 切片大小 &gt; 1.1，进入此方法进行切片</span></span><br><span class="line">            <span class="keyword">while</span> (((<span class="keyword">double</span>) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class="line">                <span class="comment">// 重新计算切片开始位置</span></span><br><span class="line">                <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class="line">                <span class="comment">// 添加切片</span></span><br><span class="line">                splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span><br><span class="line">                            blkLocations[blkIndex].getHosts(),</span><br><span class="line">                            blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">                bytesRemaining -= splitSize;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class="line">                <span class="comment">// 添加切片</span></span><br><span class="line">                splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,</span><br><span class="line">                            blkLocations[blkIndex].getHosts(),</span><br><span class="line">                            blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// not splitable</span></span><br><span class="line">            splits.add(makeSplit(path, <span class="number">0</span>, length, blkLocations[<span class="number">0</span>].getHosts(),</span><br><span class="line">                        blkLocations[<span class="number">0</span>].getCachedHosts()));</span><br><span class="line">        &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">            <span class="comment">//Create empty hosts array for zero length files</span></span><br><span class="line">            splits.add(makeSplit(path, <span class="number">0</span>, length, <span class="keyword">new</span> String[<span class="number">0</span>]));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Save the number of input files for metrics/loadgen</span></span><br><span class="line">    job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());</span><br><span class="line">    sw.stop();</span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">        LOG.debug(<span class="string">"Total # of splits generated by getSplits: "</span> + splits.size()</span><br><span class="line">            + <span class="string">", TimeTaken: "</span> + sw.now(TimeUnit.MILLISECONDS));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> splits;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>先创建一个数据存储的临时目录</li><li>开始规划切片，遍历处理目录下的每个文件</li><li>遍历文件：<blockquote><p>获取文件大小<br>计算切片大小，公式： Math.max(minSize, Math.min(maxSize, blockSize))<br>默认情况下，切片大小 = blockSize<br>开始切片：local 运行（第一个切片 0~32M，第二个切片 32~64M …）；Yarn 运行（第一个切片 0~128M，第二个切片 128~256M …）；注意：每次切片时，都需要判断切片完成后剩余部分是否是块大小的 1.1 倍，大于就切片，否则不切<br>将切片信息写入切片规划文件<br>InputSplit 只记录切片的元数据信息（起始位置、长度、所在节点列表等）</p></blockquote></li><li>提交切片规划文件（local 运行时为临时目录，集群运行时为 yarn）；Yarn 上的 MrAppMaster 根据切片规划文件计算开启 MapTask 个数。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在了解了 Hadoop 的序列化操作，实现了基本的 Bean 序列化的一个 demo，接下来分析一下 MapReduce 的框架原理。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
      <category term="input-format" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/input-format/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
      <category term="input-format" scheme="https://www.laiyy.top/tags/input-format/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（10） Map Reduce &lt;BR /&gt;  序列化</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/hadoop-10.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/hadoop-10.html</id>
    <published>2019-12-04T06:17:08.000Z</published>
    <updated>2019-12-04T06:17:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>在 MapReduce 的 <a href="/hadoop/map-reduce/hadoop-9.html#Hadoop-数据序列化类型">数据序列化类型</a> 中，介绍了几种常见的 Hadoop 序列化类，实现了一个基础的 <code>WordCount</code> Demo，使用到了 Long、String、Integer 对应的序列化类，那么接下来就需要了解一下 Hadoop 具体的怎么序列化的。</p><a id="more"></a><h1 id="Hadoop-序列化"><a href="#Hadoop-序列化" class="headerlink" title="Hadoop 序列化"></a>Hadoop 序列化</h1><h2 id="序列化概述"><a href="#序列化概述" class="headerlink" title="序列化概述"></a>序列化概述</h2><blockquote><p>什么是序列化、反序列化</p></blockquote><p>序列化：就是把内存中的对象，转换为<code>字节序列</code> 或其他数据传输协议，以便存储到磁盘或网络传输。<br>反序列化：就是将<code>收到的字节序列</code>或<code>其他数据传输协议</code>或<code>磁盘的持久化数据</code>，转换成内存中的对象。</p><blockquote><p>为什么要序列化？</p></blockquote><p>一般来说，<code>对象</code> 只能生存在内存中，断电即消失。而且，<code>对象</code> 只能由本地进程使用，不能被发送到网络上的另外一台计算机中。<br>然而，<code>序列化</code> 可以存储 <code>对象</code>，且可以将对象 <code>发送到远程计算机</code>。</p><blockquote><p>为什么不用 Java 自身的序列化？</p></blockquote><p>Java 的序列化是一个重量级的框架(Serializable)，一个对象被序列化后，别额外附带很多信息，如：校验信息、Header、继承体系等，不便于在网络中高效传输。基于此，Hadoop 开发了一套属于自己的序列化机制：Writable。</p><blockquote><p>Hadoop 序列化的特点</p></blockquote><ol><li>紧凑：高效使用存储空间</li><li>快速：读写数据的额外开销小</li><li>可扩展：随着通信协议的升级而升级</li><li>互操作：支持多语言交互</li></ol><h2 id="自定义实现序列化"><a href="#自定义实现序列化" class="headerlink" title="自定义实现序列化"></a>自定义实现序列化</h2><p>实现步骤：</p><ol><li>实现 Writable 接口</li><li>反序列化时，需要反射调用空参构造函数</li><li>重写序列化方法</li><li>重写反序列化 方法</li><li>反序列化的顺序和序列化的顺序保持一致</li><li>重写 toString</li><li>实现 Comparable 接口（MapReduce 的 Shuffle 过程要求对 key 必须能排序；当需要排序的时候才做）</li></ol><hr><h1 id="序列化-Demo"><a href="#序列化-Demo" class="headerlink" title="序列化 Demo"></a>序列化 Demo</h1><p>需求：根据 <a href="/file/hadoop/map-reduce/phone.txt">测试文件</a>，统计每个手机号的<code>上行流量</code>、<code>下行流量</code>、<code>总流量</code>。<br>文件中，倒数第三列为上行流量，倒数第二列为下行流量，最后一列为网络请求状态码。</p><h2 id="创建统计流量的-Bean-对象"><a href="#创建统计流量的-Bean-对象" class="headerlink" title="创建统计流量的 Bean 对象"></a>创建统计流量的 Bean 对象</h2><p>创建一个统计流量的 Bean 对象，并实现序列化操作</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 上行流量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 下行流量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 总流量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 空参构造，反射用</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = upFlow + downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">"\t"</span> + downFlow + <span class="string">"\t"</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略 get、set</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 序列化</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> dataOutput 输入输出</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException 可能异常</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 反序列化</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> dataInput 输入数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException 可能异常</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 必须和序列化方法顺序一致</span></span><br><span class="line">        upFlow = dataInput.readLong();</span><br><span class="line">        downFlow = dataInput.readLong();</span><br><span class="line">        sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="MapReduce-程序"><a href="#MapReduce-程序" class="headerlink" title="MapReduce 程序"></a>MapReduce 程序</h2><h3 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 1   17319758889 192.168.100.1   www.baidu.com 2481    24685   200</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">// 2. 切割</span></span><br><span class="line">        String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="comment">// 3. 封装对象</span></span><br><span class="line">        outKey.set(fields[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> length = fields.length;</span><br><span class="line">        <span class="keyword">long</span> upFlow = Long.parseLong(fields[length - <span class="number">3</span>]);</span><br><span class="line">        <span class="keyword">long</span> downFlow = Long.parseLong(fields[length - <span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">        flowBean.setUpFlow(upFlow);</span><br><span class="line">        flowBean.setDownFlow(downFlow);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 写出</span></span><br><span class="line">        context.write(outKey, flowBean);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Reducer"><a href="#Reducer" class="headerlink" title="Reducer"></a>Reducer</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 两个相同的手机号的访问记录</span></span><br><span class="line">        <span class="comment">// 11   17319788888 192.168.100.11   www.java1234.com 231    28   200</span></span><br><span class="line">        <span class="comment">// 12   17319788888 192.168.100.12    211    7852   200</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> sumUpFlow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">long</span> sumDownFlow = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FlowBean value : values) &#123;</span><br><span class="line">            sumUpFlow += value.getUpFlow();</span><br><span class="line">            sumDownFlow += value.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        flowBean.set(sumUpFlow, sumDownFlow);</span><br><span class="line">        context.write(key, flowBean);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Deiver"><a href="#Deiver" class="headerlink" title="Deiver"></a>Deiver</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取 job 对象</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置 jar 存放路径</span></span><br><span class="line">    job.setJarByClass(FlowCountDriver.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关联 Mapper、Reducer 业务类</span></span><br><span class="line">    job.setMapperClass(FlowCountMapper.class);</span><br><span class="line">    job.setReducerClass(FlowCountReducer.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 Mapper 输出的 KV 类型</span></span><br><span class="line">    job.setMapOutputKeyClass(Text.class);</span><br><span class="line">    job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定最终输出的数据 KV 类型</span></span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 job 的输入文件所在目录</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 job 的输出结果所在目录</span></span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提交 job</span></span><br><span class="line"><span class="comment">//        job.submit();</span></span><br><span class="line">    <span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    System.exit(succeed ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="运行测试"><a href="#运行测试" class="headerlink" title="运行测试"></a>运行测试</h2><p>设置输入输出路径：<br><img src="/images/hadoop/map-reduce/flow-count-args.png" alt="输入输出路径"></p><p>查看输出结果：<br><img src="/images/hadoop/map-reduce/flow-count-result.png" alt="flow count result"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 MapReduce 的 &lt;a href=&quot;/hadoop/map-reduce/hadoop-9.html#Hadoop-数据序列化类型&quot;&gt;数据序列化类型&lt;/a&gt; 中，介绍了几种常见的 Hadoop 序列化类，实现了一个基础的 &lt;code&gt;WordCount&lt;/code&gt; Demo，使用到了 Long、String、Integer 对应的序列化类，那么接下来就需要了解一下 Hadoop 具体的怎么序列化的。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（9） Map Reduce &lt;BR /&gt;  基础概念，WordCount Demo 实现</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/hadoop-9.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/hadoop-9.html</id>
    <published>2019-12-02T07:37:26.000Z</published>
    <updated>2019-12-02T07:37:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS、MapReduce、Yarn 是 Hadoop 的三大模块，其中，HDFS 负责存储，MapReduce 负责计算，Yarn 负责资源调度</p><a id="more"></a><h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><p>MapReduce 是一个 <code>分布式运算程序的编程框架</code>，是用户开发 <code>基于Hadoop的数据分析应用</code>的核心框架。</p><p>MapReduce 的核心功能，是将 用户编写的业务逻辑代码，和自带的默认组件，整合成一个完整的分布式运算程序，并发的运行在一个 Hadoop 集群上。</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><blockquote><p>易于编程</p></blockquote><p>简单地实现一些接口，就可以完成一个分布式程序。这个分布式程序可以分布到大量的廉价 pc 机器上运行。也就是说，写一个分布式程序和写一个串行程序一模一样。 就是因为这个特点，使得MapReduce编程变得非常流行。</p><blockquote><p>良好的扩展性</p></blockquote><p>当计算资源不能得到满足时，可以通过简单的扩展机器来扩展计算能力。</p><blockquote><p>高容错性</p></blockquote><p>MapReduce 设计初衷，就是使程序能够部署在廉价的 pc 机器上，这就要求它具有很高的容错性。<br>如：一台机器挂掉了，它可以把上面的计算任务转移到另外一个节点上，不至于这个任务运行失败，而且这个过程不需要人工参与，完全由 Hadoop 内部完成。</p><blockquote><p>适合 PB 级以上海量数据离线处理</p></blockquote><p>可以实现上千台服务器集群并发工作，提供数据处理能力。</p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><blockquote><p>不擅长实时计算</p></blockquote><p>无法像 MySQL 一样，在毫秒级或秒级内返回结果</p><blockquote><p>不擅长流式计算</p></blockquote><p>流式计算的输入数据是动态的，而 MapReduce 的输入数据集的静态的，不能动态变化。这是由 MapReduce 自身的设计特点决定的。</p><blockquote><p>不擅长 DAG(有向图) 计算</p></blockquote><p>有向图：多个应用程序存在依赖关系，后一个程序的输入是前一个程序的输出。<br>MapReduce 可以做 DAG 计算，但是不推荐。原因的每个 MapReduce 的输出结果都会写入磁盘，会造成大量的磁盘 IO，导致性能低下。</p><h2 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h2><p>MapReduce 运算程序一般分为两个阶段：Map 阶段(分),Reduce 阶段(合)。<br>Map 阶段的兵法 MapTask，完全并发运行，互不相干。<br>Reduce 阶段的并发 ReduceTask，完全并发运行，互补相干。但是它们的数据依赖于上一阶段的所有 MapTask 并发实例的输出。<br>MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce 阶段，如果业务逻辑复杂，只能多个 MapReduce 程序串行运行。</p><p>例如：<br><img src="/images/hadoop/map-reduce/map-reduce.png" alt="MapReduce思想"></p><h2 id="MapReduce-进程"><a href="#MapReduce-进程" class="headerlink" title="MapReduce 进程"></a>MapReduce 进程</h2><p>一个完整的 MapReduce 程序在分布式运行时有三类实例进程：</p><blockquote><p>MrAppMaster：负责整个程序的过程调度以及正太协调<br>MapTask：负责 Map 阶段的整个数据处理流程<br>ReduceTask：负责 Reduce 阶段的整个数据处理流程</p></blockquote><h2 id="Hadoop-数据序列化类型"><a href="#Hadoop-数据序列化类型" class="headerlink" title="Hadoop 数据序列化类型"></a>Hadoop 数据序列化类型</h2><table><thead><tr><th style="text-align:center">Java 类型</th><th style="text-align:center">Hadoop Writable 类型</th></tr></thead><tbody><tr><td style="text-align:center">boolean</td><td style="text-align:center">BooleanWritable</td></tr><tr><td style="text-align:center">byte</td><td style="text-align:center">ByteWritable</td></tr><tr><td style="text-align:center">int</td><td style="text-align:center">IntWritable</td></tr><tr><td style="text-align:center">float</td><td style="text-align:center">FloatWritable</td></tr><tr><td style="text-align:center">long</td><td style="text-align:center">LongWritable</td></tr><tr><td style="text-align:center">double</td><td style="text-align:center">DoubleWritable</td></tr><tr><td style="text-align:center">String</td><td style="text-align:center">Text</td></tr><tr><td style="text-align:center">Map</td><td style="text-align:center">MapWritable</td></tr><tr><td style="text-align:center">array</td><td style="text-align:center">ArrayWritable</td></tr></tbody></table><h2 id="MapReduce-编程规范"><a href="#MapReduce-编程规范" class="headerlink" title="MapReduce 编程规范"></a>MapReduce 编程规范</h2><p>MapReduce 编程分为三个部分： Mapper、Reducer、Driver</p><h3 id="Mapper-阶段"><a href="#Mapper-阶段" class="headerlink" title="Mapper 阶段"></a>Mapper 阶段</h3><ol><li>用户自定义的 Mapper 要继承自己的父类</li><li>Mapper 的数据数据是 KV 对形式，KV 类型自定义</li><li>Mapper 的业务逻辑写在 map() 方法中</li><li>Mapper 的输出数据是 KV 对形式，KV 类型自定义</li><li>map() 方法对每个 KV 只调用一次</li></ol><h2 id="Reducer-阶段"><a href="#Reducer-阶段" class="headerlink" title="Reducer 阶段"></a>Reducer 阶段</h2><ol><li>用户自定义的 Reducer 要继承自己的父类</li><li>Reducer 的输入类型对应 Mapper 的输出数据类型，也是 KV</li><li>Reducer 的业务逻辑写在对应的 reduce() 方法中</li><li>reduce() 方法对每个 KV 只调用一次</li></ol><h2 id="Driver-阶段"><a href="#Driver-阶段" class="headerlink" title="Driver 阶段"></a>Driver 阶段</h2><p>相当于 YARN 集群的客户端，用于提交整个程序到 YARN 集群，提交的是封装了 MapReduce 程序相关运行参数的 Job 对象</p><hr><h1 id="WordCount-Demo"><a href="#WordCount-Demo" class="headerlink" title="WordCount(Demo)"></a>WordCount(Demo)</h1><p>需求：给定一个文本文件 <a href="/file/hadoop/map-reduce/README.txt">README.txt</a>，统计文本中每个单词出现的总次数</p><p>流程分析：</p><blockquote><p>Mapper 阶段</p></blockquote><ol><li>将 MapTask 传给我们的文本内容转换为 String</li><li>根据空格将单词分为单词</li><li>将单词输出为 &lt;单词, 1&gt; (如果有相同的单词，输出的也是 &lt;单词, 1&gt;， 因为 map 阶段只做拆分，不做合并)</li></ol><blockquote><p>Reducer 阶段</p></blockquote><ol><li>汇总各个 key 的个数</li><li>输出该 key 的总次数</li></ol><blockquote><p>Driver</p></blockquote><ol><li>获取配置信息，获取 job 对象实例</li><li>指定本程序的 jar 包所在的本地路径</li><li>关联 Mapper、Reducer 业务类</li><li>指定 Mapper 输出的 KV 类型</li><li>指定最终输出的数据 KV 类型</li><li>指定 job 的原始文件所在目录</li><li>指定 job 的输出结果所在目录</li><li>提交 job</li></ol><h2 id="Mapper-实现"><a href="#Mapper-实现" class="headerlink" title="Mapper 实现"></a>Mapper 实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> laiyy</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2019/12/3 16:36</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * LongWritable：输入数据的 key（此处为偏移量） 类型</span></span><br><span class="line"><span class="comment"> * Text：输入数据的 value（每一行数据） 类型</span></span><br><span class="line"><span class="comment"> * Text：输出数据的 key 类型</span></span><br><span class="line"><span class="comment"> * IntWritable：输出数据的 value 类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> IntWritable writable = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 1. 将当前读入的行数据转换为 String 类型</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">// 2. 切割单词</span></span><br><span class="line">        String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="comment">// 3. 循环写出</span></span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            outKey.set(word);;</span><br><span class="line">            writable.set(<span class="number">1</span>);</span><br><span class="line">            context.write(outKey, writable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Reducer-实现"><a href="#Reducer-实现" class="headerlink" title="Reducer 实现"></a>Reducer 实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> laiyy</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2019/12/3 16:49</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Text：Map 阶段输出的 key 类型</span></span><br><span class="line"><span class="comment"> * IntWritable：Map 阶段输出的 value 的类型</span></span><br><span class="line"><span class="comment"> * Text：最终结果的 key 的类型</span></span><br><span class="line"><span class="comment"> * IntWritable：最终结果的 value 类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable();</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">            在 Map 阶段输出的数据格式为： &#123;key1,1&#125;, &#123;key2,1&#125; ,如： &#123;china,1&#125;, &#123;japan,1&#125;, &#123;china, 1&#125;</span></span><br><span class="line"><span class="comment">            在 Reducer 阶段作为输入时， 相同的将合并，即</span></span><br><span class="line"><span class="comment">            key：china，</span></span><br><span class="line"><span class="comment">            values：[1,1]</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 1. 累加求和</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        outValue.set(sum);</span><br><span class="line">        context.write(key, outValue);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Driver-实现"><a href="#Driver-实现" class="headerlink" title="Driver 实现"></a>Driver 实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取 job 对象</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 jar 存放路径</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关联 Mapper、Reducer 业务类</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定 Mapper 输出的 KV 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定最终输出的数据 KV 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定 job 的输入文件所在目录</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"d:/dev/README.txt"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定 job 的输出结果所在目录</span></span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"d:/dev/mr-demo1"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交 job，为 true 时打印执行信息</span></span><br><span class="line"><span class="comment">//        job.submit();</span></span><br><span class="line">        <span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        System.exit(succeed ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="执行测试"><a href="#执行测试" class="headerlink" title="执行测试"></a>执行测试</h2><p>运行时可以看到打印信息如下：<br><img src="/images/hadoop/map-reduce/word-count-demo.png" alt="word-count-succeed"><br><img src="/images/hadoop/map-reduce/word-count-demo-succeed.png" alt="word-count-succeed"></p><p>打开执行后的 <code>part-r-00000</code> 文件如下：<br><img src="/images/hadoop/map-reduce/word-count-result.png" alt="word-count-result"></p><h2 id="在集群中运行"><a href="#在集群中运行" class="headerlink" title="在集群中运行"></a>在集群中运行</h2><p>将 Driver 的输入、输出路径改为参数获取方式：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定 job 的输入文件所在目录</span></span><br><span class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 job 的输出结果所在目录</span></span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br></pre></td></tr></table></figure></p><p>在 pom.xml 文件中增加如下配置：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="comment">&lt;!-- 打包主类 --&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.laiyy.study.mapreduce.wordcount.WordCountDriver<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>然后使用 <code>mvn package</code> 打包后，会生成两个文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop-1.0-SNAPSHOT.jar</span><br><span class="line">hadoop-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><p>第一个 jar 是没有 hadoop 依赖的，第二个 jar 是有 hadoop 依赖的。</p><p>由于在 hadoop 集群上运行，所以可以使用第一个 jar。如果服务器上没有 hadoop 依赖，则使用第二个 jar 即可。</p><p>将第一个 jar 上传至 hadoop，由于使用的是 hadoop 集群，所以输入、输出路径均为 HDFS 路径。</p><p>运行测试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# hadoop jar hadoop-1.0-SNAPSHOT.jar com.laiyy.study.mapreduce.wordcount.WordCountDriver /laiyy /laiyy/output</span><br><span class="line">19/12/04 14:11:07 INFO client.RMProxy: Connecting to ResourceManager at hadoop03/192.168.233.132:8032</span><br><span class="line">19/12/04 14:11:09 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.</span><br><span class="line">19/12/04 14:11:10 INFO input.FileInputFormat: Total input paths to process : 3</span><br><span class="line">19/12/04 14:11:10 INFO mapreduce.JobSubmitter: number of splits:3</span><br><span class="line">19/12/04 14:11:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1575438331337_0001</span><br><span class="line">19/12/04 14:11:11 INFO impl.YarnClientImpl: Submitted application application_1575438331337_0001</span><br><span class="line">19/12/04 14:11:11 INFO mapreduce.Job: The url to track the job: http://hadoop03:8088/proxy/application_1575438331337_0001/</span><br><span class="line">19/12/04 14:11:11 INFO mapreduce.Job: Running job: job_1575438331337_0001</span><br><span class="line">19/12/04 14:11:24 INFO mapreduce.Job: Job job_1575438331337_0001 running in uber mode : false</span><br><span class="line">19/12/04 14:11:24 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>执行结束后，查看 HDFS 中 /laiyy/output 文件夹内容，并下载 <code>part-r-0000</code> 文件，查看文件输出.</p><p><img src="/images/hadoop/map-reduce/wordcount-in-cluster.png" alt="WordCount在集群运行结果"><br><img src="/images/hadoop/map-reduce/word-count-cluster-result.png" alt="WordCount在集群运行结果"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;HDFS、MapReduce、Yarn 是 Hadoop 的三大模块，其中，HDFS 负责存储，MapReduce 负责计算，Yarn 负责资源调度&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop（8） &lt;br/&gt; DataNode、小文件存档</title>
    <link href="https://www.laiyy.top/hadoop/hdfs/hadoop-8.html"/>
    <id>https://www.laiyy.top/hadoop/hdfs/hadoop-8.html</id>
    <published>2019-12-02T01:56:18.000Z</published>
    <updated>2019-12-02T01:56:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>在了解了 NameNode、SecondaryNameNode 的工作机制、FsImage 与 Edits 数据备份、NameNode 安全机制与多目录后，对 NameNode 有了一些基础了解。在此基础之上，接下来了解一下 DataNode 的工作机制。</p><a id="more"></a><h1 id="DataNode-工作机制"><a href="#DataNode-工作机制" class="headerlink" title="DataNode 工作机制"></a>DataNode 工作机制</h1><p>DataNode 中存储的每个数据块，以文件形式存储在磁盘上。包括 2 个文件：数据本身、元数据(包括数据块的长度、校验和、时间戳)</p><ol><li>DataNode 启动后，向 NameNode 注册</li><li>NameNode 注册成功后，会同步在 NameNode 元数据中</li><li>DataNode 每个一个<code>固定的周期</code>向 NameNode 再注册（默认1小时）</li><li>DataNode 与 NameNode 以 <code>3 秒一次</code> 的心跳机制判断是否断开。心跳返回结果带有 NameNode 给 DataNode 的命令</li><li>NameNode 超过 <code>10 分钟</code>没有收到心跳，则认为该节点不可用</li></ol><p><img src="/images/hadoop/client/datanode-work.png" alt="DataNode 工作机制"></p><hr><h1 id="DataNode-数据完整性"><a href="#DataNode-数据完整性" class="headerlink" title="DataNode 数据完整性"></a>DataNode 数据完整性</h1><ol><li>当 DataNode 读取 Block 时，它会计算 CheckSum(校验和)</li><li>如果计算后的 CheckSum 与 Block 创建时的值不一样，说明 Block 已经损坏</li><li>Client 读取其他 DataNode 上的 Block</li><li>DataNode 在其文件创建后，周期性的验证 CheckSum</li></ol><p><img src="/images/hadoop/client/check-sum.png" alt="DataNode 数据完整性"></p><hr><h1 id="掉线时限"><a href="#掉线时限" class="headerlink" title="掉线时限"></a>掉线时限</h1><p>DataNode 进程死亡或者网路故障，造成 DataNode 与 NameNode 无法通信，NameNode 不会立即把该节点判定为 <code>死亡</code>，要经过一段时间后才会判定为死亡，这段时间称为 <code>超时时长</code>。HDFS 默认的超时时长为 10min + 30s</p><p>超时时长计算公示： <code>2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval</code></p><p>具体可参考 <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">官方文档</a>，默认值为： <code>dfs.namenode.heartbeat.recheck-interval</code> 5 分钟(300000)、<code>dfs.heartbeat.interval</code> 3秒（3s）</p><p>如果需要修改掉线时限，可以修改 <code>hdfs-site.xml</code> 文件。</p><hr><h1 id="服役新数据节点"><a href="#服役新数据节点" class="headerlink" title="服役新数据节点"></a>服役新数据节点</h1><p>现在已经有 3 台服务器构成了一个 hadoop 集群，如果现在需要在此基础上，再增加一个新的数据节点，就称为 <code>新数据节点服役</code>。</p><blockquote><p>环境准备</p></blockquote><ol><li>以 hadoop04 为主，克隆一台 hadoop05</li><li>修改 hadoop05 的 ip、主机名称</li><li>删除原来 HDFS 中的 <code>data/</code> 和 <code>log/</code></li><li>应用 <code>/etc/profile</code> 配置文件</li></ol><blockquote><p>服役新节点</p></blockquote><ol><li>启动原始集群 02、03、04 </li><li>删除 05 的 <code>data/</code> 和 <code>log/</code></li><li>启动 05 的 DataNode(<code>sbin/hadoop-daemon.sh start datanode</code>)</li><li>启动 05 的 NodeManager(<code>sbin/yarn-daemon.sh start nodemanager</code>)</li><li>在 05 上上传文件进行测试</li><li>查看 WebUI 的 datanodes</li></ol><p><img src="/images/hadoop/client/datanodes.png" alt="datanodes"><br><img src="/images/hadoop/client/05-upload-file.png" alt="新 DataNode 上传文件"><br><img src="/images/hadoop/client/05-file-block.png" alt="新 DataNode 上传文件"></p><hr><h1 id="退役旧数据节点"><a href="#退役旧数据节点" class="headerlink" title="退役旧数据节点"></a>退役旧数据节点</h1><h2 id="主机白名单"><a href="#主机白名单" class="headerlink" title="主机白名单"></a>主机白名单</h2><p>添加到白名单里的主机节点，都允许访问 NameNode，否则不允许访问。</p><p>配置步骤：</p><blockquote><p>在 NameNode 的 <code>%HADOOP_HOME%/etc/hadoop</code> 目录下创建 <code>dfs.hosts</code> 文件，添加白名单（此次不添加 05）<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dfs.hosts</span></span><br><span class="line">hadoop02</span><br><span class="line">hadoop03</span><br><span class="line">hadoop04</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>在 NameNode 的 <code>hdfs-site.xml</code> 文件中增加 dfs.hosts 配置</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置白名单 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>分发到 03、04，并刷新 NameNode</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop]# xsync hdfs-site.xml</span><br><span class="line">[root@hadoop02 hadoop]# hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br></pre></td></tr></table></figure><blockquote><p>更新 ResourceManager</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop03 ~]# yarn rmadmin -refreshNodes</span><br><span class="line">19/12/02 14:26:29 INFO client.RMProxy: Connecting to ResourceManager at hadoop03/192.168.233.132:8033</span><br></pre></td></tr></table></figure><blockquote><p>查看 WebUI</p></blockquote><p>在执行上述操作的时候，02、03、04、05 都未关闭，也就是说，在执行上述操作之前，在 WebUI 的 DataNodes 中是可以看到 02、03、04、05 四台机器的。<br>在执行完上述操作后，再次查看 WebUI<br><img src="/images/hadoop/client/white-list.png" alt="白名单"></p><blockquote><p>如果数据不均衡，可以使用命令来实现集群的再平衡</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# sbin/start-balancer.sh </span><br><span class="line">starting balancer, logging to /opt/module/hadoop-2.7.2/logs/hadoop-root-balancer-hadoop02.out</span><br><span class="line">Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span><br></pre></td></tr></table></figure><p>此时再查看 <code>README.TXT</code> 文件的块信息，由 05 变更到 02 上了。</p><p><img src="/images/hadoop/client/data-balance.png" alt="数据平衡"></p><h2 id="黑名单退役"><a href="#黑名单退役" class="headerlink" title="黑名单退役"></a>黑名单退役</h2><p>在黑名单上的主机会被强制退出。</p><p>在测试实现这种情况之前，需要将现场恢复，即将 hdfs-site.xml 中添加的白名单配置先注释掉，并刷新 NameNode 和 ResourceManager，启动 05 上的 DataNode</p><p>在 NameNode 的 <code>%HADOOP_HOME%/etc/hadoop</code> 目录下，创建 <code>dfs.hosts.exclude</code> 文件，并添加 05</p><p>修改 NameNode 上的 <code>hdfs-site.xml</code> 文件，增加 <code>dfs.hosts.exclude</code> 配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 黑名单 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>刷新 NameNode、ResourceManager<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop]# hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br><span class="line"></span><br><span class="line">[root@hadoop03 ~]# yarn rmadmin -refreshNodes</span><br><span class="line">19/12/02 14:26:29 INFO client.RMProxy: Connecting to ResourceManager at hadoop03/192.168.233.132:8033</span><br></pre></td></tr></table></figure></p><p>查看 WebUI，提示正在退役中(Decommission In Progress)，此时正在退役的节点会将数据块复制到其他节点上，保证数据的完整性。<br><img src="/images/hadoop/client/decommission.png" alt="退役节点"></p><p>稍等一会后再刷新页面，提示 05 节点已退役完成：Decommissioned。</p><p>如果数据不平衡，可以和白名单一样，使用 balance 命令平衡数据。</p><blockquote><p>需要注意的点：</p></blockquote><ol><li>等待退役节点状态为 <code>Decommissioned</code>，此时所有的块都已经复制完成，停止该节点及节点资源管理器。注意：如果副本数是 3，服役的节点小于等于 3，是不能退役的！需要修改副本数后才能退役！</li><li>不允许白名单和黑名单中存在同一个主机</li></ol><hr><h1 id="DataNode-多目录"><a href="#DataNode-多目录" class="headerlink" title="DataNode 多目录"></a>DataNode 多目录</h1><p>修改配置与 NameNode 多目录差不多，也是修改 <code>hdfs-site.xml</code>，增加如下配置<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>分发到不同主机上后，重启 hadoop 集群即可。</p><blockquote><p>注意：</p></blockquote><p>DataNode 与 NameNode 不同，DataNode 每个目录中存放的数据<code>不一样</code>。数据不是副本！</p><hr><h1 id="小文件存档"><a href="#小文件存档" class="headerlink" title="小文件存档"></a>小文件存档</h1><h2 id="小文件存档的弊端"><a href="#小文件存档的弊端" class="headerlink" title="小文件存档的弊端"></a>小文件存档的弊端</h2><p>鉴于每个文件在 DataNode 中分块存储，每个块的元数据村存在 NameNode 中，因此 HDFS 存储小文件会非常低效。因为大量的小文件会耗尽 NameNode 中的大部分内存。<br>但是，需要注意的是，存储小文件所需要的磁盘容量和数据块的大小无关。</p><p>如：一个 1MB 的文件，设置为 128M 的块存储，实际使用的磁盘空间是 1MB，而不是 128MB。</p><h2 id="解决办法之一"><a href="#解决办法之一" class="headerlink" title="解决办法之一"></a>解决办法之一</h2><p>HDFS 存档文件或 HAR 文件，是一个更搞笑的文件存档工具，它将文件存在 HDFS 块，在减少 NameNode 内存使用的同时，允许对文件进行透明访问。<br>具体来说，HDFS 存档文件对内还是一个一个的独立文件，对外（NameNode）而言却是一个整体，减少了 NameNode 的内存。</p><blockquote><p>测试</p></blockquote><p>在 HDFS 中，存放几个测试文件：<br><img src="/images/hadoop/client/before-har.png" alt="before har"></p><p>文件压缩：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数解释</span></span><br><span class="line"><span class="comment"># archive：开始文件压缩</span></span><br><span class="line"><span class="comment"># -archiveName：指定压缩的名称</span></span><br><span class="line"><span class="comment"># -p： 从那个目录，压缩到那个目录</span></span><br><span class="line">[root@hadoop03 hadoop-2.7.2]<span class="comment"># hadoop  archive -archiveName outout.har -p / /output</span></span><br></pre></td></tr></table></figure></p><p>执行完成后，查看 WebUI<br><img src="/images/hadoop/client/har-suscceed.png" alt="文件压缩后"></p><p>可以看到，文件成功输出了，但是我们看不到文件的内容是否和压缩前一致，解决办法：使用 <code>hadoop fs -ls -R har:///output/output.har</code> 命令查看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# hadoop fs -ls -R har:///output/outout.har</span><br><span class="line">-rw-r--r--   3 root supergroup      15429 2019-12-02 16:23 har:///output/outout.har/LICENSE.txt</span><br><span class="line">-rw-r--r--   3 root supergroup        101 2019-12-02 16:22 har:///output/outout.har/NOTICE.txt</span><br><span class="line">-rw-r--r--   3 root supergroup       1366 2019-12-02 16:22 har:///output/outout.har/README.txt</span><br></pre></td></tr></table></figure><p>文档解压：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# hadoop fs -cp har:///output/outout.har /har</span><br></pre></td></tr></table></figure></p><hr><h1 id="回收站"><a href="#回收站" class="headerlink" title="回收站"></a>回收站</h1><p>开启回收站功能，可以将删除的文件，在不超时的情况下，恢复原数据，起到防止误删、备份等作用。</p><h2 id="回收站参数设置及工作机制"><a href="#回收站参数设置及工作机制" class="headerlink" title="回收站参数设置及工作机制"></a>回收站参数设置及工作机制</h2><blockquote><p>开启回收站功能参数</p></blockquote><ol><li><code>fs.trash.interval</code>：默认值为 0，表示 <code>禁用回收站</code>，其他值表示该文件的存活时间，单位 <code>分钟</code></li><li><code>fs.trash.checkpoint.interval</code>：默认值为 0，表示 <code>检查回收站的时间间隔</code>，如果为 0，则该值与 <code>fs.trash.interval</code> 的时间间隔相同。单位 <code>分钟</code></li><li>要求：<code>fs.trash.checkpoint.interval</code> &lt;= <code>fs.trash.interval</code></li></ol><p>修改 <code>core-site.xml</code> 文件<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p><blockquote><p>删除一条数据测试</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop03 hadoop-2.7.2]# hadoop fs -rm /README.txt</span><br><span class="line">19/12/02 17:31:15 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1 minutes, Emptier interval = 0 minutes.</span><br><span class="line">Moved: &apos;hdfs://hadoop02:9000/README.txt&apos; to trash at: hdfs://hadoop02:9000/user/root/.Trash/Current</span><br></pre></td></tr></table></figure><blockquote><p>在 WebUI 中查看回收站</p></blockquote><p><img src="/images/hadoop/client/trash-warn.png" alt="trash warn"></p><p>错误原因：进入垃圾回收站的默认用户名为 <code>dr.who</code>， 需要修改回收站用户名</p><p>修改 core-site.xml，并分发的 03、04 上，重启集群<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>再次查看 WebUI：<br><img src="/images/hadoop/client/trash-succeed.png" alt="trash succeed"></p><hr><h1 id="快照管理"><a href="#快照管理" class="headerlink" title="快照管理"></a>快照管理</h1><p>快照，相当于对目录做了一次备份。此操作并 <em>不会立即赋值所有文件</em> ，而是 <em>指向同一个文件</em>。当写入发生时，才会产生新文件。</p><h2 id="常见命令"><a href="#常见命令" class="headerlink" title="常见命令"></a>常见命令</h2><blockquote><p>开启指定目录的快照功能： <code>hdfs dfsadmin -allowSnapshot &lt;path&gt;</code></p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# hdfs dfsadmin -allowSnapshot /laiyy</span><br><span class="line">Allowing snaphot on /laiyy succeeded</span><br></pre></td></tr></table></figure><blockquote><p>禁用指定目录的快照功能(默认)：<code>hdfs dfsadmin -disallowSnapshot &lt;path&gt;</code><br>对指定目录创建快照：<code>hdfs dfs -createSnapshot &lt;path&gt;</code></p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# hdfs dfs -createSnapshot /laiyy</span><br><span class="line">Created snapshot /laiyy/.snapshot/s20191203-135428.244</span><br></pre></td></tr></table></figure><p>此时在 WebUI 中是看不到快照文件的，因为这个快照文件是<code>隐藏文件</code>，输入 <code>/laiyy/.snapshot</code> 即可查看<br><img src="/images/hadoop/client/no-snapshot.png" alt="看不到快照"><br><img src="/images/hadoop/client/snapshot.png" alt="看到快照"><br><img src="/images/hadoop/client/snapshot1.png" alt="看到快照"></p><blockquote><p>创建目录快照并指定名称：<code>hdfs dfs -createSnapshot &lt;path&gt; &lt;name&gt;</code><br>快照重命名：<code>hdfs dfs -renameSnapshot &lt;path&gt; &lt;oldName&gt; &lt;newName&gt;</code></p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# hdfs dfs -renameSnapshot /laiyy s20191203-135428.244 laiyy_snapshot</span><br></pre></td></tr></table></figure><p>查看 WebUI，可以看到快照名称已经修改了。<br><img src="/images/hadoop/client/rename-snapshot.png" alt="修改快照名称"></p><blockquote><p>列出当前用户可以快照的目录：<code>hdfs lsSnapshottableDir</code><br>比较两个快照目录的不同：<code>hdfs snapshotDiff &lt;path&gt; &lt;from&gt; &lt;to&gt;</code></p></blockquote><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># path 哪个路径的快照</span></span><br><span class="line"><span class="comment"># from、to：比较 form 和 to 两个快照的区别</span></span><br><span class="line"><span class="comment"># 此时的 from 用 "." 指代 /laiyy 文件夹，此时比较是没有任何区别的</span></span><br><span class="line">[root@hadoop02 hadoop-2.7.2]<span class="comment"># hdfs snapshotDiff /laiyy . .snapshot/laiyy_snapshot</span></span><br><span class="line">Difference between current directory and snapshot laiyy_snapshot under directory /laiyy:</span><br></pre></td></tr></table></figure><p>在已经创建快照之后，再往 /laiyy 下上传一个文件，比较结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# hdfs snapshotDiff /laiyy . .snapshot/laiyy_snapshot</span><br><span class="line">Difference between current directory and snapshot laiyy_snapshot under directory /laiyy:</span><br><span class="line">M.</span><br><span class="line">-./LICENSE.txt</span><br></pre></td></tr></table></figure><p>再创建一个新的快照，比较两个快照之间的区别</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# hdfs snapshotDiff /laiyy .snapshot/laiyy_snapshot .snapshot/laiyy_snapshot1</span><br><span class="line">Difference between snapshot laiyy_snapshot and snapshot laiyy_snapshot1 under directory /laiyy:</span><br><span class="line">M.</span><br><span class="line">+./LICENSE.txt</span><br></pre></td></tr></table></figure><blockquote><p>删除快照：<code>hdfs dfs -deleteSnapshot &lt;path&gt; &lt;name&gt;</code></p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -deleteSnapshot /laiyy laiyy_snapshot</span><br></pre></td></tr></table></figure><p>查看 WebUI</p><p><img src="/images/hadoop/client/delete-snapshot.png" alt="删除快照"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在了解了 NameNode、SecondaryNameNode 的工作机制、FsImage 与 Edits 数据备份、NameNode 安全机制与多目录后，对 NameNode 有了一些基础了解。在此基础之上，接下来了解一下 DataNode 的工作机制。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="hdfs" scheme="https://www.laiyy.top/categories/hadoop/hdfs/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="hdfs" scheme="https://www.laiyy.top/tags/hdfs/"/>
    
      <category term="DataNode" scheme="https://www.laiyy.top/tags/DataNode/"/>
    
      <category term="trash" scheme="https://www.laiyy.top/tags/trash/"/>
    
      <category term="snapshot" scheme="https://www.laiyy.top/tags/snapshot/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop（7） &lt;br/&gt; NameNode 和 SecondaryNameNode、集群安全模式</title>
    <link href="https://www.laiyy.top/hadoop/hdfs/hadoop-7.html"/>
    <id>https://www.laiyy.top/hadoop/hdfs/hadoop-7.html</id>
    <published>2019-11-28T01:24:43.000Z</published>
    <updated>2019-11-28T01:24:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>此前通过代码了解了 HDFS API 和 I/O 操作，并了解了 HDFS 读写数据的过程，对 HDFS 整体运行过程有了初步了解。接下来就需要了解一下 NN（NameNode）、2NN（SecondaryNameNode） 的区别</p><a id="more"></a><h1 id="NN-和-2NN-的工作机制"><a href="#NN-和-2NN-的工作机制" class="headerlink" title="NN 和 2NN 的工作机制"></a>NN 和 2NN 的工作机制</h1><p>如果 NameNode 中的元数据存储在 NameNode 节点的磁盘中，由于要经常进行随机访问，还要响应客户端请求，效率会很低。因此，元数据必须要放在内存中。但是，如果只存储在内存中，一旦断点、服务重启，元数据就会丢失。因此，基于这种情况，产生了在磁盘中备份元数据的 <code>FsImage</code>。</p><p>在此基础上，当在内存中更新元数据，如果同时更新 FsImage，会导致效率降低；如果不更新，会产生一致性问题，一旦 NameNode 断电，数据就会丢失。<br>基于这种情况，Hadoop 引入了 <code>Edits</code> 文件(只进行追加操作，效率高)。每当元数据有更新或者添加时，就会修改内存中的元数据并追加到 Edits 中。<br>这样，一旦 NameNode 断电，可以通过 FsImage 和 Edits 合并成一个元数据。</p><p>但是这样还有一个问题没有解决，那就是如果长时间添加数据到 Edits 中，会导致该文件数据过大，效率很低，而且，一旦断电，恢复元数据需要的时间很长。<br>因此，需要定时进行 FsImage 和 Edits 的合并。 如果这个操作由 NameNode 完成，效率又会降低。因此引入了一个新的节点 <code>SecondaryNameNode</code>，专门用于 FsImage 和 Edits 的合并。</p><p>流程：</p><ol><li>NameNode 启动时，加载 Edits 和 FsImage 到内存（每个 block 占元数据 150byte）</li><li>客户端进行增删改操作时，NameNode 要先记录操作日志，更新Edits，再去进行其他后续请求</li><li>SecondaryNameNode 请求 NameNode，检查是否触发检查点（触发条件：检查时间到，或 Edits 中的数据满：达到 100W 条）</li><li>2NN 请求执行检查点（CheckPoint）</li><li>NameNode 滚动正在写的 Edits（即从 edits_001 文件，滚动到 edits_002 文件，后续的操作日志将写入 edit_002 中）</li><li>将 edits_001 和 FsImage 拷贝到 2NN</li><li>2NN 将 FsImage 和 edits_001 加载到内存并合并</li><li>2NN 生成新的 FsImage（如：fsimage.checkpoint）</li><li>将新生成的 FsImage 拷贝到 NameNonde，并重命名为 fsimage，加载到内存</li></ol><p><img src="/images/hadoop/client/nn-2nn.png" alt="NameNode 工作机制"></p><hr><h1 id="镜像文件和编辑日志-FsImage、Edits"><a href="#镜像文件和编辑日志-FsImage、Edits" class="headerlink" title="镜像文件和编辑日志 (FsImage、Edits)"></a>镜像文件和编辑日志 (FsImage、Edits)</h1><p>在 NameNode 所在的服务器中，查看 fsimage 和 edits 文件(<code>/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current</code>)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">-rw-r--r--. 1 root root    1383 11月 26 11:13 edits_0000000000000000001-0000000000000000019</span><br><span class="line">-rw-r--r--. 1 root root    1816 11月 26 12:13 edits_0000000000000000020-0000000000000000043</span><br><span class="line">-rw-r--r--. 1 root root      42 11月 26 13:13 edits_0000000000000000044-0000000000000000045</span><br><span class="line">-rw-r--r--. 1 root root 1048576 11月 26 13:13 edits_0000000000000000046-0000000000000000046</span><br><span class="line">-rw-r--r--. 1 root root      42 11月 26 14:53 edits_0000000000000000047-0000000000000000048</span><br><span class="line">-rw-r--r--. 1 root root 1048576 11月 26 14:53 edits_0000000000000000049-0000000000000000049</span><br><span class="line">-rw-r--r--. 1 root root     260 11月 26 17:39 edits_0000000000000000050-0000000000000000054</span><br><span class="line">-rw-r--r--. 1 root root 1048576 11月 26 17:39 edits_0000000000000000055-0000000000000000055</span><br><span class="line">-rw-r--r--. 1 root root    1081 11月 27 10:11 edits_0000000000000000056-0000000000000000072</span><br><span class="line">-rw-r--r--. 1 root root      42 11月 27 11:11 edits_0000000000000000073-0000000000000000074</span><br><span class="line">-rw-r--r--. 1 root root 1048576 11月 27 11:48 edits_0000000000000000075-0000000000000000080</span><br><span class="line">-rw-r--r--. 1 root root    1339 11月 28 15:32 edits_0000000000000000081-0000000000000000098</span><br><span class="line">-rw-r--r--. 1 root root      42 11月 28 16:32 edits_0000000000000000099-0000000000000000100</span><br><span class="line">-rw-r--r--. 1 root root 1048576 11月 28 16:32 edits_0000000000000000101-0000000000000000101</span><br><span class="line">-rw-r--r--. 1 root root      42 11月 29 11:21 edits_0000000000000000102-0000000000000000103</span><br><span class="line">-rw-r--r--. 1 root root 1048576 11月 29 11:21 edits_0000000000000000104-0000000000000000104</span><br><span class="line">-rw-r--r--. 1 root root 1048576 11月 29 14:19 edits_inprogress_0000000000000000105</span><br><span class="line">-rw-r--r--. 1 root root     765 11月 29 11:21 fsimage_0000000000000000103</span><br><span class="line">-rw-r--r--. 1 root root      62 11月 29 11:21 fsimage_0000000000000000103.md5</span><br><span class="line">-rw-r--r--. 1 root root     765 11月 29 14:19 fsimage_0000000000000000104</span><br><span class="line">-rw-r--r--. 1 root root      62 11月 29 14:19 fsimage_0000000000000000104.md5</span><br><span class="line">-rw-r--r--. 1 root root       4 11月 29 14:19 seen_txid</span><br><span class="line">-rw-r--r--. 1 root root     207 11月 29 14:19 VERSION</span><br></pre></td></tr></table></figure></p><h2 id="查看-FsImage-文件"><a href="#查看-FsImage-文件" class="headerlink" title="查看 FsImage 文件"></a>查看 FsImage 文件</h2><p>此时，查看 <code>fsimage_0000000000000000103</code> 文件，可以看到一些简略信息，详细信息都被二进制编码了。</p><p><img src="/images/hadoop/client/cat-fsimage.png" alt="cat fsimage"></p><p>我们可以通过 <code>hdfs</code> 的命令，将文件转储为可以看懂的 XML 文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数解释：</span></span><br><span class="line"><span class="comment"># oiv：转储 fsimage 文件</span></span><br><span class="line"><span class="comment"># -p：以何种格式转储</span></span><br><span class="line"><span class="comment"># -i：转储哪个文件</span></span><br><span class="line"><span class="comment"># -o：转储到什么位置</span></span><br><span class="line">hdfs oiv -p XML -i fsimage_0000000000000000103 -o ~/fsimage.xml</span><br></pre></td></tr></table></figure></p><p>执行命令，查看对应文件夹下的 <code>fsimage.xml</code> 文件(<code>cat ~/fsimage.xml</code>)<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">fsimage</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">NameSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">genstampV1</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">genstampV1</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">genstampV2</span>&gt;</span>1011<span class="tag">&lt;/<span class="name">genstampV2</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">genstampV1Limit</span>&gt;</span>0<span class="tag">&lt;/<span class="name">genstampV1Limit</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">lastAllocatedBlockId</span>&gt;</span>1073741834<span class="tag">&lt;/<span class="name">lastAllocatedBlockId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">txid</span>&gt;</span>103<span class="tag">&lt;/<span class="name">txid</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">NameSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">INodeSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">lastInodeId</span>&gt;</span>16402<span class="tag">&lt;/<span class="name">lastInodeId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span><span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1574923975056<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">permission</span>&gt;</span>root:supergroup:rwxr-xr-x<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>9223372036854775807<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>16398<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">type</span>&gt;</span>FILE<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>log.out<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replication</span>&gt;</span>2<span class="tag">&lt;/<span class="name">replication</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1574818660432<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">atime</span>&gt;</span>1574923642491<span class="tag">&lt;/<span class="name">atime</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">perferredBlockSize</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">perferredBlockSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">permission</span>&gt;</span>root:supergroup:rw-r--r--<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">blocks</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">block</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>1073741829<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">genstamp</span>&gt;</span>1006<span class="tag">&lt;/<span class="name">genstamp</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">numBytes</span>&gt;</span>1349614<span class="tag">&lt;/<span class="name">numBytes</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">blocks</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">INodeSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">INodeReferenceSection</span>&gt;</span><span class="tag">&lt;/<span class="name">INodeReferenceSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">SnapshotSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">snapshotCounter</span>&gt;</span>0<span class="tag">&lt;/<span class="name">snapshotCounter</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">SnapshotSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">INodeDirectorySection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">directory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">parent</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">INodeDirectorySection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">FileUnderConstructionSection</span>&gt;</span><span class="tag">&lt;/<span class="name">FileUnderConstructionSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">SnapshotDiffSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">diff</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">inodeid</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">inodeid</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">diff</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">SnapshotDiffSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">SecretManagerSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">currentId</span>&gt;</span>0<span class="tag">&lt;/<span class="name">currentId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tokenSequenceNumber</span>&gt;</span>0<span class="tag">&lt;/<span class="name">tokenSequenceNumber</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">SecretManagerSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">CacheManagerSection</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">nextDirectiveId</span>&gt;</span>1<span class="tag">&lt;/<span class="name">nextDirectiveId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">CacheManagerSection</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">fsimage</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="查看-Edits-文件"><a href="#查看-Edits-文件" class="headerlink" title="查看 Edits 文件"></a>查看 Edits 文件</h2><p>使用命令将 edits 文件转储为 xml：<code>hdfs oev -p XML -i edits_0000000000000000102-0000000000000000103 -o ~/edits.xml</code><br><strong><em>oev：转储 edits 文件</em></strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">EDITS</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">EDITS_VERSION</span>&gt;</span>-63<span class="tag">&lt;/<span class="name">EDITS_VERSION</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_START_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>102<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_END_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>103<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">EDITS</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>NameNode 通过 <code>%HADOOP_HOME%/data/tmp/name/current/seen_txid</code> 文件，来确定下次开机启动的时候，合并哪些 Edits。</p><hr><h1 id="CheckPoint-设置"><a href="#CheckPoint-设置" class="headerlink" title="CheckPoint 设置"></a>CheckPoint 设置</h1><p>2NN CheckPoint 检查点时间设置，通常情况下，每隔一小时执行一次。</p><p>此配置可以参考 <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">Hadoop 官方文档</a> 中的 <code>dfs.namenode.checkpoint.period</code> 设置项，单位为 秒。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>CheckPoint 每分钟检查一次操作次数，当操作次数达到 <code>100万</code> 时，SecondaryNameNode 执行一次。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 设置 CheckPoint 操作此时达到多少时执行 2NN --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置多长时间检查一次操作数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p><hr><h1 id="NameNode-故障处理"><a href="#NameNode-故障处理" class="headerlink" title="NameNode 故障处理"></a>NameNode 故障处理</h1><p>NameNode 故障后，可以采用如下两种方式进行数据恢复。</p><h2 id="将-2NN-中的数据拷贝到-NN-存储数据的目录"><a href="#将-2NN-中的数据拷贝到-NN-存储数据的目录" class="headerlink" title="将 2NN 中的数据拷贝到 NN 存储数据的目录"></a>将 2NN 中的数据拷贝到 NN 存储数据的目录</h2><p><strong><em>注意：此操作依赖于 2NN 的数据完整性</em></strong></p><p>步骤：</p><blockquote><p>kill -9 NameNode<br>删除 NameNode 存储的数据(<code>%HADOOP_HOME%/data/tmp/name/*</code>)<br>拷贝 2NN 中的数据到 NN 存储数据目录( 2NN 下的 <code>%HADOOP_HOME%/data/tmp/namesecondary/*</code>)<br>重启 NameNode</p></blockquote><p>拷贝方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r root@hadoop04:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* /opt/module/hadoop-2.7.2/data/tmp/dfs/name/</span><br></pre></td></tr></table></figure></p><p>单独其中 NameNode： <code>sbin/hadoop-daemon.sh start namenode</code></p><h2 id="导入检查点"><a href="#导入检查点" class="headerlink" title="导入检查点"></a>导入检查点</h2><p>使用 -importCheckpoint 选项，启动 NameNode 守护进程，进而将 2NN 中的数据拷贝到 NameNode 中。</p><p>步骤：</p><ol><li><p>修改 NameNode 的 hdfs-site.xml 文件：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>your hadoop namenode dir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>kill -9 NameNode</p></li><li>删除 NameNode 中存储的数据(<code>%HADOOP_HOME%/data/tmp/name/*</code>)</li><li>如果 2NN 和 NameNode 不在同一个主机节点上，需要将 2NN 存储数据的目录，拷贝到 NN 存储数据的平级目录，并删除 <code>in_use.lock</code> 文件。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r root@hadoop04:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* /opt/module/hadoop-2.7.2/data/tmp/dfs/</span><br><span class="line">cd /opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary</span><br><span class="line">rm -rf in_use.lock</span><br></pre></td></tr></table></figure><ol start="5"><li><p>导入检查点数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -importCheckpoint</span><br></pre></td></tr></table></figure></li><li><p>启动 NameNode<br><code>sbin/hadoop-daemon.sh start namenode</code></p></li></ol><hr><h1 id="集群安全模式"><a href="#集群安全模式" class="headerlink" title="集群安全模式"></a>集群安全模式</h1><h2 id="安全模式的条件"><a href="#安全模式的条件" class="headerlink" title="安全模式的条件"></a>安全模式的条件</h2><blockquote><p>NameNode 启动</p></blockquote><p>NameNode 启动时，首先将镜像文件（FsImage）加载到内存中，并执行编辑日志（Edits）中的各项操作。<br>一旦在内存中成功简历文件系统元数据的映像，则创建一个新的 FsImage 文件和一个空白的编辑日志，此时，NameNode 开始监听 DataNode 的请求。<br>这个过程中，NameNode 一直处于安全模式，即：NameNode 的文件系统对客户端来说是<code>只读</code>的。</p><blockquote><p>DataNode 启动</p></blockquote><p>系统中的数据块的位置并不是由 NameNode 维护的，而是以<code>块列表</code>的形式存储在 DataNode 中 。<br>在系统的<code>正常操作期间</code>，NameNode 会在内存中保留所有块位置的映射信息。<br>在<code>安全模式</code>下，DataNode 会向 NameNode 发送最新的块列表信息，NameNode 了解到足够多的<code>块位置</code>信息后，即可搞笑运行文件系统。</p><blockquote><p>安全模式退出判断</p></blockquote><p>如果满足 <code>最小副本条件</code>，NameNode 将会在 <code>30s</code> 后退出安全模式。<br>所谓 <code>最小副本条件</code>：是指在整个文件系统中 99.9% 的块满足最小副本级别（默认值：dfs.replication.min=1）。<br>在启动一个刚刚格式化的 HDFS 集群时，因为系统中还没有任何块，所以 NameNode 不会进入安全模式。</p><h2 id="安全模式的命令、语法等"><a href="#安全模式的命令、语法等" class="headerlink" title="安全模式的命令、语法等"></a>安全模式的命令、语法等</h2><p>当集群处于<code>安全模式</code>，不能执行任何写操作。集群启动完成后，自动退出安全模式。</p><h3 id="常用命令："><a href="#常用命令：" class="headerlink" title="常用命令："></a>常用命令：</h3><blockquote><p>bin/hdfs dfsadmin -safemode get：查看安全模式状态</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# bin/hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure><blockquote><p>bin/hdfs dfsadmin -safemode enter：进入安全模式</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# bin/hdfs dfsadmin -safemode enter</span><br><span class="line">Safe mode is ON</span><br></pre></td></tr></table></figure><p><img src="/images/hadoop/client/safe-mode-enter.png" alt="SafeMode Enter"></p><p>此时的 HDFS 文件系统状态为：<br><img src="/images/hadoop/client/safe-mode-file-system.png" alt="SafeMode FileSystem"></p><p>上传一个文件测试一下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# bin/hdfs dfs -put README.txt /</span><br><span class="line">put: Cannot create file/README.txt._COPYING_. Name node is in safe mode.</span><br></pre></td></tr></table></figure></p><p>可以发现，上传时已经报错：不能上传文件，因为 NameNode 处于 SafeMode。所以在 HDFS 中没有新上传的文件。</p><blockquote><p>bin/hdfs dfsadmin -safemode leave：离开安全模式</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# bin/hdfs dfsadmin -safemode leave</span><br><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure><p>此时再次执行上传文件，查看文件系统状态<br><img src="/images/hadoop/client/safe-mode-leave-file-system.png" alt="SafeMode FileSystem"></p><blockquote><p>bin/hdfs dfsadmin -safemode wait：等待安全模式</p></blockquote><h3 id="等待安全模式"><a href="#等待安全模式" class="headerlink" title="等待安全模式"></a>等待安全模式</h3><p>测试等待安全模式的步骤：</p><blockquote><p>先使集群进入到安全模式</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# bin/hdfs dfsadmin -safemode enter</span><br><span class="line">Safe</span><br></pre></td></tr></table></figure><blockquote><p>在 <code>%HADOOP_HOME%</code> 路径下创建一个脚本 <code>safemode.sh</code></p></blockquote><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">hdfs dfsadmin -safemode <span class="built_in">wait</span></span><br><span class="line"></span><br><span class="line">hdfs dfs -put /opt/module/hadoop-2.7.2/NOTICE.txt /</span><br></pre></td></tr></table></figure><p>修改权限：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod 777 safemode.sh</span><br></pre></td></tr></table></figure></p><p>执行脚本，可以看到当前进程阻塞住了，并没有上传 NOTICE.txt 文件。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./safemode.sh</span><br></pre></td></tr></table></figure></p><p>在 xshell 中再打开一个窗口，执行命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# bin/hdfs dfsadmin -safemode leave</span><br><span class="line">Safe mode is OFF</span><br></pre></td></tr></table></figure></p><p>再查看之前阻塞的进程，发现已经正常通行，且 NOTICE.txt 文件上传到了 HDFS 中<br><img src="/images/hadoop/client/safe-mode-wait.png" alt="SafeMode FileSystem"></p><hr><h1 id="NameNode-多目录配置"><a href="#NameNode-多目录配置" class="headerlink" title="NameNode 多目录配置"></a>NameNode 多目录配置</h1><p><strong>NameNode 的本地目录可以配置为多个，且每个目录存在的内容相同，增加了可靠性</strong><br><strong>注意：此方法只是保证了数据的可靠性，并不是保证 NameNode 可靠性，它们对应的依然是一个 NameNode 实例</strong></p><blockquote><p>第 0 步：关闭集群</p></blockquote><blockquote><p>第一步：修改 ddfs-site.xml 文件，增加如下内容，并分发到三台机器上</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure><blockquote><p>第二步：删除 data 和 logs 中的所有数据</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# rm -rf data/ logs/</span><br><span class="line">[root@hadoop03 hadoop-2.7.2]# rm -rf data/ logs/</span><br><span class="line">[root@hadoop04 hadoop-2.7.2]# rm -rf data/ logs/</span><br></pre></td></tr></table></figure><blockquote><p>格式化集群并启动</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# hdfs namenode -format</span><br><span class="line">[root@hadoop03 hadoop-2.7.2]# hdfs namenode -format</span><br><span class="line">[root@hadoop04 hadoop-2.7.2]# hdfs namenode -format</span><br></pre></td></tr></table></figure><p>查看 <code>%HADOOP_HOME%/data/dfs</code> 文件夹：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop04 ~]# ll /opt/module/hadoop-2.7.2/data/tmp/dfs/</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x. 3 root root 21 11月 29 16:52 name1</span><br><span class="line">drwxr-xr-x. 3 root root 21 11月 29 16:52 name2</span><br></pre></td></tr></table></figure></p><p>可见两个文件夹都创建成功了。</p><p>启动集群：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# sbin/start-dfs.sh</span><br><span class="line">[root@hadoop03 hadoop-2.7.2]# sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></p><blockquote><p>上传文件并测试，可以看到在 name1、name2 中都有对应的文件，且信息完全相同</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# hadoop fs -put NOTICE.txt /</span><br></pre></td></tr></table></figure><p><img src="/images/hadoop/client/more-dir.png" alt="namenode more dir"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此前通过代码了解了 HDFS API 和 I/O 操作，并了解了 HDFS 读写数据的过程，对 HDFS 整体运行过程有了初步了解。接下来就需要了解一下 NN（NameNode）、2NN（SecondaryNameNode） 的区别&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="hdfs" scheme="https://www.laiyy.top/categories/hadoop/hdfs/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="hdfs" scheme="https://www.laiyy.top/tags/hdfs/"/>
    
      <category term="NameNode" scheme="https://www.laiyy.top/tags/NameNode/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop（6） &lt;br/&gt; HDFS API 操作</title>
    <link href="https://www.laiyy.top/hadoop/hdfs/hadoop-6.html"/>
    <id>https://www.laiyy.top/hadoop/hdfs/hadoop-6.html</id>
    <published>2019-11-27T01:24:43.000Z</published>
    <updated>2019-11-27T01:24:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>此前，完成了一个基础的完全分布式集群，并且使用 Java 程序代码实现测试连通了 Hadoop 集群，且在 HDFS 中创建了一个文件夹。由此开始学习 Hadoop 的一些 Java API 操作。</p><a id="more"></a><p>API 操作通用方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initFileSystem</span><span class="params">()</span> <span class="keyword">throws</span> URISyntaxException, IOException, InterruptedException </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 两个副本</span></span><br><span class="line">    configuration.set(<span class="string">"dfs.replication"</span>, <span class="string">"2"</span>);</span><br><span class="line"></span><br><span class="line">    fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop02:9000"</span>), configuration, <span class="string">"root"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@After</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">closeFileSystem</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> != fileSystem) &#123;</span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="HDFS-API-操作"><a href="#HDFS-API-操作" class="headerlink" title="HDFS API 操作"></a>HDFS API 操作</h1><h2 id="文件上传"><a href="#文件上传" class="headerlink" title="文件上传"></a>文件上传</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 上传文件，参数1：待上传文件位置，参数2：HDFS 路径</span></span><br><span class="line">    fileSystem.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"d:\\log\\error.log"</span>), <span class="keyword">new</span> Path(<span class="string">"/error.log"</span>));</span><br><span class="line">    System.out.println(<span class="string">"上传完成"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/images/hadoop/client/copy-from-local.png" alt="文件上传"></p><p>将 <code>$HADOOP_HOME$/etc/hadoop/hdfs-site.xml</code> 文件，拷贝到 Java 程序的 <code>/resources</code> 文件夹下，并修改为：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 副本数量改为 1 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>注释掉程序中设置副本数的代码，再次上传文件，查看上传后文件副本数</p><p>结论：<br>1、如果程序中未设置副本数，且不存在 hdfs-site.xml 文件，则以 Hadoop 中设置的 hdfs-site.xml 中的副本数优先<br>2、如果程序中未设置副本数，存在 hdfs-site.xml 文件，以程序中的 hdfs-site.xml 中的副本数优先<br>3、如果程序中设置了副本数，且存在 hdfs-site.xml，以程序中设置的副本数优先<br>4、如果程序中设置了副本数，且不存在 hdfs-site.xml，以程序中设置的副本数优先</p><p>即：<code>Java 程序</code> &gt; <code>resources 中的 hdfs-site.xml</code> &gt; <code>hadoop 中的 hdfs-site.xml</code> &gt; <code>hadoop 默认的副本数</code></p><h2 id="文件下载"><a href="#文件下载" class="headerlink" title="文件下载"></a>文件下载</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 从 HDFS 拷贝的本机</span></span><br><span class="line">    fileSystem.copyToLocalFile(<span class="keyword">new</span> Path(<span class="string">"/error.log"</span>), <span class="keyword">new</span> Path(<span class="string">"d:\\log\\copy-to-local.log"</span>));</span><br><span class="line">    <span class="comment">// 参数1：是否删除源数据，参数2：HDFS，参数3：本地路径，参数4：是否开启本地模式校验</span></span><br><span class="line">    <span class="comment">// 参数4： 为true 时，下载成功后不会生成 .crc 文件，为 false 时会生成 .crc 文件</span></span><br><span class="line">    <span class="comment">// .crc 文件：校验数据可靠性的文件</span></span><br><span class="line">    fileSystem.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/error.log"</span>), <span class="keyword">new</span> Path(<span class="string">"d:\\log\\copy-to-local-1.log"</span>), <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="文件删除"><a href="#文件删除" class="headerlink" title="文件删除"></a>文件删除</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 参数1：HDFS</span></span><br><span class="line">    <span class="comment">// 参数2：是否递归删除，当参数1是文件夹时，需要设置为 true，否则报错</span></span><br><span class="line">    fileSystem.delete(<span class="keyword">new</span> Path(<span class="string">"/error1.log"</span>), <span class="keyword">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="文件改名"><a href="#文件改名" class="headerlink" title="文件改名"></a>文件改名</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRename</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 参数1：要修改的 HDFS</span></span><br><span class="line">    <span class="comment">// 参数2：修改为 HDFS</span></span><br><span class="line">    fileSystem.rename(<span class="keyword">new</span> Path(<span class="string">"/error.log"</span>), <span class="keyword">new</span> Path(<span class="string">"/log.out"</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="查看文件详情"><a href="#查看文件详情" class="headerlink" title="查看文件详情"></a>查看文件详情</h2><p>可以查看文件名称、权限、长度、块信息等</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 参数1：HDFS</span></span><br><span class="line">    <span class="comment">// 参数2：是否遍历</span></span><br><span class="line">    <span class="comment">// 返回值：获取到的文件信息迭代器</span></span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">false</span>);</span><br><span class="line">    <span class="keyword">while</span> (files.hasNext()) &#123;</span><br><span class="line">        <span class="comment">// 文件信息</span></span><br><span class="line">        LocatedFileStatus next = files.next();</span><br><span class="line">        System.out.println(next);</span><br><span class="line">        <span class="keyword">for</span> (BlockLocation blockLocation : next.getBlockLocations()) &#123;</span><br><span class="line">            <span class="comment">// 块信息</span></span><br><span class="line">            System.out.println(blockLocation);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>返回值：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// 文件信息</span><br><span class="line">&#123;</span><br><span class="line">    "path": "hdfs://hadoop02:9000/error1.log",   // 文件路径</span><br><span class="line">    "isDirectory": false,                       // 是否是文件夹</span><br><span class="line">    "length":1349614,               // 文件长度</span><br><span class="line">    "replication": 2,           // 副本数</span><br><span class="line">    "blocksize": 134217728,     // 块大小</span><br><span class="line">    "modification_time": 1574819199782, // 修改时间</span><br><span class="line">    "access_time": 1574819199617,  </span><br><span class="line">    "owner": "root",                // 所有者</span><br><span class="line">    "group": "supergroup",          // 所有组</span><br><span class="line">    "permission": "rw-r--r--",      // 权限</span><br><span class="line">    "isSymlink": false</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 块信息</span><br><span class="line">// 分别代表：起始位置，结束位置，块所在的hadoop服务器</span><br><span class="line">0,1349614,hadoop03,hadoop02</span><br></pre></td></tr></table></figure></p><h2 id="判断是否是文件夹"><a href="#判断是否是文件夹" class="headerlink" title="判断是否是文件夹"></a>判断是否是文件夹</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testIsDir</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">while</span> (files.hasNext()) &#123;</span><br><span class="line">        System.out.println(files.next().getPath().getName() + <span class="string">" 是否是文件夹？"</span> + !files.next().isFile());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h1 id="HDFS-I-O-流操作"><a href="#HDFS-I-O-流操作" class="headerlink" title="HDFS I/O 流操作"></a>HDFS I/O 流操作</h1><h2 id="文件上传-1"><a href="#文件上传-1" class="headerlink" title="文件上传"></a>文件上传</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testPutFileToHdfs</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2、获取输入流</span></span><br><span class="line"></span><br><span class="line">    FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">"d:/log/error.log"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3、获取输出流</span></span><br><span class="line">    FSDataOutputStream fsDataOutputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/test-io.log"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4、流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(inputStream, fsDataOutputStream, configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5、关闭资源</span></span><br><span class="line">    IOUtils.closeStream(inputStream);</span><br><span class="line">    IOUtils.closeStream(fsDataOutputStream);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="文件下载-1"><a href="#文件下载-1" class="headerlink" title="文件下载"></a>文件下载</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testGetFileFromHdfs</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取输入流</span></span><br><span class="line">    FSDataInputStream inputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/log.out"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取输出流</span></span><br><span class="line">    FileOutputStream outputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"d:/log/log1.out"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(inputStream, outputStream, configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    IOUtils.closeStream(outputStream);</span><br><span class="line">    IOUtils.closeStream(inputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="HDFS-文件定位读取"><a href="#HDFS-文件定位读取" class="headerlink" title="HDFS 文件定位读取"></a>HDFS 文件定位读取</h2><p>先往 HDFS 中上传一个大于 128M 的文件，在管理器中查看一下文件的<code>分块大于1</code>。</p><p><img src="/images/hadoop/client/more-block.png" alt="大于1块的文件"></p><p>可见当前文件分为了两块存储。如果此时进行下载，会将两块数据合并起来下载。但如果只想要下载其中的一部分，现在的下载方法无法实现。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 只读取第一块的数据</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testReadFileSeek1</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取输入流</span></span><br><span class="line">    FSDataInputStream inputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-2.7.2.tar.gz"</span>));</span><br><span class="line"></span><br><span class="line">    FileOutputStream outputStream = <span class="keyword">new</span> FileOutputStream(<span class="string">"d:/log/hadoop.part1"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//  只拷贝 128 M</span></span><br><span class="line">    <span class="keyword">byte</span>[] buffer = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1024</span> * <span class="number">128</span>; i++) &#123;</span><br><span class="line">        inputStream.read(buffer);</span><br><span class="line">        outputStream.write(buffer);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    IOUtils.closeStream(outputStream);</span><br><span class="line">    IOUtils.closeStream(inputStream);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 再读取第二块</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testReadFileSeek2</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">    <span class="comment">// 获取输入流</span></span><br><span class="line">    FSDataInputStream inputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/hadoop-2.7.2.tar.gz"</span>));</span><br><span class="line">    <span class="comment">// 指定读取开始点</span></span><br><span class="line">    inputStream.seek(<span class="number">1024</span> * <span class="number">1024</span> * <span class="number">128</span>);</span><br><span class="line">    <span class="comment">// 获取输出流</span></span><br><span class="line">    FileOutputStream outputStream = <span class="keyword">new</span> FileOutputStream(<span class="string">"d:/log/hadoop.part2"</span>);</span><br><span class="line">    <span class="comment">// 对拷</span></span><br><span class="line">    IOUtils.copyBytes(inputStream, outputStream, configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关闭资源</span></span><br><span class="line">    IOUtils.closeStream(outputStream);</span><br><span class="line">    IOUtils.closeStream(inputStream);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 拼接两块数据</span></span><br><span class="line"><span class="comment">// 由于当前是 windows 环境，使用 cmd 窗口拼接。</span></span><br><span class="line"><span class="comment">// cmd 进入两块所在目录</span></span><br><span class="line"><span class="comment">// 命令：type hadoop.part2 &gt;&gt; hadoop.part1</span></span><br><span class="line"><span class="comment">// 再把 part1 的后缀改为 tar.gz 即可查看</span></span><br></pre></td></tr></table></figure><hr><h1 id="HDFS-读写数据流程"><a href="#HDFS-读写数据流程" class="headerlink" title="HDFS 读写数据流程"></a>HDFS 读写数据流程</h1><h2 id="HDFS-写数据流程"><a href="#HDFS-写数据流程" class="headerlink" title="HDFS 写数据流程"></a>HDFS 写数据流程</h2><ol><li>使用 FileSystem.get 创建一个<code>分布式文件系统</code>客户端，向 NameNode 请求上传文件</li><li>NameNode 检查 HDFS 中是否有待上传的文件（根据路径、文件名判断），如果存在该文件，则报错<code>文件已存在</code></li><li>如果 NameNode 检查后，HDFS 没有待上传的文件，则开始响应上传文件</li><li>请求上传第一个 block（根据配置不同，block 大小也不同），此时会向 DataNode 请求，由 DataNode 决定可以上传到哪几个节点上</li><li>DataNode 返回可以上传的节点（判断条件：节点距离，负载）</li><li>FileSystem 创建输出流（FsDataOutputStream），与 DataNode 建立通道（串行）</li><li>DataNode 应答，所有可上传节点应答成功后，开始传输数据</li><li>所有数据传输完成后，通知 NameNode</li></ol><p><img src="/images/hadoop/client/write-data.png" alt="hdfs 写数据流程"></p><h3 id="节点距离计算"><a href="#节点距离计算" class="headerlink" title="节点距离计算"></a>节点距离计算</h3><p><code>节点距离：两个节点到最近的共同祖先的距离总和</code></p><p>HDFS 写数据过程中，NameNode 会选择距离上传数据最近距离的 DataNode 接收数据，此时需要计算节点距离。</p><p><img src="/images/hadoop/client/distance.png" alt="节点距离"></p><blockquote><p>(d1-r1-n0, d1-r1-n0)，由于这两个节点在同一个服务器上，此时距离为 0。即：同一节点上的进程距离为 0。<br>(d1-r1-n1, d1-r1-n2)，由于这两个节点都在同一个机架上，所以 n1、n2 的共同祖先都为 r1，此时距离为 1+1=2<br>(d1-r2-n0, d1-r3-n2)，这两个节点在同一个集群的不同机架上，即这两个节点的共同祖先为 d1，节点到集群还需要经过机架，所以这两个节点到共同祖先的距离都为 2，则节点距离为 2+2=4<br>(d1-r2-n1, d2-r4-n1)，这两个节点也不在同一个集群，则共同祖先为最外围的“网段”，此时每个节点到“网段”的距离都为 3，所以节点距离为 3+3=6</p></blockquote><h3 id="机架感知（副本存储节点选择）"><a href="#机架感知（副本存储节点选择）" class="headerlink" title="机架感知（副本存储节点选择）"></a>机架感知（副本存储节点选择）</h3><p>默认情况下，当副本数为 3 时，HDFS 的副本策略是在 <code>本地机架</code> 上的一个节点放置一个副本，在 <code>本地机架的另外一个节点</code> 上放置一个副本，最后再 <code>另外一个机架</code> 的不同节点上防止最后一个副本。</p><p>老版本的 hadoop 正好相反，是在 <code>本地机架</code> 上放置一个副本，在 <code>另外一个机架</code> 上放置 2 个副本。</p><p><img src="/images/hadoop/client/fuben.png" alt="默认情况下副本选择情况"></p><h2 id="HDFS-读数据流程"><a href="#HDFS-读数据流程" class="headerlink" title="HDFS 读数据流程"></a>HDFS 读数据流程</h2><ol><li>客户端请求下载文件（向 NameNode 发送请求）</li><li>NameNode 返回目标文件元数据</li><li>客户端创建输入流</li><li>客户端请求读取数据（根据距离决定从那个 DataNode 获取数据）</li><li>DataNode 传输数据</li></ol><p><img src="/images/hadoop/client/read-data.png" alt="读数据流程"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此前，完成了一个基础的完全分布式集群，并且使用 Java 程序代码实现测试连通了 Hadoop 集群，且在 HDFS 中创建了一个文件夹。由此开始学习 Hadoop 的一些 Java API 操作。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="hdfs" scheme="https://www.laiyy.top/categories/hadoop/hdfs/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="hdfs" scheme="https://www.laiyy.top/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop（5） &lt;br/&gt; HDFS</title>
    <link href="https://www.laiyy.top/hadoop/hdfs/hadoop-5.html"/>
    <id>https://www.laiyy.top/hadoop/hdfs/hadoop-5.html</id>
    <published>2019-09-22T09:01:31.000Z</published>
    <updated>2019-11-26T09:01:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>在此前，已经成功启动、测试了 hadoop 集群的功能，了解了部分 hadoop 知识，下面就需要开始针对 hadoop 进行继续深入学习 HDFS、MapReduce 的知识。</p><a id="more"></a><h1 id="HDFS-概述"><a href="#HDFS-概述" class="headerlink" title="HDFS 概述"></a>HDFS 概述</h1><blockquote><p>HDFS 是一种分布式文件管理系统，用于文件存储，通过目录树来定位文件；其次，由于是分布式的，由多台服务器联合起来实现功能。<br>HDFS 使用场景：适合一次写入、多次独处的场景，且<code>不支持文件的修改</code>，适合用于做数据分析，不适合做网盘应用。</p></blockquote><h2 id="HDFS-的优缺点"><a href="#HDFS-的优缺点" class="headerlink" title="HDFS 的优缺点"></a>HDFS 的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><blockquote><p>高容错性</p></blockquote><p>数据自动保存多个副本。通过增加副本的形式，提供了容错性。默认 3 个副本，当有其中一个副本挂掉了，会在其他服务器上再增加一个副本，保证最多有三个副本存在。且三个副本不能在同一个机器上</p><blockquote><p>适合处理大数据</p></blockquote><p>数据规模：能够处理 GB、TB、PB 级别数据<br>文件规模：能够处理百万以上的文件数量</p><blockquote><p>可以构建在廉价机器上，通过多副本机制，提高可靠性</p></blockquote><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><blockquote><p>不适合低延迟数据访问：如毫秒级数据存储</p></blockquote><blockquote><p>无法高效的对大量小文件进行存储</p><blockquote><p>存储大量小文件的话，会占用 NameNode 大量内存来存储文件目录和块信息，这样是不可取的。NameNode 的内存总是有限的<br>小文件存储的寻址时间会超过读取时间，违反了 HDFS 的设计目标</p></blockquote></blockquote><blockquote><p>不支持并发写入、文件随机修改</p><blockquote><p>一个文件只能有一个写，不允许多个线程同时写<br>仅支持数据的追加(append)，不支持文件随机修改</p></blockquote></blockquote><h2 id="HDFS-组成架构"><a href="#HDFS-组成架构" class="headerlink" title="HDFS 组成架构"></a>HDFS 组成架构</h2><p><img src="/images/hadoop/hdfs/hdfs.png" alt="HDFS"></p><blockquote><p>NameNode(nn)：就是 Master，是一个管理者</p></blockquote><p>管理HDFS 的命名空间；配置副本策略；管理数据块映射信息；处理客户端读写请求</p><blockquote><p>DataNode：就是 Slave。NameNode 下达命令，DataNode 执行实际操作。</p></blockquote><p>存储实际的数据块；执行数据块的读/写操作</p><blockquote><p>Client：客户端</p></blockquote><ol><li>文件切分。文件上传到 HDFS 的时候，Client 将文件切分成一个一个的 Block（默认 128M）然后进行上传</li><li>与 NameNode 交互，获取文件位置信息</li><li>与 DataNode 交互，读取、写入数据</li><li>提供一些命令管理 HDFS，如 NameNode 格式化</li><li>通过一些命令访问 HDFS，如对 HDFS 的增删改查</li></ol><blockquote><p>Secondary NameNode：非 NameNode 热备，当 NameNode 挂掉后，并不会马上替换 NameNode 提供服务</p></blockquote><ol><li>辅助 NameNode，分担其工作，如：定期合并 Fsimage(镜像文件)、Edits(编辑日志)，并推送到 NameNode</li><li>紧急情况下辅助恢复 NameNode，但是可能会丢失数据</li></ol><h2 id="HDFS-文件块大小"><a href="#HDFS-文件块大小" class="headerlink" title="HDFS 文件块大小"></a>HDFS 文件块大小</h2><p>HDFS 中的文件上是分块存储（Block），大小可通过参数配置 (dfs.blocksize)，默认在 2.X 中为 128M，1.X 为 64M</p><p>HDFS 块大小设置主要取决于磁盘传输速度。</p><hr><h1 id="自带-Shell-操作"><a href="#自带-Shell-操作" class="headerlink" title="自带 Shell 操作"></a>自带 Shell 操作</h1><h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><p>bin/hadoop fs 具体命令<br>bin/hdfs dfs 具体命令</p><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><ol><li><p>启动集群</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br><span class="line">sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li><li><p>获取帮助文档</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">help</span> [<span class="built_in">command</span>]</span><br></pre></td></tr></table></figure></li><li><p>查看 HDFS 目录信息</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls</span><br><span class="line">hadoop fs -l -R [dir path] <span class="comment"># 递归查询</span></span><br></pre></td></tr></table></figure></li><li><p>在 HDFS 上创建目录</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p [your dir path]<span class="comment"># 创建多级目录</span></span><br></pre></td></tr></table></figure></li><li><p>将本地文件剪切到 HDFS</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -moveFromLocal [<span class="built_in">local</span> file] [hdfs]</span><br></pre></td></tr></table></figure></li><li><p>追加一个文件到已经存在的文件末尾</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -appendToFile [<span class="built_in">local</span> file] [hdfs]</span><br></pre></td></tr></table></figure></li></ol><p>可能的报错信息 1：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">appendToFile: Failed to APPEND_FILE /user/laiyy/haha.txt for DFSClient_NONMAPREDUCE_-1628325628_1 on 192.168.233.131 because lease recovery is in progress. Try again later.</span><br></pre></td></tr></table></figure></p><p>可能的报错信息 2：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ava.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.233.131:50010,DS-f6860e33-55fb-44b1-9b95-4a61b0264267,DISK], DatanodeInfoWithStorage[192.168.233.133:50010,DS-8191d13c-f9c0-4d3c-8e3d-fa29d8a76ee5,DISK]], original=[DatanodeInfoWithStorage[192.168.233.131:50010,DS-f6860e33-55fb-44b1-9b95-4a61b0264267,DISK], DatanodeInfoWithStorage[192.168.233.133:50010,DS-8191d13c-f9c0-4d3c-8e3d-fa29d8a76ee5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via &apos;dfs.client.block.write.replace-datanode-on-failure.policy&apos; in its configuration.</span><br></pre></td></tr></table></figure></p><p>错误原因：<br>1、使用 jps 查看三台机器上的 DataNode 是否都存在，如果缺少了某个 DataNode，则会出现这种错误。<br>2、如果三台 DataNode 都存在，则查看三台机器上的 <code>%HADOOP_HOME%/data/dfs/data/current/VERSION</code> 和 <code>%HADOOP_HOME%/data/dfs/name/current/VERSION</code> 文件，对比三台机器上的文件，查看 namenode 的 <code>namespaceID</code>、<code>clusterID</code> 是否一致，查看 datanode 的 <code>storageID</code>、<code>clusterID</code> 是否一致。如果不一致，则会出现这种错误。</p><p>解决办法：</p><blockquote><p>第一步：停止集群 <code>sbin/stop-dfs.sh</code>、<code>sbin/stop-yarn.sh</code><br>第二步：删除 <code>%HADOOP_HOME%/data</code> 下的数据<br>第三步：格式化 NameNode <code>bin/hdfs namenode -format</code><br>第四步：重启集群 <code>sbin/start-dfs.sh</code>、<code>sbin/start-yarn.sh</code></p></blockquote><ol start="7"><li>将本地文件复制到 HDFS</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyFromLocal [<span class="built_in">local</span> file] [hdfs]</span><br><span class="line">hadoop fs -put [<span class="built_in">local</span> file] [hdfs]</span><br></pre></td></tr></table></figure><ol start="8"><li>从 HDFS 拷贝到本地 </li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyToLocal [hdfs] [<span class="built_in">local</span> path]</span><br><span class="line">hadoop fs -get [hdfs] [<span class="built_in">local</span> path]</span><br></pre></td></tr></table></figure><ol start="9"><li>从 HDFS 的一个路径，拷贝到另外一个路径</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp [hdfs] [hdfs]</span><br></pre></td></tr></table></figure><ol start="10"><li>从 HDFS 的一个路径，剪切到另外一个路径</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv [hdfs] [hdfs]</span><br></pre></td></tr></table></figure><ol start="11"><li>合并下载多个文件</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -getmerge [hadfs] [<span class="built_in">local</span> path]</span><br><span class="line"><span class="comment"># like：[hadoop fs -getmerge /user/laiyy/* merge.txt]</span></span><br></pre></td></tr></table></figure><ol start="12"><li>查看文件</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -tail [hdfs txt file]</span><br></pre></td></tr></table></figure><ol start="13"><li>删除文件</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm [hdfs]</span><br></pre></td></tr></table></figure><ol start="14"><li>删除空目录</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rmdir [hdfs empty dir]</span><br></pre></td></tr></table></figure><ol start="15"><li>统计文件夹大小信息</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -du -s -h [hdfs]</span><br></pre></td></tr></table></figure><ol start="16"><li>设置 HDFS 文件副本数量</li></ol><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -setrep [num] [hdfs]</span><br></pre></td></tr></table></figure><hr><h1 id="HDFS-客户端环境测试"><a href="#HDFS-客户端环境测试" class="headerlink" title="HDFS 客户端环境测试"></a>HDFS 客户端环境测试</h1><h2 id="pom"><a href="#pom" class="headerlink" title="pom"></a>pom</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.12.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="测试使用客户端创建目录"><a href="#测试使用客户端创建目录" class="headerlink" title="测试使用客户端创建目录"></a>测试使用客户端创建目录</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"><span class="comment">// 指定 NameNode（从 core-site.xml 中获取）</span></span><br><span class="line">configuration.set(<span class="string">"fs.defaultFS"</span>,<span class="string">"hdfs://hadoop02:9000"</span>);</span><br><span class="line"><span class="comment">// 获取 hdfs 客户端</span></span><br><span class="line">FileSystem fileSystem = FileSystem.get(configuration);</span><br><span class="line"><span class="comment">// 在 hdfs 上创建路径</span></span><br><span class="line">fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/laiyy"</span>));</span><br><span class="line"><span class="comment">// 关闭资源</span></span><br><span class="line">fileSystem.close();</span><br></pre></td></tr></table></figure><p>运行结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.security.AccessControlException: Permission denied: user=Administrator, access=WRITE, inode=&quot;/laiyy&quot;:root:supergroup:drwxr-xr-x</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)</span><br><span class="line">at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1720)</span><br></pre></td></tr></table></figure></p><p>错误原因： 使用 win10 调用 hadoop，用户为 <code>Administrator</code>，而 HDFS 的用户为 <code>root</code>，用户权限不足</p><p>解决方法： 在运行 main 方法时，动态的给定一hadoop用户值。</p><p><img src="/images/hadoop/client/hadoop-user-name.png" alt="Hadoop username"></p><p>再次运行：</p><p><img src="/images/hadoop/client/client-create-dir.png" alt="client create dir"></p><h2 id="另一种方式"><a href="#另一种方式" class="headerlink" title="另一种方式"></a>另一种方式</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"><span class="comment">// 获取 hdfs 客户端；参数1：NameNode地址，参数2：配置信息，参数3：hadoop 用户</span></span><br><span class="line">FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hadoop02:9000"</span>), configuration, <span class="string">"root"</span>);</span><br><span class="line"><span class="comment">// 在 hdfs 上创建路径</span></span><br><span class="line">fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/laiyy"</span>));</span><br><span class="line"><span class="comment">// 关闭资源</span></span><br><span class="line">fileSystem.close();</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在此前，已经成功启动、测试了 hadoop 集群的功能，了解了部分 hadoop 知识，下面就需要开始针对 hadoop 进行继续深入学习 HDFS、MapReduce 的知识。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="hdfs" scheme="https://www.laiyy.top/categories/hadoop/hdfs/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="hdfs" scheme="https://www.laiyy.top/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop（4） &lt;br/&gt; 完全分布式</title>
    <link href="https://www.laiyy.top/hadoop/hadoop-4.html"/>
    <id>https://www.laiyy.top/hadoop/hadoop-4.html</id>
    <published>2019-09-21T09:01:31.000Z</published>
    <updated>2019-09-21T09:01:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>在之前的例子中，完成了一个单机版的 Hadoop + HDFS + YARN 的搭建过程，并成功执行测试成功。<br>本例中将尝试搭建一个完全分布式的 Hadoop 集群，并验证测试。</p><a id="more"></a><h1 id="Hadoop-集群准备"><a href="#Hadoop-集群准备" class="headerlink" title="Hadoop 集群准备"></a>Hadoop 集群准备</h1><p>Hadoop 集群需要准备至少三台 Linux 主机（<code>hadoop02</code>、<code>hadoop03</code>、<code>hadoop04</code>），本例使用 VM 模拟三台 CentOS 7 主机。三台主机环境需要一致，都需要关闭防火墙、设置静态 ip、设置主机名称、JDK、Hadoop 等安装</p><h2 id="虚拟机准备"><a href="#虚拟机准备" class="headerlink" title="虚拟机准备"></a>虚拟机准备</h2><p>以 hadoop01 为源，克隆三台虚拟机 <code>hadoop02</code>、<code>hadoop03</code>、<code>hadoop04</code></p><h2 id="编写一个集群分发脚本"><a href="#编写一个集群分发脚本" class="headerlink" title="编写一个集群分发脚本"></a>编写一个集群分发脚本</h2><p>需要 linux 中事先安装有 <code>rsync</code> 脚本。如果执行命令 <code>rsync</code> 提示没有此命令，在 CentOS 下，执行 <code>yum install -y rsync</code> 安装即可。</p><p>需求：循环复制文件到所有节点的相同目录下</p><p>在 hadoop01 的 root 用户家目录中，创建一个 bin 目录用于存放脚本，并创建 <code>xsync</code> 脚本文件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取输入参数的个数，如果没有参数，直接退出</span></span><br><span class="line">params=<span class="variable">$#</span></span><br><span class="line"><span class="keyword">if</span>((params==0)); <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'please input param'</span></span><br><span class="line"><span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取文件名称</span></span><br><span class="line">param_1=<span class="variable">$1</span></span><br><span class="line">file_name=`basename <span class="variable">$param_1</span>`</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'file_name is $file_name'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取上级目录的绝对路径</span></span><br><span class="line">dir=`<span class="built_in">cd</span> -P $(dirname <span class="variable">$param_1</span>); <span class="built_in">pwd</span>`</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'dir is $dir'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环 hadoop02-04，同步文件</span></span><br><span class="line"><span class="keyword">for</span>((host=2;host&lt;=4;host++)); <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">'rsync file to hadoop0$host'</span></span><br><span class="line">    rsync -rvl <span class="variable">$dir</span>/<span class="variable">$file_name</span> <span class="variable">$user</span>@hadoop0<span class="variable">$host</span>:<span class="variable">$dir</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'rsync file done'</span></span><br></pre></td></tr></table></figure><p>需要注意：</p><ol><li>执行当前脚本的 user 必须有权限操作当前主机待分发的文件或文件夹；也必须在对应的需要分发的主机上拥有此用户，且有操作对应文件夹的权限。</li><li>待分发的文件、文件夹必须使用绝对路径，以保证同步到对应主机的位置也是正确的</li></ol><blockquote><p>测试分发脚本</p></blockquote><p>执行 <code>xsync /root/bin</code>，在 hadoop02-04 上查看 /root 目录下是否同步了 xsync 脚本</p><p><img src="/images/hadoop/full-dis/xsync.png" alt="xsync"></p><p><strong><em>注意：如果 xsync 命令识别不了，可以把 xsync 文件放置在 /usr/local/bin 目录下</em></strong></p><h1 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h1><h2 id="集群部署规划"><a href="#集群部署规划" class="headerlink" title="集群部署规划"></a>集群部署规划</h2><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">hadoop02</th><th style="text-align:center">hadoop03</th><th style="text-align:center">hadoop04</th></tr></thead><tbody><tr><td style="text-align:center">HDFS</td><td style="text-align:center">NamdeNode、DataNode</td><td style="text-align:center">DataNode</td><td style="text-align:center">SecondaryNameNode</td></tr><tr><td style="text-align:center">YARN</td><td style="text-align:center">NodeManager</td><td style="text-align:center">ResourceManager、NodeManager</td><td style="text-align:center">NodeManager</td></tr></tbody></table><p>需要保证：</p><blockquote><p>NameNode 和 SecondaryNameNode 不在一台机器上<br>ResourceManager 上没有 NameNode 和 SecondaryNameNode，防止内存消耗过大</p></blockquote><h2 id="核心配置文件"><a href="#核心配置文件" class="headerlink" title="核心配置文件"></a>核心配置文件</h2><blockquote><p>core-site.xml</p></blockquote><p>在 <code>hadoop02</code>  上，修改 core-site.xml 文件</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 hdfs 地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop02:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 hadoop 运行时的数据存储位置，可以不存在，启动时会自动创建 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>HDFS</p></blockquote><p>修改 hadoop-env.sh 中的 JAVA_HOME <code>export JAVA_HOME=/opt/module/jdk1.8.0_144</code></p><p>修改 hdfs-site.xml 文件<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 副本数量改为3 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 hadoop 辅助名称节点主机配置（SecondaryNameNode，2nn） --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop04:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><blockquote><p>YARN</p></blockquote><p>修改 yarn-env.sh、mapred-env.sh 中的 JAVA_HOME <code>export JAVA_HOME=/opt/module/jdk1.8.0_144</code></p><p>修改 yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 reduce 获取数据的方式（洗牌） --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 resourcemanager 的 地址  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 开启日志聚集</span></span><br><span class="line"><span class="comment">    &lt;property&gt;</span></span><br><span class="line"><span class="comment">        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span></span><br><span class="line"><span class="comment">        &lt;value&gt;true&lt;/value&gt;</span></span><br><span class="line"><span class="comment">    &lt;/property&gt;</span></span><br><span class="line"><span class="comment">     --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 日志保留时间 7 天，单位：秒 </span></span><br><span class="line"><span class="comment">    &lt;property&gt;</span></span><br><span class="line"><span class="comment">        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span></span><br><span class="line"><span class="comment">        &lt;value&gt;604800&lt;/value&gt;</span></span><br><span class="line"><span class="comment">    &lt;/property&gt;</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改 mapred-site.xml<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 MR 运行在 YARN 上  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 修改历史服务器地址</span></span><br><span class="line"><span class="comment">    &lt;property&gt;</span></span><br><span class="line"><span class="comment">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span></span><br><span class="line"><span class="comment">        &lt;value&gt;hadoop01:10020&lt;/value&gt;</span></span><br><span class="line"><span class="comment">    &lt;/property&gt;</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改历史服务器 WebUI 地址</span></span><br><span class="line"><span class="comment">    &lt;property&gt;</span></span><br><span class="line"><span class="comment">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span></span><br><span class="line"><span class="comment">        &lt;value&gt;hadoop01:19888&lt;/value&gt;</span></span><br><span class="line"><span class="comment">    &lt;/property&gt;</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><blockquote><p>将修改后的配置文件同步到 03、04 上</p></blockquote><p><code>xsync /opt/module/hadoop-2.7.2/etc/hadoop/</code> 即可</p><h1 id="集群启动"><a href="#集群启动" class="headerlink" title="集群启动"></a>集群启动</h1><h2 id="集群单点启动"><a href="#集群单点启动" class="headerlink" title="集群单点启动"></a>集群单点启动</h2><blockquote><p>第一次启动需要格式化 NameNode</p></blockquote><p><code>bin/hdfs namenode -format</code></p><blockquote><p>启动</p></blockquote><p>在 <code>hadoop02</code> 上执行 <code>sbin/hadoop-daemon.sh start namenode</code> 及 <code>sbin/hadoop-daemon.sh start datanode</code></p><p>在 <code>hadoop03</code> 上执行 <code>sbin/hadoop-daemon.sh start datanode</code></p><p>在 <code>hadoop04</code> 上执行 <code>sbin/hadoop-daemon.sh start datanode</code></p><h2 id="集群群起"><a href="#集群群起" class="headerlink" title="集群群起"></a>集群群起</h2><h3 id="ssh-免密登陆"><a href="#ssh-免密登陆" class="headerlink" title="ssh 免密登陆"></a>ssh 免密登陆</h3><p>在 <code>hadoop02</code> 上使用 <code>ssh-keygen</code> 生成秘钥，将生成后的 <code>id_rsa.pub</code> 文件中的内容，拷贝到 <code>hadoop03</code>、<code>hadoop04</code> 中</p><p>在 <code>hadoop02</code> 中使用 <code>ssh-copy-id hadoop02</code>、<code>ssh-copy-id hadoop03</code>、<code>ssh-copy-id hadoop04</code> 命令拷贝公钥。</p><p>如果不将公钥拷贝到本机，使用 <code>ssh hadoop02</code> 登陆本机时也需要输入密码。</p><p>此时即完成 hadoop02 对 03、04 的免密登陆，以同样的方式，在 03、04 上实现免密登陆</p><h3 id="群起集群-HDFS"><a href="#群起集群-HDFS" class="headerlink" title="群起集群 HDFS"></a>群起集群 HDFS</h3><blockquote><p>配置 slaves</p></blockquote><p>在 hadoop02 中配置修改 <code>etc/hadoop/slaves</code> 文件，注意：文件内容不允许有空格，不允许有空行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim etc/hadoop/slaves </span><br><span class="line"></span><br><span class="line">hadoop02</span><br><span class="line">hadoop03</span><br><span class="line">hadoop04</span><br></pre></td></tr></table></figure><blockquote><p>分发到 03、04 上： <code>xsync etc/hadoop/slaves</code></p></blockquote><blockquote><p>停止现有的服务，群起服务</p></blockquote><p>在 hadoop02 中，使用命令 <code>sbin/start-dfs.sh</code> 群起服务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# sbin/start-dfs.sh</span><br><span class="line">Starting namenodes on [hadoop02]</span><br><span class="line">hadoop02: starting namenode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-root-namenode-hadoop02.out</span><br><span class="line">hadoop02: starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-root-datanode-hadoop02.out</span><br><span class="line">hadoop04: starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-root-datanode-hadoop04.out</span><br><span class="line">hadoop03: starting datanode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-root-datanode-hadoop03.out</span><br><span class="line">Starting secondary namenodes [hadoop04]</span><br><span class="line">hadoop04: starting secondarynamenode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-root-secondarynamenode-hadoop04.out</span><br></pre></td></tr></table></figure></p><p>使用 jps 在三台服务器上查看启动情况</p><h3 id="群起集群-YARN"><a href="#群起集群-YARN" class="headerlink" title="群起集群 YARN"></a>群起集群 YARN</h3><p><strong><em>注意：</em></strong> <code>由于在 yarn-site.xml 中存在配置 yarn.resourcemanager.hostname 的值为 hadoop03，所以在群起 yarn 时，必须在 hadoop03 上启动，否则会报错</code></p><p>使用命令 <code>sbin/start-yarn.sh</code> 群起 YARN<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop03 hadoop-2.7.2]# sbin/start-yarn.sh </span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-root-resourcemanager-hadoop03.out</span><br><span class="line">hadoop02: starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-root-nodemanager-hadoop02.out</span><br><span class="line">hadoop04: starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-root-nodemanager-hadoop04.out</span><br><span class="line">hadoop03: starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-root-nodemanager-hadoop03.out</span><br></pre></td></tr></table></figure></p><h3 id="查看启动结果"><a href="#查看启动结果" class="headerlink" title="查看启动结果"></a>查看启动结果</h3><blockquote><p>hadoop02</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hadoop-2.7.2]# jps</span><br><span class="line">2353 DataNode</span><br><span class="line">2837 NodeManager</span><br><span class="line">2985 Jps</span><br><span class="line">2221 NameNode</span><br></pre></td></tr></table></figure><blockquote><p>hadoop03</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop03 hadoop-2.7.2]# jps</span><br><span class="line">2544 ResourceManager</span><br><span class="line">1932 DataNode</span><br><span class="line">2829 NodeManager</span><br><span class="line">2991 Jps</span><br></pre></td></tr></table></figure><blockquote><p>hadoop04</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop04 hadoop-2.7.2]# jps</span><br><span class="line">1745 DataNode</span><br><span class="line">2329 Jps</span><br><span class="line">1804 SecondaryNameNode</span><br><span class="line">2175 NodeManager</span><br></pre></td></tr></table></figure><hr><h1 id="集群时间同步"><a href="#集群时间同步" class="headerlink" title="集群时间同步"></a>集群时间同步</h1><p>使用 crontab 定时同步时间</p><h2 id="crontab-定时任务"><a href="#crontab-定时任务" class="headerlink" title="crontab 定时任务"></a>crontab 定时任务</h2><blockquote><p>基本语法： <code>crontab [command]</code></p></blockquote><table><thead><tr><th style="text-align:center">command</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">-e</td><td style="text-align:center">编辑 crontab 定时任务</td></tr><tr><td style="text-align:center">-l</td><td style="text-align:center">查询 crontab 任务</td></tr><tr><td style="text-align:center">-r</td><td style="text-align:center">删除当前用户所有的 crontab 任务</td></tr></tbody></table><blockquote><p>crontab 语法参数</p></blockquote><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">含义</th><th style="text-align:center">范围</th></tr></thead><tbody><tr><td style="text-align:center">第一个 *</td><td style="text-align:center">一小时当中的第几分钟</td><td style="text-align:center">0~59</td></tr><tr><td style="text-align:center">第二个 *</td><td style="text-align:center">一天当中的第几个小时</td><td style="text-align:center">0~23</td></tr><tr><td style="text-align:center">第三个 *</td><td style="text-align:center">一个月当中的第几天</td><td style="text-align:center">1~31</td></tr><tr><td style="text-align:center">第四个 *</td><td style="text-align:center">一年当中的第几个月</td><td style="text-align:center">1~12</td></tr><tr><td style="text-align:center">第五个 *</td><td style="text-align:center">一周当中的星期几</td><td style="text-align:center">0~7(0 和 7 都代表星期日)</td></tr></tbody></table><blockquote><p>crontab 特殊符号</p></blockquote><table><thead><tr><th style="text-align:center">特殊符号</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">*</td><td style="text-align:center">代表任何时间</td></tr><tr><td style="text-align:center">,</td><td style="text-align:center">代表不连续的时间。如：“0 8,12,16 <em> </em> *”，代表每天 8点0分，12点0分，16点0分 执行</td></tr><tr><td style="text-align:center">-</td><td style="text-align:center">代表连续时间范围。如：“0 5 <em> </em> 1-6”，代表 周一到周六的凌晨5点0分 执行</td></tr><tr><td style="text-align:center">*/n</td><td style="text-align:center">代表每隔多久执行一次。如：“<em>10 </em> <em> </em> *”，代表每隔 10 分钟执行一次</td></tr></tbody></table><h2 id="时间同步"><a href="#时间同步" class="headerlink" title="时间同步"></a>时间同步</h2><h3 id="时间服务器-hadoop02"><a href="#时间服务器-hadoop02" class="headerlink" title="时间服务器(hadoop02)"></a>时间服务器(hadoop02)</h3><blockquote><p>查看 ntp 是否安装</p></blockquote><p>在 hadoop02 中，执行 <code>rpm -qa | grep ntp</code>，如果没有任何输出，则代表 ntp 没有安装。没有安装的情况下，执行 <code>yum install -y ntp</code> 进行安装。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 ~]# rpm -qa | grep ntp</span><br><span class="line">ntpdate-4.2.6p5-29.el7.centos.x86_64</span><br><span class="line">ntp-4.2.6p5-29.el7.centos.x86_64</span><br></pre></td></tr></table></figure></p><blockquote><p>修改 ntp 配置文件</p></blockquote><p>修改 <code>/etc/ntp.conf</code> 文件</p><ul><li>授权网段 <code>192.168.x.0-192.168.x.255</code> 上的机器都可以从 hadoop02 上查询和同步时间</li></ul><p>打开第 17 行的注释，修改网段即可。</p><p>将 <code>#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</code> 修改为 <code>restrict 192.168.233.0 mask 255.255.255.0 nomodify notrap</code></p><ul><li>修改集群在局域网中不使用其他互联网时间</li></ul><p>将 <code>第21~24行</code> 注释即可。</p><ul><li>当该节点(hadoop02) 丢失网络连接，依然可用采用本地时间作为时间服务器为集群中的其他节点提供时间同步</li></ul><p>在上一步的位置增加如下配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure></p><p><strong><em>注意</em></strong>：<br><br><code>127.0.0.1</code> 是本地地址 <br><br><code>127.127.1.0</code> 是回环地址</p><blockquote><p>修改 /etc/sysconfig/ntpd 文件</p></blockquote><p>在文件中增加 <code>SYNC_HWCLOCK=yes</code>，代表 保证硬件时间与系统时间一起同步</p><blockquote><p>启动/重启ntpd</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 ~]# systemctl status ntpd.service</span><br><span class="line">● ntpd.service - Network Time Service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: inactive (dead)  # 可以看到此时未启动</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop02 ~]# systemctl start ntpd.service</span><br><span class="line">[root@hadoop02 ~]# systemctl status ntpd.service</span><br><span class="line">● ntpd.service - Network Time Service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 一 2019-09-23 17:04:23 CST; 3s ago # 可以看到此时是运行状态</span><br><span class="line">  Process: 1437 ExecStart=/usr/sbin/ntpd -u ntp:ntp $OPTIONS (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 1438 (ntpd)</span><br><span class="line">   CGroup: /system.slice/ntpd.service</span><br><span class="line">           └─1438 /usr/sbin/ntpd -u ntp:ntp -g</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>设置开机启动：<code>systemctl enable ntpd.service</code></p><h3 id="其他机器-03、04"><a href="#其他机器-03、04" class="headerlink" title="其他机器(03、04)"></a>其他机器(03、04)</h3><p>在 03、04 上，使用 crontab 同步 02 的时间。</p><p>先随便修改一个时间 <code>date -s &#39;2018-11-11 11:11:11&#39;</code>，然后编写 crontab 脚本，每分钟从 hadoop02 上同步时间。等待一分钟后再次查看时间。</p><hr><h1 id="hadoop-源码编译"><a href="#hadoop-源码编译" class="headerlink" title="hadoop 源码编译"></a>hadoop 源码编译</h1><p>在 apache 的 hadoop 官方网站上，hadoop 源码是 32 位的，当需要 64 位的 hadoop 时，就需要重新编译源码</p><p>需要准备：</p><blockquote><p>能联网的 CentOS、hadoop 源码包、jdk 64 位，apache-ant、maven、protobuf 序列化框架</p></blockquote><p>本例使用版本：</p><blockquote><p>hadoop：1.7.2<br>apache-ant：1.9.9<br>maven：3.0.5<br>protobuf：2.5.0<br>jdk：1.8.0_144 64 bit</p></blockquote><p>另外需要安装插件：<code>yum install -y glibc-headers gcc-c++ make cmake openssl ncurses-devel</code> </p><p><strong><em>注意：所有操作必须在 root 用户下完成</em></strong></p><h2 id="安装-protobuf"><a href="#安装-protobuf" class="headerlink" title="安装 protobuf"></a>安装 protobuf</h2><p>解压 protobuf-2.5.0 到 /opt/module/，进入 /opt/module/protobuf-2.5.0/ 文件夹，依次执行下列命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make check</span><br><span class="line">make install</span><br><span class="line">ldconfig</span><br></pre></td></tr></table></figure></p><p>修改环境变量，设置 protobuf 的环境到 PATH 中。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0</span><br><span class="line">export PATH=$PATH:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure></p><p>验证 protobuf 安装是否成功 <code>protoc --version</code></p><h2 id="编译源码"><a href="#编译源码" class="headerlink" title="编译源码"></a>编译源码</h2><p>执行 <code>tar -zxf hadoop-2.7.2-src.tar.gz</code> 解压，然后执行 <code>mvn package -Pdist,native -DskipTests -Dtar</code>，成功后，编译好的 64 位安装包就在 hadoop-2.7.2-src/hadoop-dist/target 下。编译期间报错的话继续在此执行此命令就行。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在之前的例子中，完成了一个单机版的 Hadoop + HDFS + YARN 的搭建过程，并成功执行测试成功。&lt;br&gt;本例中将尝试搭建一个完全分布式的 Hadoop 集群，并验证测试。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop（3） &lt;br/&gt; yarn、history server、logs server</title>
    <link href="https://www.laiyy.top/hadoop/hadoop-3.html"/>
    <id>https://www.laiyy.top/hadoop/hadoop-3.html</id>
    <published>2019-09-20T09:01:31.000Z</published>
    <updated>2019-09-20T09:01:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>在上例中，测试了 HDFS 以及在 HDFS 下执行 MapReduce 示例。本例中，测试启动 YARN，并在 YARN 中执行 MapReduce 程序、并查看历史执行信息和执行日志信息</p><a id="more"></a><h1 id="配置-YARN"><a href="#配置-YARN" class="headerlink" title="配置 YARN"></a>配置 YARN</h1><h2 id="配置-yarn-env-sh"><a href="#配置-yarn-env-sh" class="headerlink" title="配置 yarn-env.sh"></a>配置 yarn-env.sh</h2><p>修改 yarn-env.sh 中的 JAVA_HOME</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim etc/hadoop/yarn-env.sh</span><br></pre></td></tr></table></figure><p>将第 23 行的注释去掉，修改 JAVA_HOME 为 <code>export JAVA_HOME=/opt/module/jdk1.8.0_144</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">22 # some Java parameters</span><br><span class="line">23 # export JAVA_HOME=/home/y/libexec/jdk1.6.0/</span><br><span class="line">24 if [ &quot;$JAVA_HOME&quot; != &quot;&quot; ]; then</span><br><span class="line">25   #echo &quot;run java in $JAVA_HOME&quot;</span><br><span class="line">26   JAVA_HOME=$JAVA_HOME</span><br><span class="line">27 fi</span><br></pre></td></tr></table></figure><h2 id="配置-yarn-site-xml"><a href="#配置-yarn-site-xml" class="headerlink" title="配置 yarn-site.xml"></a>配置 yarn-site.xml</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">vim etc/hadoop/yarn-site.xml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 reduce 获取数据的方式（洗牌） --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 resourcemanager 的 地址  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="配置-mapred-env-sh"><a href="#配置-mapred-env-sh" class="headerlink" title="配置 mapred-env.sh"></a>配置 mapred-env.sh</h2><p>修改 mapred-env.sh 的 JAVA_HOME：第 16 行去掉注释，修改 JAVA_HOME 为 <code>export JAVA_HOME=/opt/module/jdk1.8.0_144</code></p><h2 id="创建并修改-mapred-site-xml-文件"><a href="#创建并修改-mapred-site-xml-文件" class="headerlink" title="创建并修改 mapred-site.xml 文件"></a>创建并修改 mapred-site.xml 文件</h2><p>拷贝 <code>etc/hadoop/mapred-site.xml.template</code> 文件为 <code>mapred-site.xml</code> 文件，指定 MR 运行在 YARN 上<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">vim mapred-site.xml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 MR 运行在 YARN 上  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h1 id="启动-YARN"><a href="#启动-YARN" class="headerlink" title="启动 YARN"></a>启动 YARN</h1><p>在保证 NameNode 和 DataNode 启动的情况下，启动 ResourceManager 和 NodeManager。</p><p>进入 <code>/opt/module/hadoop-2.7.2</code>，利用 <code>yarn-daemon.sh</code> 启动。 注意启动顺序：ResourceManager 先启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sbin/yarn-daemon.sh start resourcemanager</span><br><span class="line"></span><br><span class="line">sbin/yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><p>查看是否启动成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 hadoop-2.7.2]# jps</span><br><span class="line">1345 NameNode</span><br><span class="line">1505 DataNode</span><br><span class="line">2068 ResourceManager</span><br><span class="line">2327 NodeManager</span><br></pre></td></tr></table></figure><h2 id="查看-Web-UI"><a href="#查看-Web-UI" class="headerlink" title="查看 Web UI"></a>查看 Web UI</h2><p>访问 hadoop01:50070，hdfs 正常使用，继续访问 hadoop01:8088，查看 MapReduce 程序运行进程</p><p>50070：HDFS<br>8088：MapReduce</p><p><img src="/images/hadoop/yarn-history-and-log/map-reduce-8088.png" alt="map reduce 进程"></p><h2 id="测试-8088-的-MapReduce"><a href="#测试-8088-的-MapReduce" class="headerlink" title="测试 8088 的 MapReduce"></a>测试 8088 的 MapReduce</h2><p>删除 hdfs 中的 /user/laiyy/output： <code>bin/hdfs dfs -rm -r /user/laiyy/output</code></p><p>执行 MapReduce word count</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/laiyy/input /user/laiyy/output</span><br></pre></td></tr></table></figure><p>控制台打印日志如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">19/09/20 17:35:36 INFO mapreduce.Job: The url to track the job: http://hadoop01:8088/proxy/application_1568971486858_0001/</span><br><span class="line">19/09/20 17:35:36 INFO mapreduce.Job: Running job: job_1568971486858_0001</span><br><span class="line">19/09/20 17:35:46 INFO mapreduce.Job: Job job_1568971486858_0001 running in uber mode : false</span><br><span class="line">19/09/20 17:35:46 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">19/09/20 17:35:52 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">19/09/20 17:35:58 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">19/09/20 17:35:59 INFO mapreduce.Job: Job job_1568971486858_0001 completed successfully</span><br></pre></td></tr></table></figure></p><blockquote><p>在什么地方执行的任务：The url to track the job: <a href="http://hadoop01:8088/proxy/application_1568971486858_0001/" target="_blank" rel="noopener">http://hadoop01:8088/proxy/application_1568971486858_0001/</a><br>map、reduce 执行流程：map x% reduce x%<br>执行结果：Job job_1568971486858_0001 completed successfully</p></blockquote><p>在 8088 上查看执行信息<br><img src="/images/hadoop/yarn-history-and-log/yarn-map-reduce.png" alt="yarn map reduce"></p><h2 id="配置-Yarn-历史运行服务器"><a href="#配置-Yarn-历史运行服务器" class="headerlink" title="配置 Yarn 历史运行服务器"></a>配置 Yarn 历史运行服务器</h2><p>在 8088 上，某任务执行结束后，可以在进度条后看到有一个 <code>history</code> 选项卡，此选项卡可以查看历史运行记录。但是在没有配置历史运行服务器的时候，此选项卡打开后是 404，要想看到历史执行记录，需要配置 <code>历史运行服务器</code></p><p>配置方式：</p><blockquote><ol><li>修改 mapred-site.xml 文件</li><li>启动历史服务器</li><li>查看 JobHistory</li></ol></blockquote><ul><li>修改 mapred-site.xml 文件</li></ul><p>打开 mapred-site.xml 文件，增加如下配置<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定 MR 运行在 YARN 上  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 修改历史服务器地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改历史服务器 WebUI 地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><ul><li>启动历史服务器</li></ul><p>执行命令 <code>sbin/mr-jobhistory-daemon.sh start historyserver</code></p><p>查看启动是否成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 hadoop-2.7.2]# jps</span><br><span class="line">1505 ResourceManager</span><br><span class="line">2373 JobHistoryServer</span><br><span class="line">1753 NodeManager</span><br><span class="line">1370 DataNode</span><br><span class="line">1292 NameNode</span><br></pre></td></tr></table></figure><p>访问 8088 中的 <code>history</code> 选项卡，查看历史执行记录</p><p>如果出现下面的情况，是因为在本机没有在 hosts 配置 <code>hadoop01</code> 的地址，修改本机 hosts 文件，增加上 hadoop01 的映射即可<br><img src="/images/hadoop/yarn-history-and-log/error-history.png" alt="error history"></p><p>配置好 hosts 后，刷新页面，即可看到该任务的执行流程<br><img src="/images/hadoop/yarn-history-and-log/history-success.png" alt="history success"></p><h2 id="日志服务器"><a href="#日志服务器" class="headerlink" title="日志服务器"></a>日志服务器</h2><p>Logs 选项卡可以查看整个执行过程的相关日志，此时点击 Logs 选项卡，会提示如下信息<br><img src="/images/hadoop/yarn-history-and-log/no-logs.png" alt="没有日志服务器"></p><p>按照提示信息，我们需要配置 <code>日志服务器</code></p><blockquote><p>日志聚集的概念：应用运行完成后，将程序运行日志的信息上传到 HDFS 系统上<br>日志聚集的好处：可以方便的查看到程序运行详情，方便开发调试</p></blockquote><p><strong><em>注意</em></strong>：<br>开启日志聚集功能，需要重启 <code>NodeManager</code>、<code>ResourceManager</code>、<code>HistoryManager</code></p><p>开启日志聚集的步骤：</p><blockquote><ol><li>停止服务</li><li>修改配置 <code>yarn-site.xml</code></li><li>重新启动</li><li>执行测试</li></ol></blockquote><ul><li>停止服务</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 hadoop-2.7.2]# sbin/mr-jobhistory-daemon.sh stop historyserver</span><br><span class="line">stopping historyserver</span><br><span class="line">[root@hadoop01 hadoop-2.7.2]# sbin/yarn-daemon.sh stop nodemanager</span><br><span class="line">stopping nodemanager</span><br><span class="line">nodemanager did not stop gracefully after 5 seconds: killing with kill -9</span><br><span class="line">[root@hadoop01 hadoop-2.7.2]# sbin/yarn-daemon.sh stop resourcemanager</span><br><span class="line">stopping resourcemanager</span><br><span class="line">[root@hadoop01 hadoop-2.7.2]# jps</span><br><span class="line">3332 Jps</span><br><span class="line">1370 DataNode</span><br><span class="line">1292 NameNode</span><br></pre></td></tr></table></figure><ul><li>配置 yarn-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 reduce 获取数据的方式（洗牌） --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 resourcemanager 的 地址  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 开启日志聚集 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 日志保留时间 7 天，单位：秒 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>重新启动</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 hadoop-2.7.2]# sbin/yarn-daemon.sh start resourcemanager</span><br><span class="line">starting resourcemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-root-resourcemanager-hadoop01.out</span><br><span class="line">[root@hadoop01 hadoop-2.7.2]# sbin/yarn-daemon.sh start nodemanager</span><br><span class="line">starting nodemanager, logging to /opt/module/hadoop-2.7.2/logs/yarn-root-nodemanager-hadoop01.out</span><br><span class="line">[root@hadoop01 hadoop-2.7.2]# sbin/mr-jobhistory-daemon.sh start historyserver</span><br><span class="line">starting historyserver, logging to /opt/module/hadoop-2.7.2/logs/mapred-root-historyserver-hadoop01.out</span><br><span class="line">[root@hadoop01 hadoop-2.7.2]# jps</span><br><span class="line">3825 Jps</span><br><span class="line">3622 NodeManager</span><br><span class="line">3784 JobHistoryServer</span><br><span class="line">1370 DataNode</span><br><span class="line">1292 NameNode</span><br><span class="line">3373 ResourceManager</span><br></pre></td></tr></table></figure><ul><li>执行测试</li></ul><p>删除 hdfs 上的 output 文件夹，重新执行 word count 示例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -rm -r /user/laiyy/output</span><br><span class="line"></span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/laiyy/input /user/laiyy/output</span><br></pre></td></tr></table></figure><p>在 8088 上选择最后一个执行的任务，进入 <code>history</code> 选项卡，再进入 <code>Logs</code> 选项卡查看结果<br><img src="/images/hadoop/yarn-history-and-log/log.png" alt="logs"></p><hr><h1 id="配置文件的说明"><a href="#配置文件的说明" class="headerlink" title="配置文件的说明"></a>配置文件的说明</h1><p>Hadoop 配置文件分为量类：默认配置文件、自定义配置文件。自定义配置文件的优先级更高</p><h2 id="默认配置文件"><a href="#默认配置文件" class="headerlink" title="默认配置文件"></a>默认配置文件</h2><table><thead><tr><th style="text-align:center">默认配置文件</th><th style="text-align:center">存放位置</th></tr></thead><tbody><tr><td style="text-align:center">core-default.xml</td><td style="text-align:center">hadoop-common-xxx.jar/core-default.xml</td></tr><tr><td style="text-align:center">hdfs-default.xml</td><td style="text-align:center">hadoop-hdfs-xxx.jar/hdfs-default.xml</td></tr><tr><td style="text-align:center">yarn-default.xml</td><td style="text-align:center">hadoop-yarn-common-xxx.jar/yarn-default.xml</td></tr><tr><td style="text-align:center">mapred-defaultt.xml</td><td style="text-align:center">hadoop-mapreduce-client-core-xxx.jar/mapred-default.xml</td></tr></tbody></table><h2 id="自定义配置文件"><a href="#自定义配置文件" class="headerlink" title="自定义配置文件"></a>自定义配置文件</h2><p><code>core-site.xml</code>、<code>hdfs-site.xml</code>、<code>yarn-site.xml</code>、<code>mapred-site.xml</code> 存放在 <code>$HADOOP_HOME/etc/hadoop</code> 文件夹下，可根据需求修改配置</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上例中，测试了 HDFS 以及在 HDFS 下执行 MapReduce 示例。本例中，测试启动 YARN，并在 YARN 中执行 MapReduce 程序、并查看历史执行信息和执行日志信息&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop（2） &lt;br/&gt; 伪分布式</title>
    <link href="https://www.laiyy.top/hadoop/hadoop-2.html"/>
    <id>https://www.laiyy.top/hadoop/hadoop-2.html</id>
    <published>2019-09-20T03:43:06.000Z</published>
    <updated>2019-09-20T03:43:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>配置伪分布式集群，需要注意修改对应的 hdfs 配置文件、JAVA_HOME、副本备份个数等信息。另外在启动集群之前，需要格式化 NameNode（只有第一次启动需要格式化）</p><a id="more"></a><h1 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h1><h2 id="修改-core-site-xml-文件"><a href="#修改-core-site-xml-文件" class="headerlink" title="修改 core-site.xml 文件"></a>修改 core-site.xml 文件</h2><p>修改 core-site.xml 中关于 hdfs、数据存储等配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/module/hadoop-2.7.2/etc/hadoop</span><br><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure><p>将 <code>configutation</code> 标签内容修改为：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 hdfs 地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改 hadoop 运行时的数据存储位置，可以不存在，启动时会自动创建 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/etc/hadoop/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="修改-hadoop-env-sh"><a href="#修改-hadoop-env-sh" class="headerlink" title="修改 hadoop-env.sh"></a>修改 hadoop-env.sh</h2><p>修改 hadoop 的默认 JDK 路径，如果不修改配置，则可能在分布式集群环境下导致 JAVA_HOME 失效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br></pre></td></tr></table></figure><p>将 JAVA_HOME 从原来的 <code>export JAVA_HOME=${JAVA_HOME}</code>，修改为 <code>/opt/module/jdk1.8.0_144/</code></p><h2 id="修改-hdfs-site-xml"><a href="#修改-hdfs-site-xml" class="headerlink" title="修改 hdfs-site.xml"></a>修改 hdfs-site.xml</h2><p>指定 hdfs 副本的数量为 1 个，默认为 3 个<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><hr><h1 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h1><h2 id="格式化-NameNode"><a href="#格式化-NameNode" class="headerlink" title="格式化 NameNode"></a>格式化 NameNode</h2><p>第一次启动集群时，需要格式化 NameNode，后面如果再启动时不需要格式化 NameNode</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/module/hadoop-2.7.2</span><br><span class="line"></span><br><span class="line">bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><p>控制台输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">19/09/20 15:23:31 INFO namenode.FSNamesystem: Retry cache on namenode is enabled</span><br><span class="line">19/09/20 15:23:31 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis</span><br><span class="line">19/09/20 15:23:31 INFO util.GSet: Computing capacity for map NameNodeRetryCache</span><br><span class="line">19/09/20 15:23:31 INFO util.GSet: VM type       = 64-bit</span><br><span class="line">19/09/20 15:23:31 INFO util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB</span><br><span class="line">19/09/20 15:23:31 INFO util.GSet: capacity      = 2^15 = 32768 entries</span><br><span class="line">19/09/20 15:23:31 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1194915434-192.168.233.130-1568964211854</span><br><span class="line">19/09/20 15:23:31 INFO common.Storage: Storage directory /opt/module/hadoop-2.7.2/etc/hadoop/data/tmp/dfs/name has been successfully formatted.</span><br><span class="line">19/09/20 15:23:31 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">19/09/20 15:23:31 INFO util.ExitUtil: Exiting with status 0</span><br><span class="line">19/09/20 15:23:31 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at hadoop01/192.168.233.130</span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure></p><p>当看到控制台输出 <code>SHUTDOWN_MSG: Shutting down NameNode at hadoop01/192.168.233.130</code> 时，即为格式化完成。</p><h2 id="启动-NameNode、DataNode"><a href="#启动-NameNode、DataNode" class="headerlink" title="启动 NameNode、DataNode"></a>启动 NameNode、DataNode</h2><p>启动 NameNode</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 hadoop-2.7.2]# sbin/hadoop-daemon.sh start namenode</span><br><span class="line">starting namenode, logging to /opt/module/hadoop-2.7.2/logs/hadoop-root-namenode-hadoop01.out</span><br></pre></td></tr></table></figure><p>启动 DataNode<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/hadoop-daemon.sh start datanode</span><br></pre></td></tr></table></figure></p><h2 id="验证是否启动成功"><a href="#验证是否启动成功" class="headerlink" title="验证是否启动成功"></a>验证是否启动成功</h2><blockquote><p>JPS 验证</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 hadoop-2.7.2]# jps</span><br><span class="line">1345 NameNode</span><br><span class="line">1505 DataNode</span><br><span class="line">1583 Jps</span><br></pre></td></tr></table></figure><blockquote><p>Web UI 验证</p></blockquote><p>浏览器访问 hadoop01 的 ip:50070，查看 WebUI 是否可以访问</p><p>集群的基础信息：<br><img src="/images/hadoop/hdfs-info/hdfs-1.png" alt="hdfs"></p><p>集群的详细信息：<br><img src="/images/hadoop/hdfs-info/hdfs-2.png" alt="hdfs"></p><p>DataNode 信息：<br><img src="/images/hadoop/hdfs-info/hdfs-3.png" alt="hdfs"></p><p>HDFS 文件管理系统：<br><img src="/images/hadoop/hdfs-info/hdfs-4.png" alt="hdfs"></p><hr><h1 id="HDFS-管理"><a href="#HDFS-管理" class="headerlink" title="HDFS 管理"></a>HDFS 管理</h1><h2 id="创建一个文件夹"><a href="#创建一个文件夹" class="headerlink" title="创建一个文件夹"></a>创建一个文件夹</h2><p>在 hdfs 根目录下再创建其他文件夹以保存文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -mkdir -p /user/laiyy/input</span><br></pre></td></tr></table></figure><p>然后在 WebUI 中查看<br><img src="/images/hadoop/hdfs-info/hdfs-5.png" alt="hdfs"></p><h2 id="将-wcinput-文件夹里的东西上传到-hdfs-中"><a href="#将-wcinput-文件夹里的东西上传到-hdfs-中" class="headerlink" title="将 wcinput 文件夹里的东西上传到 hdfs 中"></a>将 wcinput 文件夹里的东西上传到 hdfs 中</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/module/demo</span><br><span class="line"></span><br><span class="line">../hadoop-2.7.2/bin/hdfs dfs -put wcinput/wc.input /user/laiyy/input</span><br></pre></td></tr></table></figure><p><code>-put</code> :上传文件</p><p>整体命令：将 wcinput 下的 wc.input 文件，上传到 hdfs 中的 /user/laiyy/input 下</p><p>验证上传结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 demo]# ../hadoop-2.7.2/bin/hdfs dfs -ls -R /user/laiyy/input</span><br><span class="line">-rw-r--r--   1 root supergroup         57 2019-09-20 16:20 /user/laiyy/input/wc.input</span><br></pre></td></tr></table></figure></p><p><img src="/images/hadoop/hdfs-info/hdfs-6.png" alt="hdfs"></p><h2 id="使用-hdfs-的文件路径运行-word-count-示例"><a href="#使用-hdfs-的文件路径运行-word-count-示例" class="headerlink" title="使用 hdfs 的文件路径运行 word count 示例"></a>使用 hdfs 的文件路径运行 word count 示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/laiyy/input /user/laiyy/output</span><br></pre></td></tr></table></figure><p>此时的 <code>input</code>、<code>output</code> 的路径都是 hdfs 的路径，不是linux的路径。</p><p>查看执行结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 hadoop-2.7.2]# bin/hdfs dfs -ls -R /user/laiyy</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-09-20 16:20 /user/laiyy/input</span><br><span class="line">-rw-r--r--   1 root supergroup         57 2019-09-20 16:20 /user/laiyy/input/wc.input</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-09-20 16:28 /user/laiyy/output</span><br><span class="line">-rw-r--r--   1 root supergroup          0 2019-09-20 16:28 /user/laiyy/output/_SUCCESS</span><br><span class="line">-rw-r--r--   1 root supergroup         55 2019-09-20 16:28 /user/laiyy/output/part-r-00000</span><br></pre></td></tr></table></figure></p><p><img src="/images/hadoop/hdfs-info/hdfs-7.png" alt="hdfs"></p><p><strong><em>直接查看 hdfs 中 output 中的执行结果</em></strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 hadoop-2.7.2]# bin/hdfs dfs -cat /user/laiyy/output/part-r-00000</span><br><span class="line">hadoop3</span><br><span class="line">hdfs1</span><br><span class="line">laiyy1</span><br><span class="line">laiyy07281</span><br><span class="line">mapreduce1</span><br><span class="line">yarn1</span><br></pre></td></tr></table></figure></p><hr><h1 id="关于-NameNode-格式化"><a href="#关于-NameNode-格式化" class="headerlink" title="关于 NameNode 格式化"></a>关于 NameNode 格式化</h1><p>一定不能经常格式化 NameNode。当需要格式化 NameNode 时，需要先用 jps 命令，查看一下 NameNode 和 DataNode 是否都已经关闭，如果没有关闭，需要关闭 NameNode 和 DataNode。</p><p>在关闭 NameNode 和 DataNode 的情况下，删除 <code>HADOOP_HOME</code> 下的 <code>data</code> 和 <code>log</code> 文件夹，然后执行 NameNode 格式化命令。</p><p>其中 <code>data</code> 文件夹可能不在 <code>HADOOP_HOME</code> 下，此时该文件夹在 <code>HADOOP_HOME/etc/hadoop</code> 下。</p><blockquote><p>为何在 DataNode 存在时不能格式化 NameNode？</p></blockquote><p>在 <code>data</code> 文件夹或 <code>data/tmp/dfs</code> 文件夹下，有两个文件夹，分别为 <code>data</code>、<code>name</code>，分别对应 DataNode 和 NameNode。</p><p>在 <code>data/current/</code> 和 <code>name/current/</code> 下，都有一个 <code>VERSION</code> 文件，在 <code>VERSION</code> 文件中，可以看到对应的信息：</p><ul><li><p>NameNode</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">namespaceID=1748201392</span><br><span class="line">clusterID=CID-7c41ca27-aaa9-4b85-9966-36af0e361a58</span><br><span class="line">cTime=0</span><br><span class="line">storageType=NAME_NODE</span><br><span class="line">blockpoolID=BP-1270008624-192.168.233.130-1568966288591</span><br><span class="line">layoutVersion=-63</span><br></pre></td></tr></table></figure></li><li><p>DataNode</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">storageID=DS-9be19535-bada-4b25-9076-f7260386a990</span><br><span class="line">clusterID=CID-7c41ca27-aaa9-4b85-9966-36af0e361a58</span><br><span class="line">cTime=0</span><br><span class="line">datanodeUuid=a4e2c662-26b0-4f7f-8bd4-f6012b54c3e7</span><br><span class="line">storageType=DATA_NODE</span><br><span class="line">layoutVersion=-56</span><br></pre></td></tr></table></figure></li></ul><p>对比两个文件，可以看到 DataNode 和 NameNode 中的 <code>clusterID</code> 完全一致。</p><p>此时，如果由于 DataNode 没有停止，或 <code>data</code> 文件夹没有清空，就格式化 NameNode 的话，会导致 NameNode 的 <code>clusterID</code> 重新生成， 此时，NameNode 和 DataNode 的 <code>clusterID</code> 不一致，导致崩溃。</p><blockquote><p>DataNode 和 NameNode 在 clusterID 不一致，导致只能同时只能有一个工作的问题 </p></blockquote><p><img src="/images/hadoop/hdfs-info/format-namenode-error.png" alt="format namenode error"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;配置伪分布式集群，需要注意修改对应的 hdfs 配置文件、JAVA_HOME、副本备份个数等信息。另外在启动集群之前，需要格式化 NameNode（只有第一次启动需要格式化）&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 学习（1）  &lt;br /&gt; 基础概念、基础环境搭建安装、官方示例</title>
    <link href="https://www.laiyy.top/hadoop/hadoop-1.html"/>
    <id>https://www.laiyy.top/hadoop/hadoop-1.html</id>
    <published>2019-09-17T15:19:13.000Z</published>
    <updated>2019-09-17T15:19:13.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><p>Hadoop 是 Apache 基金会所开发的分布式系统基础架构，主要解决海量数据的存储、分析计算问题。Hadoop 通常是指一个更广泛的概念：Hadoop 生态圈（包括 HBase、Spark 等）</p><a id="more"></a><h2 id="Hadoop-的三大发型版本"><a href="#Hadoop-的三大发型版本" class="headerlink" title="Hadoop 的三大发型版本"></a>Hadoop 的三大发型版本</h2><blockquote><p>Apache</p></blockquote><p>最原始版本，对于入门学习最好</p><blockquote><p>Cloudera</p></blockquote><p>收费，在大型互联网公司用的比较多</p><blockquote><p>Hortonworks</p></blockquote><p>文档比较好</p><h2 id="Hadoop-的优势"><a href="#Hadoop-的优势" class="headerlink" title="Hadoop 的优势"></a>Hadoop 的优势</h2><blockquote><p>高可靠性</p></blockquote><p>Hadoop 底层维护多个数据副本（默认3个），所以即使 Hadoop 某个计算元素或存储出现故障，也不会导致数据的丢失</p><blockquote><p>高扩展性</p></blockquote><p>在集群中分配任务数据，可方便的扩展数以千计的节点</p><blockquote><p>高效性</p></blockquote><p>在 MapReduce 思想下，Hadoop 是并行工作的，以加快任务处理速度</p><blockquote><p>高容错性</p></blockquote><p>自动将失败的任务重新分配执行</p><h2 id="1-x-与-2-x-的区别"><a href="#1-x-与-2-x-的区别" class="headerlink" title="1.x 与 2.x 的区别"></a>1.x 与 2.x 的区别</h2><p>Hadoop 包含的模块，以及 1.X、2.X 的区别：<br><img src="/images/hadoop/install/hadoop.png" alt="Hadoop"></p><hr><h1 id="Hadoop-三大组件"><a href="#Hadoop-三大组件" class="headerlink" title="Hadoop 三大组件"></a>Hadoop 三大组件</h1><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p>HDFS 架构：</p><ul><li>NameNode(nn)：<br>相当于一本书的目录；存储文件的元数据，如：文件名、目录结构、文件属性（生成时间、副本数、文件权限），以及每个文件的快列表和快所在的 DataNode</li><li>DateNode(dn)：<br>相当于目录对应的具体内容；在本地文件系统存储文件块数据，以及块数据的校验和</li><li>Secondary NameNode(2nn)：用来监控 HDFS 状态的辅助后台程序，每个一段时间获取 HDFS 元数据快照。</li></ul><h2 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h2><p>ResourceManager（相当于老板） &gt; NodeManager（相当于技术总监）/ApplicationMaster（相当于项目经理）</p><p>其中 NodeManager 负责某一个节点，ApplicationMaster 负责节点中的某个任务</p><ul><li><p>ResourceManager</p><blockquote><p>处理客户端请求：管理整个服务器集群资源（磁盘、cpu等）<br>监控 NodeManager<br>启动、监控 ApplicationMaster（在集群上运行的任务）<br>资源的分配、调度</p></blockquote></li><li><p>NodeManager</p><blockquote><p>管理单个节点上的资源<br>处理来自 ResourceManager 的命令<br>处理 ApplicationMaster 的命令</p></blockquote></li><li><p>ApplicationMaster</p><blockquote><p>负责数据切分<br>为应用程序申请资源并分配给内部任务<br>任务的监控和容错</p></blockquote></li><li><p>Container：为 ApplicationMaster 服务</p><blockquote><p>YARN 中资源的抽象，封装了节点上多维度资源，如：内存、CPU、磁盘、网络等，服务 ApplicationMaster</p></blockquote></li></ul><p><img src="/images/hadoop/install/yarn.png" alt="yarn"></p><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>MapReduce 将计算过程分为两个阶段：Map 阶段、Reduce 阶段</p><ul><li>Map 阶段：并行处理输入数据</li><li>Reduce 阶段：对 Map 结果进行汇总</li></ul><p><img src="/images/hadoop/install/map-reduce.png" alt="MapReduce"></p><h2 id="大数据生态体系"><a href="#大数据生态体系" class="headerlink" title="大数据生态体系"></a>大数据生态体系</h2><p><img src="/images/hadoop/install/life-cycle.png" alt="大数据生态体系"></p><hr><h1 id="Hadoop-环境搭建"><a href="#Hadoop-环境搭建" class="headerlink" title="Hadoop 环境搭建"></a>Hadoop 环境搭建</h1><p>环境准备</p><blockquote><p>CentOS 7<br>JDK 1.8<br>Hadoop 2.9.2</p></blockquote><h2 id="CentOS-设置"><a href="#CentOS-设置" class="headerlink" title="CentOS 设置"></a>CentOS 设置</h2><h3 id="修改网卡"><a href="#修改网卡" class="headerlink" title="修改网卡"></a>修改网卡</h3><p>使用 ROOT 用户，设置 NAT 模式网络连接、设置静态 IP （防止 dhcp 导致 ip 变化）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br></pre></td></tr></table></figure></p><p>禁用掉 ipv6，将获取 ip 的方式从 <code>dhcp</code> 改为 <code>static</code>，设置静态 ip 地址 <code>IPADDR</code>、网关 <code>GATEWAY</code>、dns <code>DNS1</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=no</span><br><span class="line">IPV6_AUTOCONF=no</span><br><span class="line">IPV6_DEFROUTE=no</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=ens33</span><br><span class="line">UUID=672a42a7-bbbb-453e-8ebe-ca0ae27eef49</span><br><span class="line">DEVICE=ens33</span><br><span class="line">ONBOOT=yes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">IPADDR=192.168.52.100</span><br><span class="line">GATEWAY=192.168.52.2</span><br><span class="line">DNS1=192.168.52.2</span><br></pre></td></tr></table></figure><p><strong><em> 注意：网关需要与虚拟机中的 NAT 网卡设置一致！</em></strong></p><p><img src="/images/hadoop/install/network-1.png" alt="网卡设置"><br><img src="/images/hadoop/install/network-2.png" alt="网卡设置"><br><img src="/images/hadoop/install/network-3.png" alt="网卡设置"></p><p><strong><em>另外，也需要将 VM8 网卡设置一个静态ip，且这个静态 ip 不能与虚拟机的静态 ip一致，必须在一个网段</em></strong><br><img src="/images/hadoop/install/network-4.png" alt="网卡设置"></p><h3 id="修改-hostname、hosts"><a href="#修改-hostname、hosts" class="headerlink" title="修改 hostname、hosts"></a>修改 hostname、hosts</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br><span class="line"></span><br><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=hadoop01</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line"></span><br><span class="line">192.168.52.100 hadoop01</span><br><span class="line">192.168.52.101 hadoop02</span><br><span class="line">192.168.52.102 hadoop03</span><br><span class="line">192.168.52.103 hadoop04</span><br></pre></td></tr></table></figure><p>其中，<code>hosts</code> 文件中的 <code>hadoop02</code>、<code>hadoop03</code>、<code>hadoop04</code> 暂时还没有，先配置上，为后面集群做准备。</p><h2 id="安装-JDK、Hadoop"><a href="#安装-JDK、Hadoop" class="headerlink" title="安装 JDK、Hadoop"></a>安装 JDK、Hadoop</h2><p>寻找一个文件夹或创建一个空文件夹，作为 jdk、hadoop 的安装目录。本例以 /opt 文件夹为例。</p><p>在 /opt 文件夹下创建 <code>module</code> 文件夹，存放 jdk、hadoop 安装文件，创建 <code>software</code> 文件夹，存在 jdk、hadoop 安装包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt</span><br><span class="line">mkdir module software</span><br></pre></td></tr></table></figure><p>利用 xftp 将下载好的 jdk1.8、hadoop 2.9.2 存入 software 文件夹下，解压安装到 module 文件夹下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd software</span><br><span class="line"></span><br><span class="line">tar -zxf jdk-8u144-linux-x64.tar.gz -C ../module/</span><br><span class="line">tar -zxf hadoop-2.9.2.tar.gz -C ../module/</span><br></pre></td></tr></table></figure><p>设置 jdk、hadoop 环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><p>在文件最后添加：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-2.9.2</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure></p><p>应用环境变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure></p><p>使用 <code>java</code>、<code>javac</code>、<code>hadoop</code> 命令，测试环境变量设置是否成功</p><hr><h1 id="官方案例-grep"><a href="#官方案例-grep" class="headerlink" title="官方案例[grep]"></a>官方案例[grep]</h1><h2 id="创建一个用于输入的文件夹"><a href="#创建一个用于输入的文件夹" class="headerlink" title="创建一个用于输入的文件夹"></a>创建一个用于输入的文件夹</h2><p>任意找一个地方，创建一个 input 文件夹，本例使用与 hadoop 统计目录。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p demo/input</span><br></pre></td></tr></table></figure></p><!-- more --><p>拷贝 hadoop/etc/hadoop/*.xml，到 input 文件夹，用于 grep 案例的输入源</p><h2 id="执行官方-demo"><a href="#执行官方-demo" class="headerlink" title="执行官方 demo"></a>执行官方 demo</h2><p>在 demo 文件夹下，执行 hadoop 官方 demo<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar ../hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input/ output &apos;dfs[a-z.]+&apos;</span><br></pre></td></tr></table></figure></p><p>解释：</p><blockquote><p>hadoop：以 hadoop 命令执行<br>jar：执行的是 jar 包<br>xx.jar：具体 jar 包，本例是 2.7.2 版本的官方 hadoop MapReduce 示例<br>grep：由于有多个示例，选择执行哪个示例，此处是执行 <code>grep</code> 案例<br>input：指明输入示例的文件夹<br>output：指明输出结果的文件夹，且这个文件夹必须不存在，否则会报<code>文件夹已存在</code>的错误<br>dfs[a-z.]+：正则过滤</p></blockquote><h2 id="查看控制台输出"><a href="#查看控制台输出" class="headerlink" title="查看控制台输出"></a>查看控制台输出</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">19/09/20 14:29:11 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id</span><br><span class="line">19/09/20 14:29:11 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=</span><br><span class="line">19/09/20 14:29:11 INFO input.FileInputFormat: Total input paths to process : 8</span><br><span class="line">19/09/20 14:29:11 INFO mapreduce.JobSubmitter: number of splits:8</span><br><span class="line">19/09/20 14:29:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1540340101_0001</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">19/09/20 14:29:13 INFO mapred.LocalJobRunner: reduce &gt; reduce</span><br><span class="line">19/09/20 14:29:13 INFO mapred.Task: Task &apos;attempt_local801724989_0002_r_000000_0&apos; done.</span><br><span class="line">19/09/20 14:29:13 INFO mapred.LocalJobRunner: Finishing task: attempt_local801724989_0002_r_000000_0</span><br><span class="line">19/09/20 14:29:13 INFO mapred.LocalJobRunner: reduce task executor complete.</span><br><span class="line">19/09/20 14:29:14 INFO mapreduce.Job: Job job_local801724989_0002 running in uber mode : false</span><br><span class="line">19/09/20 14:29:14 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">19/09/20 14:29:14 INFO mapreduce.Job: Job job_local801724989_0002 completed successfully</span><br><span class="line">19/09/20 14:29:14 INFO mapreduce.Job: Counters: 30</span><br><span class="line">File System Counters</span><br><span class="line">FILE: Number of bytes read=1158544</span><br><span class="line">FILE: Number of bytes written=2216290</span><br><span class="line">FILE: Number of read operations=0</span><br><span class="line">FILE: Number of large read operations=0</span><br><span class="line">FILE: Number of write operations=0</span><br><span class="line">Map-Reduce Framework</span><br><span class="line">Map input records=1</span><br><span class="line">Map output records=1</span><br><span class="line">Map output bytes=17</span><br><span class="line">Map output materialized bytes=25</span><br><span class="line">Input split bytes=120</span><br><span class="line">Combine input records=0</span><br><span class="line">Combine output records=0</span><br><span class="line">Reduce input groups=1</span><br><span class="line">Reduce shuffle bytes=25</span><br><span class="line">Reduce input records=1</span><br><span class="line">Reduce output records=1</span><br><span class="line">Spilled Records=2</span><br><span class="line">Shuffled Maps =1</span><br><span class="line">Failed Shuffles=0</span><br><span class="line">Merged Map outputs=1</span><br><span class="line">GC time elapsed (ms)=18</span><br><span class="line">Total committed heap usage (bytes)=273203200</span><br><span class="line">Shuffle Errors</span><br><span class="line">BAD_ID=0</span><br><span class="line">CONNECTION=0</span><br><span class="line">IO_ERROR=0</span><br><span class="line">WRONG_LENGTH=0</span><br><span class="line">WRONG_MAP=0</span><br><span class="line">WRONG_REDUCE=0</span><br><span class="line">File Input Format Counters </span><br><span class="line">Bytes Read=123</span><br><span class="line">File Output Format Counters </span><br><span class="line">Bytes Written=23</span><br></pre></td></tr></table></figure><h2 id="检查执行结果"><a href="#检查执行结果" class="headerlink" title="检查执行结果"></a>检查执行结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 demo]# ll output/</span><br><span class="line">总用量 4</span><br><span class="line">-rw-r--r--. 1 root root 11 9月  20 14:29 part-r-00000</span><br><span class="line">-rw-r--r--. 1 root root  0 9月  20 14:29 _SUCCESS</span><br></pre></td></tr></table></figure><p>看到生成了两个文件：<code>part-r-00000</code>、<code>_SUCCESS</code>。</p><p>其中，<code>_SUCCESS</code> 文件大小为 0，没有任何内容，只是标记当前执行成功了。</p><p>查看 <code>part-r-00000</code> 文件的内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 demo]# cat output/part-r-00000 </span><br><span class="line">1dfsadmin</span><br></pre></td></tr></table></figure></p><p>可以看到统计到的符合正则过滤的条件的单词，只有一个，为 <code>dfsadmin</code>。此时如果再次执行刚才的 hadoop 任务，控制台将报错如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">19/09/20 14:42:58 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized</span><br><span class="line">org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/opt/module/demo/output already exists</span><br><span class="line">at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146)</span><br><span class="line">at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:266)</span><br><span class="line">at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:139)</span><br></pre></td></tr></table></figure></p><hr><h1 id="官方案例-word-count"><a href="#官方案例-word-count" class="headerlink" title="官方案例[word count]"></a>官方案例[word count]</h1><h2 id="创建输入源"><a href="#创建输入源" class="headerlink" title="创建输入源"></a>创建输入源</h2><p>创建一个 <code>wcinput</code> 用于 word count 案例的输入源，并创建一个 <code>wc.input</code> 文件，输入一些字符串<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 demo]# mkdir wcinput</span><br><span class="line">[root@hadoop01 demo]# cd wcinput/</span><br><span class="line">[root@hadoop01 wcinput]# vim wc.input</span><br><span class="line">hadoop yarn</span><br><span class="line">hadoop mapreduce</span><br><span class="line">hadoop hdfs</span><br><span class="line">laiyy</span><br><span class="line">laiyy0728</span><br></pre></td></tr></table></figure></p><h2 id="执行-word-count-示例"><a href="#执行-word-count-示例" class="headerlink" title="执行 word count 示例"></a>执行 word count 示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar ../hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput/ wcoutput</span><br></pre></td></tr></table></figure><p>与上例的区别仅仅是将 <code>grep</code> 换为 <code>wordcount</code>，调整了输入、输出目录，去掉了正则过滤。</p><p>查看输出结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop01 demo]# cat wcoutput/part-r-00000 </span><br><span class="line">hadoop3</span><br><span class="line">hdfs1</span><br><span class="line">laiyy1</span><br><span class="line">laiyy07281</span><br><span class="line">mapreduce1</span><br><span class="line">yarn1</span><br></pre></td></tr></table></figure></p><p>可以看到每个单词出现的频率</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Hadoop&quot;&gt;&lt;a href=&quot;#Hadoop&quot; class=&quot;headerlink&quot; title=&quot;Hadoop&quot;&gt;&lt;/a&gt;Hadoop&lt;/h1&gt;&lt;p&gt;Hadoop 是 Apache 基金会所开发的分布式系统基础架构，主要解决海量数据的存储、分析计算问题。Hadoop 通常是指一个更广泛的概念：Hadoop 生态圈（包括 HBase、Spark 等）&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>RocketMQ（3） Rocket 集群</title>
    <link href="https://www.laiyy.top/rocketmq/rocketmq-3.html"/>
    <id>https://www.laiyy.top/rocketmq/rocketmq-3.html</id>
    <published>2019-04-21T11:29:16.000Z</published>
    <updated>2019-04-21T11:29:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>RocketMQ 集群模式分为四种：<code>单 master</code>、<code>多 master</code>、<code>多 master 多 slave 异步复制</code>、<code>多 master 多 slave 同步双写</code></p><a id="more"></a><h1 id="四种集群模式"><a href="#四种集群模式" class="headerlink" title="四种集群模式"></a>四种集群模式</h1><h2 id="单-master"><a href="#单-master" class="headerlink" title="单 master"></a>单 master</h2><p>风险较大，一旦 broker 宕机或者重启，将导致整个服务部可用。不建议线上环境使用</p><h2 id="多-master"><a href="#多-master" class="headerlink" title="多 master"></a>多 master</h2><p>一个集群全部都是 master，没有 slave</p><ul><li><p>优点<br>配置简单，单个 master 宕机，或者重启未付，对应用没有影响，在磁盘配置为 RAID10 时，即是机器宕机不可恢复的情况，消息也不会丢失（异步刷盘会丢失少量消息，同步刷盘不会丢失消息），性能最高</p></li><li><p>缺点<br>单个 broker 宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息的实时性会受到影响。</p></li></ul><h2 id="多-master-多-slave-异步复制"><a href="#多-master-多-slave-异步复制" class="headerlink" title="多 master 多 slave 异步复制"></a>多 master 多 slave 异步复制</h2><p>每个 master 配置一个 slave，有多对 master slave，HA 采用的是异步复制方式，主备有短暂的消息延迟（毫秒级），master 收到消息后立即向应用返回成功标志，同时向 slave 写入消息。</p><ul><li><p>优点<br>即是磁盘损坏，消息丢失的非常少，且消息的实时性不会受到影响。因为 master 宕机后，消费者仍然可以从 slave 消费，此过程对应用透明，不需要人工干预，性能同多个 master 模式一样</p></li><li><p>缺点<br>master 宕机，磁盘损坏下，会丢失少量消息</p></li></ul><h2 id="多-master-多-slave-同步双写"><a href="#多-master-多-slave-同步双写" class="headerlink" title="多 master 多 slave 同步双写"></a>多 master 多 slave 同步双写</h2><p>每个 master 配置一个 slave，有多对 master slave，HA 采用同步双写模式，主备都成功才会返回成功</p><ul><li><p>优点<br>数据与服务都无单点，master 宕机情况下，消息无延迟，服务可用性与数据可用性最高</p></li><li><p>缺点<br>性能比异步复制低 10% 左右，发送单个 master 的 RT 会略高，主机宕机后，slave 不能自动切换为主机（后续版本会支持）</p></li></ul><hr><h1 id="一主一从"><a href="#一主一从" class="headerlink" title="一主一从"></a>一主一从</h1><h2 id="修改-master-配置"><a href="#修改-master-配置" class="headerlink" title="修改 master 配置"></a>修改 master 配置</h2><p>进入 <code>conf/2m-2s-async</code>，修改文件：<code>broker-a-s.properties</code>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf broker-a-s.properties </span><br><span class="line">cp broker-a.properties  broker-a-s.properties</span><br></pre></td></tr></table></figure></p><p>然后打开 <code>broker-a-s.properties</code>，修改：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brokerId=1</span><br><span class="line">brokerRole=SLAVE</span><br></pre></td></tr></table></figure></p><p>修改两个配置文件的 nameserver 为两个服务器对应的 nameserver 地址，多个地址用英文分号分割</p><h2 id="修改-slave-配置"><a href="#修改-slave-配置" class="headerlink" title="修改 slave 配置"></a>修改 slave 配置</h2><p>将 master 的 <code>broker-a.properties</code>、<code>broker-a-s.properties</code> 同步过来，在 master 上执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp broker-a.properties 192.168.52.201:/usr/local/include/mq/rocketmq/conf/2m-2s-async/</span><br><span class="line">scp broker-a-s.properties 192.168.52.201:/usr/local/include/mq/rocketmq/conf/2m-2s-async/</span><br></pre></td></tr></table></figure></p><h2 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h2><p>依次启动 master、slave 的 nameserver<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./bin/mqnamesrv &amp;</span><br></pre></td></tr></table></figure></p><p>在 master 上使用 <code>broker-a.properties</code> 启动 broker<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup sh ./bin/mqbroker -c /usr/local/include/mq/rocketmq/conf/2m-2s-async/broker-a.properties &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p><p>在 slave 上使用 <code>broker-a-s.properties</code> 启动 broker<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup sh ./bin/mqbroker -c /usr/local/include/mq/rocketmq/conf/2m-2s-async/broker-a-s.properties &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p><h2 id="验证集群"><a href="#验证集群" class="headerlink" title="验证集群"></a>验证集群</h2><p>在 rocketmq-console 中，修改 nameserver 配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rocketmq.config.namesrvAddr=192.168.52.200:9876;192.168.52.201:9876</span><br></pre></td></tr></table></figure></p><p>启动 console，并查看集群属性<br><img src="/images/rocketmq/1-master-1-slave.png" alt="一主一从"></p><h2 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h2><p>当主节点挂掉后，消息将无法写入</p><hr><h1 id="双主双从"><a href="#双主双从" class="headerlink" title="双主双从"></a>双主双从</h1><p>双主双从，异步刷盘，同步复制（生产环境建议采用此方式）</p><h2 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h2><p>准备4份 RocketMQ 环境，修改配置文件 <code>conf/2m-2s-sync/broker-a.properties</code>，将 <code>brokerRole</code> 改为：<code>SYNC_MASTER</code>,<code>flushDiskType</code> 改为 <code>ASYNC_FLUSH</code>，nameserver 为四台服务器的 nameserver 地址其他与之前 async 的配置一样</p><p>修改 <code>conf/2m-2s-sync/broker-a-s.0properties</code> 的 <code>brokerId</code> 为大于 0 的值，<code>brokerRole</code> 为 <code>SLAVE</code>，nameserver 为四台服务器的 nameserver 地址。</p><p>修改 <code>conf/2m-2s-sync/broker-b.0properties</code>、<code>conf/2m-2s-sync/broker-b-2.0properties</code>，与 a 的区别在与 <code>brokerName</code> 都为 broker-b</p><h2 id="启动集群-1"><a href="#启动集群-1" class="headerlink" title="启动集群"></a>启动集群</h2><p>每台机器都启动 nameserveer<br><code>nohup ./bin/mqnamesrv &amp;</code></p><p>在第一台机器上启动 broker-a<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup sh ./bin/mqbroker -c /usr/local/include/mq/rocketmq/conf/2m-2s-sync/broker-a.properties &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p><p>在第二台机器上启动 broker-b<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup sh ./bin/mqbroker -c /usr/local/include/mq/rocketmq/conf/2m-2s-sync/broker-b.properties &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p><p>在第三台机器上启动 broker-a-s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup sh ./bin/mqbroker -c /usr/local/include/mq/rocketmq/conf/2m-2s-sync/broker-a-s.properties &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p><p>在第四台机器上启动 broker-b-s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup sh ./bin/mqbroker -c /usr/local/include/mq/rocketmq/conf/2m-2s-sync/broker-b-s.properties &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p><h2 id="验证集群-1"><a href="#验证集群-1" class="headerlink" title="验证集群"></a>验证集群</h2><p>修改 rocket-console 的配置：<code>rocketmq.config.namesrvAddr=192.168.52.200:9876;192.168.52.201:9876;192.168.52.202:9876;192.168.52.203:9876</code>，启动 console，打开 <code>集群选项卡</code>：<br><img src="/images/rocketmq/2-master-2-slave-sync.png" alt="双主双从-同步双写-异步刷盘"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RocketMQ 集群模式分为四种：&lt;code&gt;单 master&lt;/code&gt;、&lt;code&gt;多 master&lt;/code&gt;、&lt;code&gt;多 master 多 slave 异步复制&lt;/code&gt;、&lt;code&gt;多 master 多 slave 同步双写&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="rocketmq" scheme="https://www.laiyy.top/categories/rocketmq/"/>
    
    
  </entry>
  
  <entry>
    <title>RocketMQ（2）  顺序消息、事务消息</title>
    <link href="https://www.laiyy.top/rocketmq/rocketmq-2.html"/>
    <id>https://www.laiyy.top/rocketmq/rocketmq-2.html</id>
    <published>2019-04-21T09:29:47.000Z</published>
    <updated>2019-04-21T09:29:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>RocketMQ 顺序消息：消息有序是指可以按照消息发送顺序来消费。RocketMQ 可以严格的保证消息有序，但是这个顺序逼格不是全局顺序，只是分区(queue)顺序。要保证群居顺序，只能有一个分区。</p><a id="more"></a><h1 id="顺序消息"><a href="#顺序消息" class="headerlink" title="顺序消息"></a>顺序消息</h1><p>在 MQ 模型中，顺序要由三个阶段保证：</p><ul><li>消息被发送时，保持顺序</li><li>消息被存储时的顺序和发送的顺序一致</li><li>消息被消费时的顺序和存储的顺序一致</li></ul><p>发送时保持顺序，意味着对于有顺序要求的消息，用户应该在同一个线程中采用同步的方式发送。存储保持和发送的顺序一致，则要求在同一线程中被发送出来的消息 A/B，存储时 A 要在 B 之前。而消费保持和存储一致，则要求消息 A/B 到达 Consumer 之后必须按照先后顺序被处理。</p><p><img src="/images/rocketmq/order.png" alt="order"></p><h2 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.laiyy.study.rocketmqprovider.order;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.rocketmq.client.exception.MQBrokerException;</span><br><span class="line"><span class="keyword">import</span> org.apache.rocketmq.client.exception.MQClientException;</span><br><span class="line"><span class="keyword">import</span> org.apache.rocketmq.client.producer.DefaultMQProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.rocketmq.client.producer.MessageQueueSelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.rocketmq.client.producer.SendResult;</span><br><span class="line"><span class="keyword">import</span> org.apache.rocketmq.common.message.Message;</span><br><span class="line"><span class="keyword">import</span> org.apache.rocketmq.common.message.MessageQueue;</span><br><span class="line"><span class="keyword">import</span> org.apache.rocketmq.remoting.common.RemotingHelper;</span><br><span class="line"><span class="keyword">import</span> org.apache.rocketmq.remoting.exception.RemotingException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.UnsupportedEncodingException;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> laiyy</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2019/4/21 16:18</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> MQClientException, UnsupportedEncodingException, RemotingException, InterruptedException, MQBrokerException </span>&#123;</span><br><span class="line">        <span class="comment">// 1、创建 DefaultMQProducer</span></span><br><span class="line">        DefaultMQProducer producer = <span class="keyword">new</span> DefaultMQProducer(<span class="string">"demo-producer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2、设置 name server</span></span><br><span class="line">        producer.setNamesrvAddr(<span class="string">"192.168.52.200:9876"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3、开启 producer</span></span><br><span class="line">        producer.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 连续发送 5 条信息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">1</span>; index &lt;= <span class="number">5</span>; index++) &#123;</span><br><span class="line">            <span class="comment">// 创建消息</span></span><br><span class="line">            Message message = <span class="keyword">new</span> Message(<span class="string">"TOPIC_DEMO"</span>, <span class="string">"TAG_A"</span>, <span class="string">"KEYS_!"</span>, (<span class="string">"HELLO！"</span> + index).getBytes(RemotingHelper.DEFAULT_CHARSET));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 指定 MessageQueue，顺序发送消息</span></span><br><span class="line">            <span class="comment">// 第一个参数：消息体</span></span><br><span class="line">            <span class="comment">// 第二个参数：选中指定的消息队列对象（会将所有的消息队列传进来，需要自己选择）</span></span><br><span class="line">            <span class="comment">// 第三个参数：选择对应的队列下标</span></span><br><span class="line">            SendResult result = producer.send(message, <span class="keyword">new</span> MessageQueueSelector() &#123;</span><br><span class="line">                <span class="comment">// 第一个参数：所有的消息队列对象</span></span><br><span class="line">                <span class="comment">// 第二个参数：消息体</span></span><br><span class="line">                <span class="comment">// 第三个参数：传入的消息队列下标</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> MessageQueue <span class="title">select</span><span class="params">(List&lt;MessageQueue&gt; list, Message message, Object o)</span> </span>&#123;</span><br><span class="line">                    <span class="comment">// 获取队列下标</span></span><br><span class="line">                    <span class="keyword">int</span> index = (<span class="keyword">int</span>) o;</span><br><span class="line">                    <span class="keyword">return</span> list.get(index);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;, <span class="number">0</span>);</span><br><span class="line">            System.out.println(<span class="string">"发送第："</span> + index + <span class="string">" 条信息成功："</span> + result);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关闭 producer</span></span><br><span class="line">        producer.shutdown();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>控制台输出结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">发送第：1 条信息成功：SendResult [sendStatus=SEND_OK, msgId=C0A800677E4C18B4AAC26ACE66560000, offsetMsgId=C0A834C800002A9F00000000000000B8, messageQueue=MessageQueue [topic=TOPIC_DEMO, brokerName=broker-a, queueId=0], queueOffset=1]</span><br><span class="line">发送第：2 条信息成功：SendResult [sendStatus=SEND_OK, msgId=C0A800677E4C18B4AAC26ACE66630001, offsetMsgId=C0A834C800002A9F0000000000000171, messageQueue=MessageQueue [topic=TOPIC_DEMO, brokerName=broker-a, queueId=0], queueOffset=2]</span><br><span class="line">发送第：3 条信息成功：SendResult [sendStatus=SEND_OK, msgId=C0A800677E4C18B4AAC26ACE66660002, offsetMsgId=C0A834C800002A9F000000000000022A, messageQueue=MessageQueue [topic=TOPIC_DEMO, brokerName=broker-a, queueId=0], queueOffset=3]</span><br><span class="line">发送第：4 条信息成功：SendResult [sendStatus=SEND_OK, msgId=C0A800677E4C18B4AAC26ACE66690003, offsetMsgId=C0A834C800002A9F00000000000002E3, messageQueue=MessageQueue [topic=TOPIC_DEMO, brokerName=broker-a, queueId=0], queueOffset=4]</span><br><span class="line">发送第：5 条信息成功：SendResult [sendStatus=SEND_OK, msgId=C0A800677E4C18B4AAC26ACE666C0004, offsetMsgId=C0A834C800002A9F000000000000039C, messageQueue=MessageQueue [topic=TOPIC_DEMO, brokerName=broker-a, queueId=0], queueOffset=5]</span><br><span class="line">17:45:11.545 [NettyClientSelector_1] INFO RocketmqRemoting - closeChannel: close the connection to remote address[192.168.52.200:10909] result: true</span><br><span class="line">17:45:11.548 [NettyClientSelector_1] INFO RocketmqRemoting - closeChannel: close the connection to remote address[192.168.52.200:9876] result: true</span><br><span class="line">17:45:11.549 [NettyClientSelector_1] INFO RocketmqRemoting - closeChannel: close the connection to remote address[192.168.52.200:10911] result: true</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure></p><p>可以看到，所有消息的  <code>queueId</code> 都为 0，顺序消息生产成功。</p><h2 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> MQClientException </span>&#123;</span><br><span class="line">        <span class="comment">// 1、创建 DefaultMQPushConsumer</span></span><br><span class="line">        DefaultMQPushConsumer consumer = <span class="keyword">new</span> DefaultMQPushConsumer(<span class="string">"demo-consumer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2、设置 name server</span></span><br><span class="line">        consumer.setNamesrvAddr(<span class="string">"192.168.52.200:9876"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置消息拉取最大数</span></span><br><span class="line">        consumer.setConsumeMessageBatchMaxSize(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3、设置 subscribe</span></span><br><span class="line">        consumer.subscribe(<span class="string">"TOPIC_DEMO"</span>, <span class="comment">// 要消费的主题</span></span><br><span class="line">                <span class="string">"*"</span> <span class="comment">// 过滤规则</span></span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4、创建消息监听</span></span><br><span class="line">        consumer.registerMessageListener(<span class="keyword">new</span> MessageListenerOrderly() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> ConsumeOrderlyStatus <span class="title">consumeMessage</span><span class="params">(List&lt;MessageExt&gt; list, ConsumeOrderlyContext consumeOrderlyContext)</span> </span>&#123;</span><br><span class="line">                <span class="comment">// 5、获取消息信息</span></span><br><span class="line">                <span class="keyword">for</span> (MessageExt msg : list) &#123;</span><br><span class="line">                    <span class="comment">// 获取主题</span></span><br><span class="line">                    String topic = msg.getTopic();</span><br><span class="line">                    <span class="comment">// 获取标签</span></span><br><span class="line">                    String tags = msg.getTags();</span><br><span class="line">                    <span class="comment">// 获取信息</span></span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        String result = <span class="keyword">new</span> String(msg.getBody(), RemotingHelper.DEFAULT_CHARSET);</span><br><span class="line">                        System.out.println(<span class="string">"Consumer 消费信息：topic："</span> + topic+ <span class="string">"，tags："</span> + tags + <span class="string">"，消息体："</span> + result);</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                        <span class="keyword">return</span> ConsumeOrderlyStatus.SUSPEND_CURRENT_QUEUE_A_MOMENT;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 6、返回消息读取状态</span></span><br><span class="line">                <span class="keyword">return</span> ConsumeOrderlyStatus.SUCCESS;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 启动消费者</span></span><br><span class="line">        consumer.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>顺序消费者与之前的 demo 最大的不同，在于 <code>message listener</code> 从 <code>MessageListenerConcurrently</code> 变为 <code>MessageListenerOrderly</code>，消费标识从 <code>ConsumeConcurrentlyStatus</code> 变为 <code>ConsumeOrderlyStatus</code>。</p><p>查看控制台输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Consumer 消费信息：topic：TOPIC_DEMO，tags：TAG_A，消息体：HELLO！1</span><br><span class="line">Consumer 消费信息：topic：TOPIC_DEMO，tags：TAG_A，消息体：HELLO！2</span><br><span class="line">Consumer 消费信息：topic：TOPIC_DEMO，tags：TAG_A，消息体：HELLO！3</span><br><span class="line">Consumer 消费信息：topic：TOPIC_DEMO，tags：TAG_A，消息体：HELLO！4</span><br><span class="line">Consumer 消费信息：topic：TOPIC_DEMO，tags：TAG_A，消息体：HELLO！5</span><br></pre></td></tr></table></figure></p><hr><h1 id="事务消息"><a href="#事务消息" class="headerlink" title="事务消息"></a>事务消息</h1><p>在 RocketMQ 4.3 版本后，开放了事务消息。</p><h2 id="RocketMQ-事务消息流程"><a href="#RocketMQ-事务消息流程" class="headerlink" title="RocketMQ 事务消息流程"></a>RocketMQ 事务消息流程</h2><p>RocketMQ 的事务消息，只要是通过消息的异步处理，可以保证本地事务和消息发送同事成功执行或失败，从而保证数据的最终一致性。</p><p><img src="/images/rocketmq/transaction-message.png" alt="Transaction message"></p><p>MQ 事务消息解决分布式事务问题，但是第三方 MQ 支持事务消息的中间件不多，如 RockctMQ，它们支持事务的方式也是类似于采用二阶段提交，但是市面上一些主流的 MQ 都是不支持事务消息的，如：Kafka、RabbitMQ</p><p>以 RocketMQ 为例，事务消息实现思路大致为：</p><ul><li>第一阶段的 Prepared 消息，会拿到消息的地址</li><li>第二阶段执行本地事务</li><li>第三阶段通过第一阶段拿到的地址去访问消息，并修改状态</li></ul><p>也就是说，在业务方法内想要消息队列提交两次消息，一次发送消息和一次确认消息。如果确认消息发送失败，RocketMQ 会定期扫描消息集群中的事务消息。这时候发现了 prepared 消息，它会向消息发送者确认，所以生产方需要实现一个 check 接口。RocketMQ 会根据发送端设置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或同时失败。<br><img src="/images/rocketmq/transaction-message-1.png" alt="Transaction message"></p><p>事务消息的成功投递需要三个 Topic，分别是</p><ul><li>Half Topic：用于记录所有的 prepare 消息</li><li>Op Half Topic：记录以及提交了状态的 prepare 消息</li><li>Real Topic：事务消息真正的 topic，在 commit 后才会将消息写入该 topic，从而进行消息投递。</li></ul><h2 id="事务消息实现"><a href="#事务消息实现" class="headerlink" title="事务消息实现"></a>事务消息实现</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TransactionProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> MQClientException, UnsupportedEncodingException, RemotingException, InterruptedException, MQBrokerException </span>&#123;</span><br><span class="line">        <span class="comment">// 1、创建 TransactionMQProducer</span></span><br><span class="line">        TransactionMQProducer producer = <span class="keyword">new</span> TransactionMQProducer(<span class="string">"transaction-producer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2、设置 name server</span></span><br><span class="line">        producer.setNamesrvAddr(<span class="string">"192.168.52.200:9876"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3、指定消息监听对象，用于执行本地事务和消息回查</span></span><br><span class="line">        TransactionListenerImpl transactionListener = <span class="keyword">new</span> TransactionListenerImpl();</span><br><span class="line">        producer.setTransactionListener(transactionListener);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4、线程池</span></span><br><span class="line">        ThreadPoolExecutor executor = <span class="keyword">new</span> ThreadPoolExecutor(<span class="number">2</span>, <span class="number">5</span>, <span class="number">100</span>, TimeUnit.SECONDS, <span class="keyword">new</span> ArrayBlockingQueue&lt;Runnable&gt;(<span class="number">2000</span>), <span class="keyword">new</span> ThreadFactory() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Thread <span class="title">newThread</span><span class="params">(Runnable r)</span> </span>&#123;</span><br><span class="line">                Thread thread = <span class="keyword">new</span> Thread(r);</span><br><span class="line">                thread.setName(<span class="string">"client-transaction-msg-thread"</span>);</span><br><span class="line">                <span class="keyword">return</span> thread;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        producer.setExecutorService(executor);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5、开启 producer</span></span><br><span class="line">        producer.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6、创建消息</span></span><br><span class="line">        Message message = <span class="keyword">new</span> Message(<span class="string">"TRANSACTION_TOPIC"</span>, <span class="string">"TAG_A"</span>, <span class="string">"KEYS_!"</span>, <span class="string">"HELLO！TRANSACTION!"</span>.getBytes(RemotingHelper.DEFAULT_CHARSET));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7、发送消息</span></span><br><span class="line">        TransactionSendResult result = producer.sendMessageInTransaction(message, <span class="string">"hello-transaction"</span>);</span><br><span class="line"></span><br><span class="line">        System.out.println(result);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭 producer</span></span><br><span class="line">        producer.shutdown();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>事务消息监听器：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TransactionListenerImpl</span> <span class="keyword">implements</span> <span class="title">TransactionListener</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 存储对应书屋的状态信息， key：事务id，value：事务执行的状态</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> ConcurrentMap&lt;String, Integer&gt; maps = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 执行本地事务</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> message</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> o</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> LocalTransactionState <span class="title">executeLocalTransaction</span><span class="params">(Message message, Object o)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 事务id</span></span><br><span class="line">        String transactionId = message.getTransactionId();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 0：执行中，状态未知</span></span><br><span class="line">        <span class="comment">// 1：本地事务执行成功</span></span><br><span class="line">        <span class="comment">// 2：本地事务执行失败</span></span><br><span class="line"></span><br><span class="line">        maps.put(transactionId, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">"正在执行本地事务。。。。"</span>);</span><br><span class="line">            <span class="comment">// 模拟本地事务</span></span><br><span class="line">            TimeUnit.SECONDS.sleep(<span class="number">65</span>);</span><br><span class="line">            System.out.println(<span class="string">"本地事务执行成功。。。。"</span>);</span><br><span class="line">            maps.put(transactionId, <span class="number">1</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            maps.put(transactionId, <span class="number">2</span>);</span><br><span class="line">            <span class="keyword">return</span> LocalTransactionState.ROLLBACK_MESSAGE;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> LocalTransactionState.COMMIT_MESSAGE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 消息回查</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> messageExt</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> LocalTransactionState <span class="title">checkLocalTransaction</span><span class="params">(MessageExt messageExt)</span> </span>&#123;</span><br><span class="line">        String transactionId = messageExt.getTransactionId();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"正在执行消息回查，事务id："</span> + transactionId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取事务id的执行状态</span></span><br><span class="line">        <span class="keyword">if</span> (maps.containsKey(transactionId)) &#123;</span><br><span class="line">            <span class="keyword">int</span> status = maps.get(transactionId);</span><br><span class="line">            System.out.println(<span class="string">"消息回查状态："</span> + status);</span><br><span class="line">            <span class="keyword">switch</span> (status) &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">return</span> LocalTransactionState.UNKNOW;</span><br><span class="line">                <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">                    <span class="keyword">return</span> LocalTransactionState.COMMIT_MESSAGE;</span><br><span class="line">                <span class="keyword">default</span>:</span><br><span class="line">                    <span class="keyword">return</span> LocalTransactionState.ROLLBACK_MESSAGE;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> LocalTransactionState.UNKNOW;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>运行生产者，查看控制台输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">正在执行本地事务。。。。</span><br><span class="line">正在执行消息回查，事务id：C0A800678F0818B4AAC26AEDDEB10000</span><br><span class="line">消息回查状态：0</span><br><span class="line">本地事务执行成功。。。。</span><br></pre></td></tr></table></figure></p><p>需要注意：消息回查会隔一段时间执行一次，如果执行本地事务的时间太短，则控制台不会输出事务回查日志。</p><hr><h1 id="广播消息"><a href="#广播消息" class="headerlink" title="广播消息"></a>广播消息</h1><h2 id="生产者-1"><a href="#生产者-1" class="headerlink" title="生产者"></a>生产者</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Producer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> MQClientException, UnsupportedEncodingException, RemotingException, InterruptedException, MQBrokerException </span>&#123;</span><br><span class="line">        <span class="comment">// 1、创建 DefaultMQProducer</span></span><br><span class="line">        DefaultMQProducer producer = <span class="keyword">new</span> DefaultMQProducer(<span class="string">"boardcast-producer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2、设置 name server</span></span><br><span class="line">        producer.setNamesrvAddr(<span class="string">"192.168.52.200:9876"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3、开启 producer</span></span><br><span class="line">        producer.start();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">1</span>; index &lt;= <span class="number">10</span>; index++) &#123;</span><br><span class="line">            Message message = <span class="keyword">new</span> Message(<span class="string">"BOARD_CAST_TOPIC"</span>, <span class="string">"TAG_A"</span>, <span class="string">"KEYS_"</span> + index, (<span class="string">"HELLO！"</span> + index).getBytes(RemotingHelper.DEFAULT_CHARSET));</span><br><span class="line">            SendResult result = producer.send(message);</span><br><span class="line">            System.out.println(result);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭 producer</span></span><br><span class="line">        producer.shutdown();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="消费者-1"><a href="#消费者-1" class="headerlink" title="消费者"></a>消费者</h2><p>消费者需要将消费模式修改为 广播消费：  consumer.setMessageModel(MessageModel.BROADCASTING);</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Consumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> MQClientException </span>&#123;</span><br><span class="line">        <span class="comment">// 1、创建 DefaultMQPushConsumer</span></span><br><span class="line">        DefaultMQPushConsumer consumer = <span class="keyword">new</span> DefaultMQPushConsumer(<span class="string">"boardcast-consumer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2、设置 name server</span></span><br><span class="line">        consumer.setNamesrvAddr(<span class="string">"192.168.52.200:9876"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置消息拉取最大数</span></span><br><span class="line">        consumer.setConsumeMessageBatchMaxSize(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改消费模式，默认是集群消费模式，修改为广播消费模式</span></span><br><span class="line">        consumer.setMessageModel(MessageModel.BROADCASTING);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3、设置 subscribe</span></span><br><span class="line">        consumer.subscribe(<span class="string">"BOARD_CAST_TOPIC"</span>, <span class="comment">// 要消费的主题</span></span><br><span class="line">                <span class="string">"*"</span> <span class="comment">// 过滤规则</span></span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4、创建消息监听</span></span><br><span class="line">        consumer.registerMessageListener(<span class="keyword">new</span> MessageListenerConcurrently() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> ConsumeConcurrentlyStatus <span class="title">consumeMessage</span><span class="params">(List&lt;MessageExt&gt; list, ConsumeConcurrentlyContext consumeConcurrentlyContext)</span> </span>&#123;</span><br><span class="line">                <span class="comment">// 5、获取消息信息</span></span><br><span class="line">                <span class="keyword">for</span> (MessageExt msg : list) &#123;</span><br><span class="line">                    <span class="comment">// 获取主题</span></span><br><span class="line">                    String topic = msg.getTopic();</span><br><span class="line">                    <span class="comment">// 获取标签</span></span><br><span class="line">                    String tags = msg.getTags();</span><br><span class="line">                    <span class="comment">// 获取信息</span></span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        String result = <span class="keyword">new</span> String(msg.getBody(), RemotingHelper.DEFAULT_CHARSET);</span><br><span class="line">                        System.out.println(<span class="string">"A  Consumer 消费信息：topic："</span> + topic+ <span class="string">"，tags："</span> + tags + <span class="string">"，消息体："</span> + result);</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                        <span class="keyword">return</span> ConsumeConcurrentlyStatus.RECONSUME_LATER;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 6、返回消息读取状态</span></span><br><span class="line">                <span class="keyword">return</span> ConsumeConcurrentlyStatus.CONSUME_SUCCESS;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        consumer.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><h3 id="生产者控制台输出"><a href="#生产者控制台输出" class="headerlink" title="生产者控制台输出"></a>生产者控制台输出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">SendResult [sendStatus=SEND_OK, msgId=C0A80067971418B4AAC26B2965570000, offsetMsgId=C0A834C800002A9F00000000000026D0, messageQueue=MessageQueue [topic=BOARD_CAST_TOPIC, brokerName=broker-a, queueId=1], queueOffset=0]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=C0A80067971418B4AAC26B2965660001, offsetMsgId=C0A834C800002A9F000000000000278F, messageQueue=MessageQueue [topic=BOARD_CAST_TOPIC, brokerName=broker-a, queueId=2], queueOffset=10]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=C0A80067971418B4AAC26B29656C0002, offsetMsgId=C0A834C800002A9F000000000000284E, messageQueue=MessageQueue [topic=BOARD_CAST_TOPIC, brokerName=broker-a, queueId=3], queueOffset=0]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=C0A80067971418B4AAC26B2965700003, offsetMsgId=C0A834C800002A9F000000000000290D, messageQueue=MessageQueue [topic=BOARD_CAST_TOPIC, brokerName=broker-a, queueId=0], queueOffset=0]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=C0A80067971418B4AAC26B29657B0004, offsetMsgId=C0A834C800002A9F00000000000029CC, messageQueue=MessageQueue [topic=BOARD_CAST_TOPIC, brokerName=broker-a, queueId=1], queueOffset=1]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=C0A80067971418B4AAC26B2965880005, offsetMsgId=C0A834C800002A9F0000000000002A8B, messageQueue=MessageQueue [topic=BOARD_CAST_TOPIC, brokerName=broker-a, queueId=2], queueOffset=11]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=C0A80067971418B4AAC26B29658E0006, offsetMsgId=C0A834C800002A9F0000000000002B4A, messageQueue=MessageQueue [topic=BOARD_CAST_TOPIC, brokerName=broker-a, queueId=3], queueOffset=1]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=C0A80067971418B4AAC26B2965960007, offsetMsgId=C0A834C800002A9F0000000000002C09, messageQueue=MessageQueue [topic=BOARD_CAST_TOPIC, brokerName=broker-a, queueId=0], queueOffset=1]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=C0A80067971418B4AAC26B29659D0008, offsetMsgId=C0A834C800002A9F0000000000002CC8, messageQueue=MessageQueue [topic=BOARD_CAST_TOPIC, brokerName=broker-a, queueId=1], queueOffset=2]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=C0A80067971418B4AAC26B2965AB0009, offsetMsgId=C0A834C800002A9F0000000000002D87, messageQueue=MessageQueue [topic=BOARD_CAST_TOPIC, brokerName=broker-a, queueId=2], queueOffset=12]</span><br><span class="line">19:24:35.135 [NettyClientSelector_1] INFO RocketmqRemoting - closeChannel: close the connection to remote address[192.168.52.200:10911] result: true</span><br><span class="line">19:24:35.140 [NettyClientSelector_1] INFO RocketmqRemoting - closeChannel: close the connection to remote address[192.168.52.200:9876] result: true</span><br><span class="line">19:24:35.140 [NettyClientSelector_1] INFO RocketmqRemoting - closeChannel: close the connection to remote address[192.168.52.200:10909] result: true</span><br></pre></td></tr></table></figure><h3 id="消费者控制台输出"><a href="#消费者控制台输出" class="headerlink" title="消费者控制台输出"></a>消费者控制台输出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">A  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！1</span><br><span class="line">A  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！2</span><br><span class="line">A  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！5</span><br><span class="line">A  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！4</span><br><span class="line">A  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！3</span><br><span class="line">A  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！7</span><br><span class="line">A  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！6</span><br><span class="line">A  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！8</span><br><span class="line">A  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！9</span><br><span class="line">A  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！10</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">B  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！1</span><br><span class="line">B  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！2</span><br><span class="line">B  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！3</span><br><span class="line">B  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！5</span><br><span class="line">B  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！4</span><br><span class="line">B  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！6</span><br><span class="line">B  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！7</span><br><span class="line">B  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！8</span><br><span class="line">B  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！9</span><br><span class="line">B  Consumer 消费信息：topic：BOARD_CAST_TOPIC，tags：TAG_A，消息体：HELLO！10</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RocketMQ 顺序消息：消息有序是指可以按照消息发送顺序来消费。RocketMQ 可以严格的保证消息有序，但是这个顺序逼格不是全局顺序，只是分区(queue)顺序。要保证群居顺序，只能有一个分区。&lt;/p&gt;
    
    </summary>
    
      <category term="rocketmq" scheme="https://www.laiyy.top/categories/rocketmq/"/>
    
    
      <category term="MQ" scheme="https://www.laiyy.top/tags/MQ/"/>
    
      <category term="RocketMQ" scheme="https://www.laiyy.top/tags/RocketMQ/"/>
    
  </entry>
  
</feed>
