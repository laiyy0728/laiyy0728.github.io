<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Laiyy 的个人小站</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.laiyy.top/"/>
  <updated>2019-12-31T02:53:36.000Z</updated>
  <id>https://www.laiyy.top/</id>
  
  <author>
    <name>Laiyy</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hive(八) &lt;BR/&gt; DML</title>
    <link href="https://www.laiyy.top/hive/dml/hive-8.html"/>
    <id>https://www.laiyy.top/hive/dml/hive-8.html</id>
    <published>2019-12-31T02:53:36.000Z</published>
    <updated>2019-12-31T02:53:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>DDL 是数据定义语言，DML 是数据操作语言。</p><a id="more"></a><h1 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h1><h2 id="load-方式"><a href="#load-方式" class="headerlink" title="load 方式"></a>load 方式</h2><p>语法： <code>load data [local] inpath &#39;&#39; [overwrite] into table table_name [partition(partition_name=partition_value)]</code></p><blockquote><p>load data：表示加载数据<br>local：表示从本地加载，如果没有则是从 hdfs 加载<br>inpath：表示加载数据的路径<br>overwrite：表示覆盖表中已有数据，没有则是追加<br>into table：追加数据<br>table_name：具体的表名<br>partition：分区信息</p></blockquote><h2 id="insert-方式"><a href="#insert-方式" class="headerlink" title="insert 方式"></a>insert 方式</h2><blockquote><p>基本插入</p></blockquote><p>语法：<code>insert into table_name values (value1, value2..)</code></p><blockquote><p>查询插入</p></blockquote><p>语法：<code>insert into table_name select field from_table where params</code></p><blockquote><p>覆盖插入</p></blockquote><p>语法：<code>insert overwrite table_name</code></p><blockquote><p>多模式查询（根据多张表查询结果）</p></blockquote><p>语法：<code>from table_name insert overwrite|into table table_name [partition] select fields [where params]</code></p><p>其中：<code>insert overwrite|into table_name [partition] select fields [where params]</code> 可以多次书写。</p><p>注意点：<code>from table_name</code> 在最前面，后面的 select 是不需要带 <code>from table_name</code> 的。</p><h2 id="创建并插入"><a href="#创建并插入" class="headerlink" title="创建并插入"></a>创建并插入</h2><blockquote><p>方式一：从其他表中查询数据并创建</p></blockquote><p>语法：<code>create table if not exists table_name as select fields from table_name</code></p><blockquote><p>方式二：通过 location 指定从哪加载数据并创建</p></blockquote><p>语法：<code>create table if not exists table_name(column column_type) location &#39;/data/path&#39;</code></p><h2 id="import-方式"><a href="#import-方式" class="headerlink" title="import 方式"></a>import 方式</h2><p><strong><em>注意：此操作必须先用 export 导出后，再将数据导入</em></strong></p><p>语法：<code>import table table_name [partition(partition_name=partition_value)] from &#39;/file/path&#39;</code></p><blockquote><p>导入到一个已存在的表中（空表无结构）</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; import table test from &apos;/export&apos;;</span><br><span class="line">Copying data from hdfs://hadoop02:9000/export/data</span><br><span class="line">Copying file: hdfs://hadoop02:9000/export/data/dept.txt</span><br><span class="line">Loading data to table default.test</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.436 seconds</span><br></pre></td></tr></table></figure><blockquote><p>导入到一个已存在的表中（空表有结构）</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; import table people from &apos;/export&apos;;</span><br><span class="line">FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Column Schema does not match</span><br></pre></td></tr></table></figure><p>此时会报 schema 不匹配，原因：import 导入的是 export 的数据，此数据是带有元数据的。</p><blockquote><p>导入到一个不存在的表中</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; import table people1 from &apos;/export&apos;;</span><br><span class="line">Copying data from hdfs://hadoop02:9000/export/data</span><br><span class="line">Copying file: hdfs://hadoop02:9000/export/data/dept.txt</span><br><span class="line">Loading data to table default.people1</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.32 seconds</span><br></pre></td></tr></table></figure><hr><h1 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h1><h2 id="insert-导出"><a href="#insert-导出" class="headerlink" title="insert 导出"></a>insert 导出</h2><blockquote><p>将查询结果导出的本地</p></blockquote><p>语法：<code>insert overwrite local directory &#39;/local/file/path&#39; select fields from table_name</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &apos;/opt/module/hive/tmp_data/dept&apos; select * from dept;</span><br><span class="line">Query ID = root_20191231145413_67fa5115-2833-40fb-91bd-911b288dea6f</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks is set to 0 since there&apos;s no reduce operator</span><br><span class="line">Starting Job = job_1577759600542_0001, Tracking URL = http://hadoop03:8088/proxy/application_1577759600542_0001/</span><br><span class="line">Kill Command = /opt/module/hadoop-2.7.2/bin/hadoop job  -kill job_1577759600542_0001</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span><br><span class="line">2019-12-31 14:54:51,880 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2019-12-31 14:55:19,855 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 9.6 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 9 seconds 600 msec</span><br><span class="line">Ended Job = job_1577759600542_0001</span><br><span class="line">Copying data to local directory /opt/module/hive/tmp_data/dept</span><br><span class="line">Copying data to local directory /opt/module/hive/tmp_data/dept</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1   Cumulative CPU: 9.6 sec   HDFS Read: 2961 HDFS Write: 69 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 9 seconds 600 msec</span><br><span class="line">OK</span><br><span class="line">dept.deptnodept.dnamedept.loc</span><br><span class="line">Time taken: 68.597 seconds</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 tmp_data]# cd dept/</span><br><span class="line">[root@hadoop02 dept]# ll</span><br><span class="line">总用量 4</span><br><span class="line">-rw-r--r-- 1 root root 69 12月 31 14:55 000000_0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 dept]# cat 000000_0 </span><br><span class="line">10ACCOUNTING1700</span><br><span class="line">20RESEARCH1800</span><br><span class="line">30SALES1900</span><br><span class="line">40OPERATIONS1700</span><br></pre></td></tr></table></figure><p>可以看到此时导出的数据是没有格式的</p><blockquote><p>将查询结果格式化后导出到本地</p></blockquote><p>语法：<code>insert overwrite local directory &#39;/local/file/path&#39; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; select * from table_name</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &apos;/opt/module/hive/tmp_data/dept&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos; select * from dept;</span><br><span class="line">Query ID = root_20191231150022_adc3b27c-0666-40f8-9577-9689dec1e4ba</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks is set to 0 since there&apos;s no reduce operator</span><br><span class="line">Starting Job = job_1577759600542_0002, Tracking URL = http://hadoop03:8088/proxy/application_1577759600542_0002/</span><br><span class="line">Kill Command = /opt/module/hadoop-2.7.2/bin/hadoop job  -kill job_1577759600542_0002</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span><br><span class="line">2019-12-31 15:00:28,451 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2019-12-31 15:00:46,034 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.3 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 3 seconds 300 msec</span><br><span class="line">Ended Job = job_1577759600542_0002</span><br><span class="line">Copying data to local directory /opt/module/hive/tmp_data/dept</span><br><span class="line">Copying data to local directory /opt/module/hive/tmp_data/dept</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1   Cumulative CPU: 3.3 sec   HDFS Read: 3144 HDFS Write: 69 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 3 seconds 300 msec</span><br><span class="line">OK</span><br><span class="line">dept.deptnodept.dnam</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 tmp_data]# cat dept/000000_0 </span><br><span class="line">10ACCOUNTING1700</span><br><span class="line">20RESEARCH1800</span><br><span class="line">30SALES1900</span><br><span class="line">40OPERATIONS1700</span><br></pre></td></tr></table></figure><blockquote><p>将查询结果打出到 HDFS</p></blockquote><p>语法：<code>insert overwrite directory &#39;/hdfs/file/path&#39; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\t&#39; select * from table_name</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory &apos;/dept&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos; select * from dept;</span><br><span class="line">Query ID = root_20191231150613_00f90db5-63a6-4f64-be64-615787dfa988</span><br><span class="line">Total jobs = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is set to 0 since there&apos;s no reduce operator</span><br><span class="line">Starting Job = job_1577759600542_0003, Tracking URL = http://hadoop03:8088/proxy/application_1577759600542_0003/</span><br><span class="line">Kill Command = /opt/module/hadoop-2.7.2/bin/hadoop job  -kill job_1577759600542_0003</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span><br><span class="line">2019-12-31 15:06:23,590 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2019-12-31 15:06:27,736 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.21 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 1 seconds 210 msec</span><br><span class="line">Ended Job = job_1577759600542_0003</span><br><span class="line">Stage-3 is selected by condition resolver.</span><br><span class="line">Stage-2 is filtered out by condition resolver.</span><br><span class="line">Stage-4 is filtered out by condition resolver.</span><br><span class="line">Moving data to: hdfs://hadoop02:9000/dept/.hive-staging_hive_2019-12-31_15-06-13_809_519050561514978017-1/-ext-10000</span><br><span class="line">Moving data to: /dept</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1   Cumulative CPU: 1.21 sec   HDFS Read: 3080 HDFS Write: 69 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 1 seconds 210 msec</span><br><span class="line">OK</span><br><span class="line">dept.deptnodept.dnamedept.loc</span><br><span class="line">Time taken: 15.096 seconds</span><br></pre></td></tr></table></figure><p><img src="/images/hive/db/insert-to-hdfs.png" alt="insert 方式导出"></p><h2 id="其他方式导出"><a href="#其他方式导出" class="headerlink" title="其他方式导出"></a>其他方式导出</h2><blockquote><p>hadoop 命令导出</p></blockquote><p>语法：<code>hadoop fs -get /hdfs/file/path /hdfs|local/file/path</code> </p><blockquote><p>Hive shell 导出</p></blockquote><p>语法： <code>bin/hive -e &#39;select * from table_name;&#39; &gt; /local/file/path</code></p><blockquote><p>export 导出到 hdfs</p></blockquote><p>语法：<code>hive (default)&gt; export table table_name to /hdfs/file/path</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; export table dept to &apos;/export&apos;;</span><br><span class="line">Copying data from file:/tmp/root/de98917c-077b-4ec5-a8f5-479a75cc3c12/hive_2019-12-31_15-14-28_860_4397941394477204195-1/-local-10000/_metadata</span><br><span class="line">Copying file: file:/tmp/root/de98917c-077b-4ec5-a8f5-479a75cc3c12/hive_2019-12-31_15-14-28_860_4397941394477204195-1/-local-10000/_metadata</span><br><span class="line">Copying data from hdfs://hadoop02:9000/user/hive/warehouse/dept</span><br><span class="line">Copying file: hdfs://hadoop02:9000/user/hive/warehouse/dept/dept.txt</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.376 seconds</span><br></pre></td></tr></table></figure><p><img src="/images/hive/db/export.png" alt="export"></p><hr><h1 id="清空表"><a href="#清空表" class="headerlink" title="清空表"></a>清空表</h1><p>语法： <code>truncate table table_name</code></p><p><strong><em>此操作只能删除管理表，不能删除外部表中的数据</em></strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DDL 是数据定义语言，DML 是数据操作语言。&lt;/p&gt;
    
    </summary>
    
      <category term="hive" scheme="https://www.laiyy.top/categories/hive/"/>
    
      <category term="dml" scheme="https://www.laiyy.top/categories/hive/dml/"/>
    
    
      <category term="hive" scheme="https://www.laiyy.top/tags/hive/"/>
    
      <category term="dml" scheme="https://www.laiyy.top/tags/dml/"/>
    
  </entry>
  
  <entry>
    <title>Hive(七) &lt;BR/&gt; DDL 3 分区表、修改表</title>
    <link href="https://www.laiyy.top/hive/ddl/hive-7.html"/>
    <id>https://www.laiyy.top/hive/ddl/hive-7.html</id>
    <published>2019-12-30T09:33:55.000Z</published>
    <updated>2019-12-30T09:33:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>除了管理表、内部表外，常用的表还有分区表。分区表实际上是一个 HDFS 文件夹，文件夹下存放的是对应分区的数据</p><a id="more"></a><h1 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h1><p>分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区的所有数据。<br>Hive 中的分区就是分目录，把一个大的数据集根据业务需要，分隔成小的数据集。<br>在查询的时候，通过 WHERE 字句中的表达式选择查询所需要的指定分区，这样的查询效率会提高很多。</p><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><blockquote><p>创建一个表，指定分区</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition(</span><br><span class="line">    deptno <span class="built_in">int</span>, dname <span class="keyword">string</span>, loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><blockquote><p>加载数据查看问题</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &apos;/opt/module/hive/tmp_data/dept.txt&apos; into table dept_partition;</span><br><span class="line">FAILED: SemanticException [Error 10062]: Need to specify partition columns because the destination table is partitioned</span><br></pre></td></tr></table></figure><p>可以看到，数据加载失败了。原因是我们创建表的时候，设置了根据 <code>month</code> 分区，而在加载数据的时候没有指定分区列造成的。</p><blockquote><p>按照分区加载数据</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &apos;/opt/module/hive/tmp_data/dept.txt&apos; into table dept_partition partition(month=&apos;2019-07&apos;);</span><br><span class="line">Loading data to table default.dept_partition partition (month=2019-07)</span><br><span class="line">Partition default.dept_partition&#123;month=2019-07&#125; stats: [numFiles=1, numRows=0, totalSize=71, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.464 seconds</span><br></pre></td></tr></table></figure><p><img src="/images/hive/db/partition.png" alt="分区加载数据"></p><blockquote><p>再次加载两个分区</p></blockquote><p><img src="/images/hive/db/partition-2.png" alt="分区加载数据"></p><blockquote><p>查询数据</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition;</span><br><span class="line">OK</span><br><span class="line">dept_partition.deptnodept_partition.dnamedept_partition.locdept_partition.month</span><br><span class="line">10ACCOUNTING17002019-07</span><br><span class="line">20RESEARCH18002019-07</span><br><span class="line">30SALES19002019-07</span><br><span class="line">40OPERATIONS17002019-07</span><br><span class="line">10ACCOUNTING17002019-08</span><br><span class="line">20RESEARCH18002019-08</span><br><span class="line">30SALES19002019-08</span><br><span class="line">40OPERATIONS17002019-08</span><br><span class="line">10ACCOUNTING17002019-09</span><br><span class="line">20RESEARCH18002019-09</span><br><span class="line">30SALES19002019-09</span><br><span class="line">40OPERATIONS17002019-09</span><br><span class="line">Time taken: 0.226 seconds, Fetched: 12 row(s)</span><br></pre></td></tr></table></figure><p>可以看到，三个分区的数据都能查到，且把分区参数当做了一列 <code>dept_partition.month</code></p><blockquote><p>利用 where 字句查询某分区的数据</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&apos;2019-08&apos;;</span><br><span class="line">OK</span><br><span class="line">dept_partition.deptnodept_partition.dnamedept_partition.locdept_partition.month</span><br><span class="line">10ACCOUNTING17002019-08</span><br><span class="line">20RESEARCH18002019-08</span><br><span class="line">30SALES19002019-08</span><br><span class="line">40OPERATIONS17002019-08</span><br><span class="line">Time taken: 1.148 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure><blockquote><p>查看元数据</p></blockquote><p><img src="/images/hive/db/partition-3.png" alt="分区查询"><br><img src="/images/hive/db/partition-4.png" alt="分区查询"></p><h2 id="增加分区"><a href="#增加分区" class="headerlink" title="增加分区"></a>增加分区</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(month='2019-10');</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.202 seconds</span><br></pre></td></tr></table></figure><blockquote><p>同时添加多个分区</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'2019-11'</span>) <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'2019-12'</span>);</span><br></pre></td></tr></table></figure><h2 id="删除分区"><a href="#删除分区" class="headerlink" title="删除分区"></a>删除分区</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition(month='2019-12');</span><br><span class="line">Dropped the partition month=2019-12</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.376 seconds</span><br></pre></td></tr></table></figure><blockquote><p>同时删除多个分区</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition(month='2019-11'), partition(month='2019-10');</span><br><span class="line">Dropped the partition month=2019-10</span><br><span class="line">Dropped the partition month=2019-11</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.371 seconds</span><br></pre></td></tr></table></figure><p><strong><em>注意：添加多个分区时，分区与分区之间用 空格 隔开；删除多个分区时，分区与分区之间用 英文逗号 隔开！！</em></strong></p><h2 id="查看分区表有多少分区"><a href="#查看分区表有多少分区" class="headerlink" title="查看分区表有多少分区"></a>查看分区表有多少分区</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show partitions dept_partition;</span><br><span class="line">OK</span><br><span class="line">partition</span><br><span class="line">month=2019-07</span><br><span class="line">month=2019-08</span><br><span class="line">month=2019-09</span><br><span class="line">Time taken: 0.055 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure><h2 id="查看分区结构"><a href="#查看分区结构" class="headerlink" title="查看分区结构"></a>查看分区结构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted dept_partition;</span><br><span class="line">OK</span><br><span class="line">col_namedata_typecomment</span><br><span class="line"># col_name            data_type           comment             </span><br><span class="line">  </span><br><span class="line">deptno              int                                     </span><br><span class="line">dname               string                                  </span><br><span class="line">loc                 string                                  </span><br><span class="line">  </span><br><span class="line"># Partition Information  </span><br><span class="line"># col_name            data_type           comment             </span><br><span class="line">  </span><br><span class="line">month               string       # 此处即是分区的结构                           </span><br><span class="line">  </span><br><span class="line"># Detailed Table Information  </span><br><span class="line">Database:           default              </span><br><span class="line"># 省略           </span><br><span class="line">Time taken: 0.062 seconds, Fetched: 34 row(s)</span><br></pre></td></tr></table></figure><h2 id="分区表的注意事项"><a href="#分区表的注意事项" class="headerlink" title="分区表的注意事项"></a>分区表的注意事项</h2><h3 id="创建二级分区表"><a href="#创建二级分区表" class="headerlink" title="创建二级分区表"></a>创建二级分区表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition2(</span><br><span class="line">    deptno <span class="built_in">int</span>, dname <span class="keyword">string</span>, loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p>与一级分区的区别是 <code>partitioned by</code> 中定义两个字段。</p><h3 id="加载数据到二级分区"><a href="#加载数据到二级分区" class="headerlink" title="加载数据到二级分区"></a>加载数据到二级分区</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &apos;/opt/module/hive/tmp_data/dept.txt&apos; into table dept_partition2 partition(month=&apos;2019-10&apos;, day=&apos;30&apos;);</span><br><span class="line">Loading data to table default.dept_partition2 partition (month=2019-10, day=30)</span><br><span class="line">Partition default.dept_partition2&#123;month=2019-10, day=30&#125; stats: [numFiles=1, numRows=0, totalSize=71, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.358 seconds</span><br></pre></td></tr></table></figure><h3 id="查看分区数据"><a href="#查看分区数据" class="headerlink" title="查看分区数据"></a>查看分区数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition2 where month=&apos;2019-10&apos; and day=&apos;30&apos;;</span><br><span class="line">OK</span><br><span class="line">dept_partition2.deptnodept_partition2.dnamedept_partition2.locdept_partition2.monthdept_partition2.day</span><br><span class="line">10ACCOUNTING17002019-1030</span><br><span class="line">20RESEARCH18002019-1030</span><br><span class="line">30SALES19002019-1030</span><br><span class="line">40OPERATIONS17002019-1030</span><br><span class="line">Time taken: 0.104 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure><p><img src="/images/hive/db/high-partition.png" alt="多级分区"></p><h2 id="分区表与数据产生关联的三种方式"><a href="#分区表与数据产生关联的三种方式" class="headerlink" title="分区表与数据产生关联的三种方式"></a>分区表与数据产生关联的三种方式</h2><h3 id="上传数据后修复"><a href="#上传数据后修复" class="headerlink" title="上传数据后修复"></a>上传数据后修复</h3><p>适用场景：在 HDFS 上存在分区目录，且目录中存在文件，但是元数据中没有对应的分区信息。</p><p>以 <code>dept_partition</code> 为例，此时这个分区表只有三个分区</p><p><img src="/images/hive/db/partition-5.png" alt="分区"></p><ol><li>创建一个 <code>2019-11</code> 文件夹，上传 dept.txt 数据。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 tmp_data]# hadoop fs -mkdir -p /user/hive/warehouse/dept_partition/month=2019-10</span><br><span class="line">[root@hadoop02 tmp_data]# hadoop fs -put dept.txt /user/hive/warehouse/dept_partition/month=2019-10</span><br></pre></td></tr></table></figure><ol start="2"><li>查询 <code>month=2019-10</code> 分区的数据。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&apos;2019-10&apos;;</span><br><span class="line">OK</span><br><span class="line">dept_partition.deptnodept_partition.dnamedept_partition.locdept_partition.month</span><br><span class="line">Time taken: 0.134 seconds</span><br></pre></td></tr></table></figure><ol start="3"><li>此时，执行分区修复</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; msck repair table dept_partition;</span><br><span class="line">OK</span><br><span class="line">Partitions not in metastore:dept_partition:month=2019-10</span><br><span class="line">Repair: Added partition to metastore dept_partition:month=2019-10</span><br><span class="line">Time taken: 0.201 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><ol start="4"><li>再次查询分区<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where month=&apos;2019-10&apos;;</span><br><span class="line">OK</span><br><span class="line">dept_partition.deptnodept_partition.dnamedept_partition.locdept_partition.month</span><br><span class="line">10ACCOUNTING17002019-10</span><br><span class="line">20RESEARCH18002019-10</span><br><span class="line">30SALES19002019-10</span><br><span class="line">40OPERATIONS17002019-10</span><br><span class="line">Time taken: 0.088 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure></li></ol><h3 id="修改表分区"><a href="#修改表分区" class="headerlink" title="修改表分区"></a>修改表分区</h3><p>只需要将上述第三步修改为： <code>alter table dept_partition where month=&#39;2019-10&#39;</code></p><h3 id="load-数据"><a href="#load-数据" class="headerlink" title="load 数据"></a>load 数据</h3><p>将第一种方式的前三步合并为一步： <code>load data local inpath &#39;/file/path&#39; into table table_name partition(partition_name=partition_value)</code></p><hr><h1 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h1><h2 id="表重命名"><a href="#表重命名" class="headerlink" title="表重命名"></a>表重命名</h2><p>语法：<code>alter table table_name rename to new_table_name;</code></p><p><strong><em>此操作会修改元数据和 HDFS 的文件夹名称</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table test rename to rename_table;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.185 seconds</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">tab_name</span><br><span class="line">dept</span><br><span class="line">dept_partition</span><br><span class="line">dept_partition2</span><br><span class="line">emp</span><br><span class="line">people</span><br><span class="line">rename_table</span><br><span class="line">Time taken: 0.016 seconds, Fetched: 6 row(s)</span><br></pre></td></tr></table></figure><h2 id="增加-修改-替换列信息"><a href="#增加-修改-替换列信息" class="headerlink" title="增加/修改/替换列信息"></a>增加/修改/替换列信息</h2><h3 id="修改列"><a href="#修改列" class="headerlink" title="修改列"></a>修改列</h3><p>语法：<code>alter table table_name CHANGE [column] col_old_name col_new_name column_type [COMMENT col_coment] [FIRST | AFTER column_name]</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table test CHANGE column name sex int;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.082 seconds</span><br><span class="line"></span><br><span class="line">hive (default)&gt; desc test;</span><br><span class="line">OK</span><br><span class="line">col_namedata_typecomment</span><br><span class="line">id                  int                                     </span><br><span class="line">sex                 int                                  </span><br><span class="line">Time taken: 0.054 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><h3 id="增加、替换列信息"><a href="#增加、替换列信息" class="headerlink" title="增加、替换列信息"></a>增加、替换列信息</h3><p>语法：<code>alter table table_name ADD | REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table test add columns (name string);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.132 seconds</span><br><span class="line">hive (default)&gt; select * from test;</span><br><span class="line">OK</span><br><span class="line">test.idtest.name</span><br><span class="line">1NULL</span><br><span class="line">2NULL</span><br><span class="line">3NULL</span><br><span class="line">4NULL</span><br><span class="line">5NULL</span><br><span class="line">Time taken: 0.288 seconds, Fetched: 5 row(s)</span><br></pre></td></tr></table></figure><p><strong><em>注意：ADD 表示增加一个字段，字段位置在所有列的后面（partition列的前面）</em></strong><br><strong><em>REPLACE 是替换表中的 所有字段！不能只替换一个字段</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table test replace columns (name string);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.083 seconds</span><br><span class="line">hive (default)&gt; desc test;</span><br><span class="line">OK</span><br><span class="line">col_namedata_typecomment</span><br><span class="line">name                string                                  </span><br><span class="line">Time taken: 0.051 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><h2 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop table test;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.206 seconds</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;除了管理表、内部表外，常用的表还有分区表。分区表实际上是一个 HDFS 文件夹，文件夹下存放的是对应分区的数据&lt;/p&gt;
    
    </summary>
    
      <category term="hive" scheme="https://www.laiyy.top/categories/hive/"/>
    
      <category term="ddl" scheme="https://www.laiyy.top/categories/hive/ddl/"/>
    
    
      <category term="hive" scheme="https://www.laiyy.top/tags/hive/"/>
    
      <category term="ddl" scheme="https://www.laiyy.top/tags/ddl/"/>
    
  </entry>
  
  <entry>
    <title>Hive(六) &lt;BR/&gt; DDL 2</title>
    <link href="https://www.laiyy.top/hive/ddl/hive-6.html"/>
    <id>https://www.laiyy.top/hive/ddl/hive-6.html</id>
    <published>2019-12-30T06:28:26.000Z</published>
    <updated>2019-12-30T06:28:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>创建表的时候，分为 <code>管理表</code> 和 <code>外部表</code>。管理表也称为 <code>内部表</code>。</p><a id="more"></a><h1 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h1><p>默认创建的表都是管理表（内部表）。Hive 会控制表中数据的生命周期，Hive 默认情况下会将这些表的数据存储在由配置项 <code>hive.metastore.warehouse.dir</code> 所定义的目录的子目录下。<br>当我们删除一个管理表时，Hive 也会删除这个表中的数据。<br>管理表不适合和其他工具共享数据。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="创建普通表"><a href="#创建普通表" class="headerlink" title="创建普通表"></a>创建普通表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> students(</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">'/user/hive/warehouse/student2'</span></span><br></pre></td></tr></table></figure><h3 id="根据查询结果创建表（查询的结果会添加的新创建的表中）"><a href="#根据查询结果创建表（查询的结果会添加的新创建的表中）" class="headerlink" title="根据查询结果创建表（查询的结果会添加的新创建的表中）"></a>根据查询结果创建表（查询的结果会添加的新创建的表中）</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> people2 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">name</span>, friends, children, address <span class="keyword">from</span> people;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create table if not exists people2 as select name, friends, children, address from people;</span><br><span class="line">Query ID = root_20191230143721_e7ecd58e-3605-4e64-a4d5-daa161afdc82</span><br><span class="line">Total jobs = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is set to 0 since there&apos;s no reduce operator</span><br><span class="line">Starting Job = job_1577671926689_0001, Tracking URL = http://hadoop03:8088/proxy/application_1577671926689_0001/</span><br><span class="line">Kill Command = /opt/module/hadoop-2.7.2/bin/hadoop job  -kill job_1577671926689_0001</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span><br><span class="line">2019-12-30 14:37:32,362 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2019-12-30 14:37:42,730 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.55 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 1 seconds 550 msec</span><br><span class="line">Ended Job = job_1577671926689_0001</span><br><span class="line">Stage-4 is selected by condition resolver.</span><br><span class="line">Stage-3 is filtered out by condition resolver.</span><br><span class="line">Stage-5 is filtered out by condition resolver.</span><br><span class="line">Moving data to: hdfs://hadoop02:9000/user/hive/warehouse/.hive-staging_hive_2019-12-30_14-37-21_922_8266176305010522642-1/-ext-10001</span><br><span class="line">Moving data to: hdfs://hadoop02:9000/user/hive/warehouse/people2</span><br><span class="line">Table default.people2 stats: [numFiles=1, numRows=2, totalSize=116, rawDataSize=114]</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1   Cumulative CPU: 1.55 sec   HDFS Read: 3867 HDFS Write: 188 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 1 seconds 550 msec</span><br><span class="line">OK</span><br><span class="line">namefriendschildrenaddress</span><br><span class="line">Time taken: 22.204 seconds</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from people2;</span><br><span class="line">OK</span><br><span class="line">people2.namepeople2.friendspeople2.childrenpeople2.address</span><br><span class="line">iyy[&quot;liyl&quot;,&quot;nixy&quot;]&#123;&quot;xiao lai&quot;:18,&quot;xiao yang&quot;:19&#125;&#123;&quot;street&quot;:&quot;zheng zhou&quot;,&quot;city&quot;:&quot;henan&quot;&#125;</span><br><span class="line">lierg[&quot;cuihua&quot;,&quot;goudan&quot;]&#123;&quot;zhang mazi&quot;:17,&quot;zhao si&quot;:20&#125;&#123;&quot;street&quot;:&quot;kai feng&quot;,&quot;city&quot;:&quot;henan&quot;&#125;</span><br><span class="line">Time taken: 0.052 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><hr><h1 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h1><p>Hive 并不完全拥有这份数据。当删除表的时候，实际数据不会删除，但描述这个表的元数据会被删除。</p><h2 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h2><h3 id="创建部门、员工两张表"><a href="#创建部门、员工两张表" class="headerlink" title="创建部门、员工两张表"></a>创建部门、员工两张表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> default.dept(</span><br><span class="line">deptno <span class="built_in">int</span>,</span><br><span class="line">dname <span class="keyword">string</span>,</span><br><span class="line">loc <span class="built_in">int</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> default.emp(</span><br><span class="line">empno <span class="built_in">int</span>,</span><br><span class="line">ename <span class="keyword">string</span>,</span><br><span class="line">job <span class="keyword">string</span>,</span><br><span class="line">mgr <span class="built_in">int</span>,</span><br><span class="line">hiredate <span class="keyword">string</span>,</span><br><span class="line">sal <span class="keyword">double</span>,</span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><p>向两张表中导入 <a href="/file/hive/dept.txt">部门数据</a> 和 <a href="/file/hive/emp.txt">员工数据</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &apos;/opt/module/hive/tmp_data/dept.txt&apos; into table dept;</span><br><span class="line">Loading data to table default.dept</span><br><span class="line">Table default.dept stats: [numFiles=1, totalSize=71]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.231 seconds</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &apos;/opt/module/hive/tmp_data/emp.txt&apos; into table emp;</span><br><span class="line">Loading data to table default.emp</span><br><span class="line">Table default.emp stats: [numFiles=1, totalSize=669]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.198 seconds</span><br></pre></td></tr></table></figure><h3 id="删除-dept"><a href="#删除-dept" class="headerlink" title="删除 dept"></a>删除 dept</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop table dept;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.42 seconds</span><br></pre></td></tr></table></figure><p><img src="/images/hive/db/drop-dept.png" alt="删除表"></p><p>可以看到，删除表后，数据还在。</p><h3 id="再次创建-dept-表"><a href="#再次创建-dept-表" class="headerlink" title="再次创建 dept 表"></a>再次创建 dept 表</h3><p>再次使用上面的建表语句，把 dept 表建出来，但是 <strong><em>不导入任何数据</em></strong></p><p>使用查询语句查询 dept</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; create external table if not exists default.dept(</span><br><span class="line">              &gt; deptno int,</span><br><span class="line">              &gt; dname string,</span><br><span class="line">              &gt; loc int</span><br><span class="line">              &gt; )</span><br><span class="line">              &gt; row format delimited fields terminated by &apos;\t&apos;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.043 seconds</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from dept;</span><br><span class="line">OK</span><br><span class="line">dept.deptnodept.dnamedept.loc</span><br><span class="line">10ACCOUNTING1700</span><br><span class="line">20RESEARCH1800</span><br><span class="line">30SALES1900</span><br><span class="line">40OPERATIONS1700</span><br><span class="line">Time taken: 0.052 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure><p>可以看到，如果数据存在，那么创建表只要能指定到这个 HDFS 文件夹，那么就能查出来数据。</p><hr><h1 id="管理表与外部表的互相转换"><a href="#管理表与外部表的互相转换" class="headerlink" title="管理表与外部表的互相转换"></a>管理表与外部表的互相转换</h1><h2 id="查询表的类型"><a href="#查询表的类型" class="headerlink" title="查询表的类型"></a>查询表的类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted test;</span><br><span class="line">OK</span><br><span class="line">col_namedata_typecomment</span><br><span class="line"># col_name            data_type           comment             </span><br><span class="line">  </span><br><span class="line">id                  int                                     </span><br><span class="line">  </span><br><span class="line"># Detailed Table Information  </span><br><span class="line"># 省略                  </span><br><span class="line">Location:           hdfs://hadoop02:9000/user/hive/warehouse/test </span><br><span class="line">Table Type:         MANAGED_TABLE     # 此处即为表类型，可以看到是管理表   </span><br><span class="line"># 省略      </span><br><span class="line">  </span><br><span class="line"># Storage Information  </span><br><span class="line">SerDe Library:      org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe </span><br><span class="line"># 省略</span><br></pre></td></tr></table></figure><h2 id="修改为外部表"><a href="#修改为外部表" class="headerlink" title="修改为外部表"></a>修改为外部表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table test set tblproperties(&apos;EXTERNAL&apos;=&apos;TRUE&apos;);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.129 seconds</span><br></pre></td></tr></table></figure><h2 id="再次查看表类型"><a href="#再次查看表类型" class="headerlink" title="再次查看表类型"></a>再次查看表类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc formatted test;</span><br><span class="line">OK</span><br><span class="line">col_namedata_typecomment</span><br><span class="line"># col_name            data_type           comment             </span><br><span class="line">  </span><br><span class="line">id                  int                                     </span><br><span class="line">  </span><br><span class="line"># Detailed Table Information  </span><br><span class="line">Database:           default              </span><br><span class="line"># 省略                  </span><br><span class="line">Location:           hdfs://hadoop02:9000/user/hive/warehouse/test </span><br><span class="line">Table Type:         EXTERNAL_TABLE       # 可以看到表类型已经改为了外部表 </span><br><span class="line"># 省略         </span><br><span class="line">  </span><br><span class="line"># Storage Information  </span><br><span class="line">SerDe Library:      org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe </span><br><span class="line"># 省略</span><br></pre></td></tr></table></figure><h2 id="再次改为管理表"><a href="#再次改为管理表" class="headerlink" title="再次改为管理表"></a>再次改为管理表</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter table test set tblproperties(&apos;EXTERNAL&apos;=&apos;FALSE&apos;);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.129 seconds</span><br></pre></td></tr></table></figure><h2 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h2><blockquote><p>EXTERNAL 必须为大写<br>设置是否是外部表只能通过 EXTERNAL 为 TRUE 或 FALSE 来控制<br><code>&#39;EXTERNAL&#39;=&#39;FALSE&#39;</code>、<code>&#39;EXTERNAL&#39;=&#39;TRUE&#39;</code>，固定写法，区分大小写</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;创建表的时候，分为 &lt;code&gt;管理表&lt;/code&gt; 和 &lt;code&gt;外部表&lt;/code&gt;。管理表也称为 &lt;code&gt;内部表&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="hive" scheme="https://www.laiyy.top/categories/hive/"/>
    
      <category term="ddl" scheme="https://www.laiyy.top/categories/hive/ddl/"/>
    
    
      <category term="hive" scheme="https://www.laiyy.top/tags/hive/"/>
    
      <category term="ddl" scheme="https://www.laiyy.top/tags/ddl/"/>
    
  </entry>
  
  <entry>
    <title>Hive(五) &lt;BR/&gt; DDL</title>
    <link href="https://www.laiyy.top/hive/ddl/hive-5.html"/>
    <id>https://www.laiyy.top/hive/ddl/hive-5.html</id>
    <published>2019-12-30T03:33:02.000Z</published>
    <updated>2019-12-30T03:33:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>DDL 语句：数据库模型定义语言，用于定义数据库、表的结构。<br>DDL 语句不仅可以操作数据库，也可以操作数据表。</p><a id="more"></a><h1 id="DDL-库操作"><a href="#DDL-库操作" class="headerlink" title="DDL 库操作"></a>DDL 库操作</h1><h2 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h2><p>注意一定要加上 <code>if not exists</code>，避免报错。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> db_hive ;</span><br></pre></td></tr></table></figure><p><img src="/images/hive/db/create_db.png" alt="创建数据库"></p><h2 id="创建数据库，并指定-HDFS-的存储路径"><a href="#创建数据库，并指定-HDFS-的存储路径" class="headerlink" title="创建数据库，并指定 HDFS 的存储路径"></a>创建数据库，并指定 HDFS 的存储路径</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> db_hive2 location <span class="string">'/db_hive2.db'</span>;</span><br></pre></td></tr></table></figure><p><img src="/images/hive/db/create_db_2.png" alt="创建数据库并指定存储路径"></p><h2 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">database_name</span><br><span class="line">db_hive</span><br><span class="line">db_hive2</span><br><span class="line">default</span><br><span class="line">Time taken: 0.013 seconds, Fetched: 3 row(s)</span><br></pre></td></tr></table></figure><blockquote><p>模糊查询库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show databases like &apos;db_hive*&apos;;</span><br><span class="line">OK</span><br><span class="line">database_name</span><br><span class="line">db_hive</span><br><span class="line">db_hive2</span><br><span class="line">Time taken: 0.01 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure></p></blockquote><h2 id="查看数据库详情"><a href="#查看数据库详情" class="headerlink" title="查看数据库详情"></a>查看数据库详情</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc database db_hive;</span><br><span class="line">OK</span><br><span class="line">db_namecommentlocationowner_nameowner_typeparameters</span><br><span class="line">db_hivehdfs://hadoop02:9000/user/hive/warehouse/db_hive.dbrootUSER</span><br><span class="line">Time taken: 0.016 seconds, Fetched: 1 row(s</span><br></pre></td></tr></table></figure><h2 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h2><p>可以使用 ALTER DATABASE 没了为某个数据库设置键值对属性(dbpeoperties)，来描述这个数据库的属性信息。<br><strong><em>数据库的其他元数据信息都是不可更改的，包括数据吗名和数据库所在目录位置。</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; alter database db_hive set dbproperties(&apos;createtime&apos;=&apos;20191230&apos;);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.019 seconds</span><br></pre></td></tr></table></figure><blockquote><p>extended 可以查看附加值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; desc database extended db_hive;</span><br><span class="line">OK</span><br><span class="line">db_namecommentlocationowner_nameowner_typeparameters</span><br><span class="line">db_hivehdfs://hadoop02:9000/user/hive/warehouse/db_hive.dbrootUSER&#123;createtime=20191230&#125;</span><br><span class="line">Time taken: 0.013 seconds, Fetched: 1 row(s</span><br></pre></td></tr></table></figure></p></blockquote><h2 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop database is not exists db_hive;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.075 seconds</span><br></pre></td></tr></table></figure><h2 id="强制删除数据库"><a href="#强制删除数据库" class="headerlink" title="强制删除数据库"></a>强制删除数据库</h2><p>如果数据库中有数据，可以使用 <code>CASCADE</code> 命令强制删除。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; drop database is not exists db_hive2 cascade;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.033 seconds</span><br></pre></td></tr></table></figure><h1 id="DDL-表操作"><a href="#DDL-表操作" class="headerlink" title="DDL 表操作"></a>DDL 表操作</h1><h2 id="建表语法"><a href="#建表语法" class="headerlink" title="建表语法"></a>建表语法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name</span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">[COMMENT table_comment]</span><br><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">[CLUSTERED BY (col_name, col_name, ...)</span><br><span class="line">[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</span><br><span class="line">[ROW FORMAT row_format]</span><br><span class="line">[STORED AS file_format]</span><br><span class="line">[LOCATION hdfs_path]</span><br></pre></td></tr></table></figure><h3 id="建表语法解释"><a href="#建表语法解释" class="headerlink" title="建表语法解释"></a>建表语法解释</h3><blockquote><p>CREATE TABLE</p></blockquote><p>创建一个指定名字的表，如果表名称已存在，则会出现异常；可以通过 <code>IF NOT EXISTS</code> 来规避。</p><blockquote><p>EXTERNAL</p></blockquote><p>可以让用户创建一个 <code>外部表</code>，在建表的同时指定一个指向实际数据的路径。<br><strong><em>Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何更改。</em></strong><br><strong><em>在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据。</em></strong></p><blockquote><p>COMMENT</p></blockquote><p>为表和列添加注释</p><blockquote><p>PARTITIONED BY</p></blockquote><p>创建分区表</p><blockquote><p>CLUSTERED BY</p></blockquote><p>创建分桶表，按照列进行分区</p><blockquote><p>SORTED BY</p></blockquote><p>排序规则，不常用，按照列进行分桶</p><blockquote><p>ROW FORMAT</p></blockquote><p>行格式化规则<br>包括：<br><code>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</code></p><p><code>SERDE serde_name [WITH SERDEPROPERTIES](peoperty_name=property_value, ...)</code></p><p>在建表时可以自定义 serDe（Serialize、Deserialize 的简称，目的是用于序列化、反序列化），或者使用自带的 serDe。<br>如果没有指定 ROW FORMAT 或 ROW FORMAT DELIMITED，将会使用自带的 serDe。<br>在建表时，还需要为表指定列，在指定表的列的同事也会指定自定义的 serDe，Hive 通过 serDe 确定表的具体的列的数据</p><blockquote><p>STORED AS</p></blockquote><p>指定存储文件类型。常用类型为： SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）。</p><p>如果是纯文本数据，可以使用 <code>STORED AS TEXTFILE</code>。如果数据需要压缩，可以使用 <code>STORED AS SEQUENCEFILE</code></p><blockquote><p>LOCATION</p></blockquote><p>指定表再 HDFS 上的存储位置</p><blockquote><p>LIKE</p></blockquote><p>允许用户复制现有的表结构，但是不复制数据</p><h3 id="查看表信息"><a href="#查看表信息" class="headerlink" title="查看表信息"></a>查看表信息</h3><p>以之前的 <code>peopel</code> 表为例。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; show create table people;</span><br><span class="line">OK</span><br><span class="line">createtab_stmt</span><br><span class="line">CREATE TABLE `people`(</span><br><span class="line">  `name` string, </span><br><span class="line">  `friends` array&lt;string&gt;, </span><br><span class="line">  `children` map&lt;string,int&gt;, </span><br><span class="line">  `address` struct&lt;street:string,city:string&gt;)</span><br><span class="line">ROW FORMAT DELIMITED </span><br><span class="line">  FIELDS TERMINATED BY &apos;,&apos; </span><br><span class="line">  COLLECTION ITEMS TERMINATED BY &apos;_&apos; </span><br><span class="line">  MAP KEYS TERMINATED BY &apos;:&apos; </span><br><span class="line">  LINES TERMINATED BY &apos;\n&apos; </span><br><span class="line">STORED AS INPUTFORMAT </span><br><span class="line">  &apos;org.apache.hadoop.mapred.TextInputFormat&apos; </span><br><span class="line">OUTPUTFORMAT </span><br><span class="line">  &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;</span><br><span class="line">LOCATION</span><br><span class="line">  &apos;hdfs://hadoop02:9000/user/hive/warehouse/people&apos;</span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">  &apos;COLUMN_STATS_ACCURATE&apos;=&apos;true&apos;, </span><br><span class="line">  &apos;numFiles&apos;=&apos;1&apos;, </span><br><span class="line">  &apos;totalSize&apos;=&apos;116&apos;, </span><br><span class="line">  &apos;transient_lastDdlTime&apos;=&apos;1577674200&apos;)</span><br><span class="line">Time taken: 0.132 seconds, Fetched: 21 row(s)</span><br></pre></td></tr></table></figure><p>可以看到，除了建表时手动写的一些语句外，还增加了 <code>STORED AS</code>、<code>LOCATIONM</code>、<code>TBLPROPERTIES</code> 等。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DDL 语句：数据库模型定义语言，用于定义数据库、表的结构。&lt;br&gt;DDL 语句不仅可以操作数据库，也可以操作数据表。&lt;/p&gt;
    
    </summary>
    
      <category term="hive" scheme="https://www.laiyy.top/categories/hive/"/>
    
      <category term="ddl" scheme="https://www.laiyy.top/categories/hive/ddl/"/>
    
    
      <category term="hive" scheme="https://www.laiyy.top/tags/hive/"/>
    
      <category term="ddl" scheme="https://www.laiyy.top/tags/ddl/"/>
    
  </entry>
  
  <entry>
    <title>Hive(四) &lt;BR/&gt; Hive 数据类型</title>
    <link href="https://www.laiyy.top/hive/hive-4.html"/>
    <id>https://www.laiyy.top/hive/hive-4.html</id>
    <published>2019-12-30T01:45:25.000Z</published>
    <updated>2019-12-30T01:45:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 中也有和 Java 类似的基本数据类型、复杂数据类型，也可以进行类型转换。</p><a id="more"></a><hr><h1 id="Hive-的数据类型"><a href="#Hive-的数据类型" class="headerlink" title="Hive 的数据类型"></a>Hive 的数据类型</h1><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><table><thead><tr><th style="text-align:center">Hive 数据类型</th><th style="text-align:center">Java 数据类型</th><th style="text-align:center">长度</th><th style="text-align:center">例子</th></tr></thead><tbody><tr><td style="text-align:center">TINYINT</td><td style="text-align:center">byte</td><td style="text-align:center">1 byte 有符号整数</td><td style="text-align:center">20</td></tr><tr><td style="text-align:center">SMALLINT</td><td style="text-align:center">short</td><td style="text-align:center">2 byte 有符号整数</td><td style="text-align:center">20</td></tr><tr><td style="text-align:center"><strong><em>INT</em></strong></td><td style="text-align:center">int</td><td style="text-align:center">4 byte 有符号整数</td><td style="text-align:center">20</td></tr><tr><td style="text-align:center"><strong><em>BIGINT</em></strong></td><td style="text-align:center">long</td><td style="text-align:center">8 byte 有符号整数</td><td style="text-align:center">20</td></tr><tr><td style="text-align:center">BOOLEAN</td><td style="text-align:center">boolean</td><td style="text-align:center">布尔类型</td><td style="text-align:center">TRUE、FALSE</td></tr><tr><td style="text-align:center">FLOAT</td><td style="text-align:center">float</td><td style="text-align:center">单精度浮点数</td><td style="text-align:center">3.14</td></tr><tr><td style="text-align:center"><strong><em>DOUBLE</em></strong></td><td style="text-align:center">double</td><td style="text-align:center">双精度浮点数</td><td style="text-align:center">3.14</td></tr><tr><td style="text-align:center"><strong><em>STRING</em></strong></td><td style="text-align:center">string</td><td style="text-align:center">字符类型，可指定字符集，可使用单引号或双引号</td><td style="text-align:center"><code>&#39;test&#39;</code>、<code>&quot;test&quot;</code></td></tr><tr><td style="text-align:center">TIMESTAMP</td><td style="text-align:center"></td><td style="text-align:center">时间类型</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">BINARY</td><td style="text-align:center"></td><td style="text-align:center">字节数组</td></tr></tbody></table><p>Hive 是 String 类型，相当于数据库的 varchar 类型。该类型是一个可变字符串，理论上可以存储 2GB 的字符数。</p><h2 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h2><table><thead><tr><th style="text-align:center">数据类型</th><th style="text-align:center">描述</th><th style="text-align:center">语法</th></tr></thead><tbody><tr><td style="text-align:center">STRUCT</td><td style="text-align:center">结构体，类似于Java Bean，可以通过 a.b 访问</td><td style="text-align:center">struct()</td></tr><tr><td style="text-align:center">MAP</td><td style="text-align:center">键值对，使用数组标识符可以访问数据</td><td style="text-align:center">map()</td></tr><tr><td style="text-align:center">ARRAY</td><td style="text-align:center">一组具有相同类型和名称的变量的集合</td><td style="text-align:center">Array()</td></tr></tbody></table><p>ARRAY 和 Map 与 Java 中的 Array、Map 类型，STRUCT 与 c 语言中的 struct 类型，封装了一个命名字段集合，复杂数据类型允许任意层次嵌套。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p><strong><em>注意：Hive 每次只能解析一行数据，如果是有结构，且美化过的 JSON，Hive 解析不了。</em></strong></p><p>如下数据，对应 Hive 而言是没有格式的，解析不了。<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"zhangsan"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>使用 <a href="/file/hive/people.txt">测试数据</a>，测试 Hive 的复杂数据类型</p><p>测试数据解析：<br><strong><em>laiyy,liyl_nixy,xiao lai:18_xiao yang:19,zheng zhou_henan</em></strong><br>对应为：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"laiyy"</span>,</span><br><span class="line">    <span class="attr">"friends"</span>: [<span class="string">"liyl"</span>, <span class="string">"nixy"</span>],</span><br><span class="line">    <span class="attr">"children"</span>: &#123;</span><br><span class="line">        <span class="attr">"xiao lai"</span>:<span class="number">18</span>,</span><br><span class="line">        <span class="attr">"xiao yang"</span>:<span class="number">19</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"address"</span>: &#123;</span><br><span class="line">        <span class="attr">"street"</span>: <span class="string">"zheng zhou"</span>,</span><br><span class="line">        <span class="attr">"city"</span>: <span class="string">"henan"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><blockquote><p>在 Hive 上创建一个 people 表</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> people(</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">    friends <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,  <span class="comment">-- 创建一个 friends 字段，类型为 array，存储 string </span></span><br><span class="line">    children <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="built_in">int</span>&gt;, <span class="comment">-- 创建一个 children 字段，类型为 map，key 为 string 类型， value 为 int 类型</span></span><br><span class="line">    address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>, city:<span class="keyword">string</span>&gt; <span class="comment">-- 创建一个 address 字段，类型为 struct，其中 street 字段为 string 类型，city 字段为 string 类型</span></span><br><span class="line">) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> <span class="comment">-- 列分隔符</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'_'</span> <span class="comment">-- 数据分隔符，分隔 MAP、STRUCT、ARRAY</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span> <span class="comment">-- MAP 中的 KV 分隔符</span></span><br><span class="line"><span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span> <span class="comment">-- 以 \n 分隔每一行</span></span><br></pre></td></tr></table></figure><p>HQL 解释为：以 <code>\n</code> 分隔每一行，读取一行数据，以 <code>,</code> 分隔每个字段的值；如果值中有 <code>_</code>，则为数组类型；如果值中有 <code>:</code>，则为 map，<code>:</code> 分隔 map 的 KV</p><blockquote><p>加载数据并查询测试</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &apos;/opt/module/hive/tmp_data/people.txt&apos; into table people;</span><br><span class="line">Loading data to table default.people</span><br><span class="line">Table default.people stats: [numFiles=1, totalSize=116]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.899 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from people;</span><br><span class="line">OK</span><br><span class="line">people.namepeople.friendspeople.childrenpeople.address</span><br><span class="line">laiyy[&quot;liyl&quot;,&quot;nixy&quot;]&#123;&quot;xiao lai&quot;:18,&quot;xiao yang&quot;:19&#125;&#123;&quot;street&quot;:&quot;zheng zhou&quot;,&quot;city&quot;:&quot;henan&quot;&#125;</span><br><span class="line">lierg[&quot;cuihua&quot;,&quot;goudan&quot;]&#123;&quot;zhang mazi&quot;:17,&quot;zhao si&quot;:20&#125;&#123;&quot;street&quot;:&quot;kai feng&quot;,&quot;city&quot;:&quot;henan&quot;&#125;</span><br><span class="line">Time taken: 0.293 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><p>由此可见，复杂结构体的测试也通过了。</p><blockquote><p>测试单字段查询</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select friends from people;</span><br><span class="line">OK</span><br><span class="line">friends</span><br><span class="line">[&quot;liyl&quot;,&quot;nixy&quot;]</span><br><span class="line">[&quot;cuihua&quot;,&quot;goudan&quot;]</span><br><span class="line">Time taken: 0.078 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><blockquote><p>查询数组中的某个下标的数据</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select friends[1] from people;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">nixy</span><br><span class="line">goudan</span><br><span class="line">Time taken: 0.07 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><blockquote><p>查询 map 中的某个 key</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select children[&apos;xiao lai&apos;] from people;</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">18</span><br><span class="line">NULL</span><br><span class="line">Time taken: 0.073 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><blockquote><p>查询结构体的某个字段</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select address.city from people;</span><br><span class="line">OK</span><br><span class="line">city</span><br><span class="line">henan</span><br><span class="line">henan</span><br><span class="line">Time taken: 0.054 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><h2 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h2><p>Hive 的原子数据类型是可以进行 <code>隐式转换</code> 的，类似于 Java 的类型转换。如某表达式用 INT 类型，TINYINT 会自动转换为 INT 联系，但是 Hive 不会反向转化（某表达式用 TINYINT，INT 不会自动转为 TINYINT，会返回错误，除非使用强制类型转换 CAST）</p><h3 id="隐式类型转换规则"><a href="#隐式类型转换规则" class="headerlink" title="隐式类型转换规则"></a>隐式类型转换规则</h3><blockquote><p>任何整数类型都可以隐式转换为一个范围更广的类型<br>所有整数类型、FLOAT、STRING(数值) 都行都可以隐式转换为 DOUBLE<br>TINYINT、SMALLINT、INT 都可以转换为 FLOAT<br>BOOLEAN 不可以转换为其他任何类型</p></blockquote><h3 id="CAST-强制转换"><a href="#CAST-强制转换" class="headerlink" title="CAST 强制转换"></a>CAST 强制转换</h3><p>如 CAST(‘1’ AS INT)，可以将字符串 1 转换为整数 1；如果强制类型转换失败，如 CAST(‘X’ AS INT)，表达式会返回 NULL。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select CAST(&apos;1&apos; AS INT);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">1</span><br><span class="line">Time taken: 0.086 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select CAST(&apos;abc&apos; AS INT);</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line">NULL</span><br><span class="line">Time taken: 0.072 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 中也有和 Java 类似的基本数据类型、复杂数据类型，也可以进行类型转换。&lt;/p&gt;
    
    </summary>
    
      <category term="hive" scheme="https://www.laiyy.top/categories/hive/"/>
    
    
      <category term="hive" scheme="https://www.laiyy.top/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive(三) &lt;BR/&gt; JDBC 访问、常用命令</title>
    <link href="https://www.laiyy.top/hive/hive-3.html"/>
    <id>https://www.laiyy.top/hive/hive-3.html</id>
    <published>2019-12-26T08:30:50.000Z</published>
    <updated>2019-12-26T08:30:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 的元数据可以存储在 MySQL 中，解决了 derby 只能单客户端连接的问题。Hive 也可以开启类似 JDBC 的连接查询方式。</p><a id="more"></a><h1 id="HiveJDBC-访问"><a href="#HiveJDBC-访问" class="headerlink" title="HiveJDBC 访问"></a>HiveJDBC 访问</h1><p>hiveserver2：可以提供一个类似于 JDBC 的连接方式，连接成功后，可以在程序中使用 SQL 进行操作（不是 HQL）。</p><blockquote><p>修改 hive-site.xml</p></blockquote><p>在 hive-site.xml 中，增加配置 hiveserver2 连接的 用户名、密码。<br><strong><em>注意：此用户必须拥有 HDFS 中 /tmp/hive 目录的读写权限</em></strong><br><strong><em>此用户是系统用户，并不是随意指定的</em></strong><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.client.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.client.password<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>创建一个 <code>test.txt</code>，将 test.txt 存储在 test 表中（<code>create table test(id int)</code>）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath &apos;/opt/module/hive/tmp_data/test.txt&apos; into table test;</span><br><span class="line">Loading data to table default.test</span><br><span class="line">Table default.test stats: [numFiles=1, totalSize=11]</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.355 seconds</span><br><span class="line"></span><br><span class="line">hive&gt; select * from test;</span><br><span class="line">OK</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">NULL</span><br><span class="line">Time taken: 0.233 seconds, Fetched: 6 row(s)</span><br></pre></td></tr></table></figure><blockquote><p>启动 hiveserver2：<code># bin/hiveserver2</code>，是个阻塞进程<br>启动 beeline<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hive]# bin/beeline </span><br><span class="line">Beeline version 1.2.1 by Apache Hive</span><br><span class="line">beeline&gt;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>连接 hiveserver2</p></blockquote><p><strong><em>如果没有配置 hiveserver2 的用户名、密码，则在进行下面的连接时，会提示连接被拒绝</em></strong></p><p><strong><em>如果在 hadoop 的 <code>vore-site.xml</code> 文件中没有如下配置，也会提示连接被拒绝</em></strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hadoop 集群的 core-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>成功连接</p></blockquote><p>语法： <code>!connect jdbc:hive2://host:port</code><br>host 为 hive 所在服务器，port 为 10000.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">beeline&gt; !connect jdbc:hive2://hadoop02:10000</span><br><span class="line">Connecting to jdbc:hive2://hadoop02:10000</span><br><span class="line">Enter username for jdbc:hive2://hadoop02:10000: root</span><br><span class="line">Enter password for jdbc:hive2://hadoop02:10000: ******</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br></pre></td></tr></table></figure><blockquote><p>测试</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop02:10000&gt; show tables;</span><br><span class="line">+-----------+--+</span><br><span class="line">| tab_name  |</span><br><span class="line">+-----------+--+</span><br><span class="line">| test      |</span><br><span class="line">+-----------+--+</span><br><span class="line">1 row selected (0.096 seconds)</span><br><span class="line"></span><br><span class="line">0: jdbc:hive2://hadoop02:10000&gt; select * from test;</span><br><span class="line">+----------+--+</span><br><span class="line">| test.id  |</span><br><span class="line">+----------+--+</span><br><span class="line">| 1        |</span><br><span class="line">| 2        |</span><br><span class="line">| 3        |</span><br><span class="line">| 4        |</span><br><span class="line">| 5        |</span><br><span class="line">+----------+--+</span><br></pre></td></tr></table></figure><hr><h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hive]# bin/hive -help</span><br><span class="line">usage: hive</span><br><span class="line"> -d,--define &lt;key=value&gt;          Variable subsitution to apply to hive</span><br><span class="line">                                  commands. e.g. -d A=B or --define A=B</span><br><span class="line">    --database &lt;databasename&gt;     Specify the database to use</span><br><span class="line"> -e &lt;quoted-query-string&gt;         SQL from command line</span><br><span class="line"> -f &lt;filename&gt;                    SQL from files</span><br><span class="line"> -H,--help                        Print help information</span><br><span class="line">    --hiveconf &lt;property=value&gt;   Use value for given property</span><br><span class="line">    --hivevar &lt;key=value&gt;         Variable subsitution to apply to hive</span><br><span class="line">                                  commands. e.g. --hivevar A=B</span><br><span class="line"> -i &lt;filename&gt;                    Initialization SQL file</span><br><span class="line"> -S,--silent                      Silent mode in interactive shell</span><br><span class="line"> -v,--verbose                     Verbose mode (echo executed SQL to the</span><br><span class="line">                                  console)</span><br></pre></td></tr></table></figure><h2 id="常用命令-1"><a href="#常用命令-1" class="headerlink" title="常用命令"></a>常用命令</h2><h3 id="命令行执行-hql"><a href="#命令行执行-hql" class="headerlink" title="命令行执行 hql"></a>命令行执行 hql</h3><p>执行完后自动退出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hive]# bin/hive -e &quot;select * from test;&quot;</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties</span><br><span class="line">OK</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">Time taken: 1.208 seconds, Fetched:5 row(s)</span><br><span class="line">[root@hadoop02 hive]#</span><br></pre></td></tr></table></figure><h3 id="执行文件内的-hql"><a href="#执行文件内的-hql" class="headerlink" title="执行文件内的 hql"></a>执行文件内的 hql</h3><p>创建一个文件： hql.txt，文件内容为<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">test</span>;</span><br></pre></td></tr></table></figure></p><blockquote><p>执行</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hive]# bin/hive -f tmp_data/hql.txt </span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties</span><br><span class="line">OK</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">Time taken: 1.155 seconds, Fetched: 5 row(s)</span><br></pre></td></tr></table></figure><h2 id="hive-命令"><a href="#hive-命令" class="headerlink" title="hive 命令"></a>hive 命令</h2><h3 id="查看-hdfs"><a href="#查看-hdfs" class="headerlink" title="查看 hdfs"></a>查看 hdfs</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; dfs -ls /;</span><br><span class="line">Found 2 items</span><br><span class="line">drwx-wx-wx   - root supergroup          0 2019-12-23 15:17 /tmp</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-12-23 15:18 /user</span><br></pre></td></tr></table></figure><h3 id="查看本地文件系统"><a href="#查看本地文件系统" class="headerlink" title="查看本地文件系统"></a>查看本地文件系统</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; ! ls /opt/module;</span><br><span class="line">hadoop-2.7.2</span><br><span class="line">hive</span><br><span class="line">jdk1.8.0_144</span><br><span class="line">zookeeper-3.4.10</span><br></pre></td></tr></table></figure><h3 id="查看历史命令"><a href="#查看历史命令" class="headerlink" title="查看历史命令"></a>查看历史命令</h3><p>进入当前用户的根目录，查看 <code>.hivehistory</code> 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 ~]# cat .hivehistory </span><br><span class="line">show databaes;</span><br><span class="line">show databases;</span><br><span class="line">use default;</span><br><span class="line">create table stu(id int, name string) row format delimited fields terminated by &apos;\t&apos;;</span><br><span class="line">quit;</span><br><span class="line">show tables;</span><br><span class="line">create table test(id int);</span><br><span class="line">show tables;</span><br><span class="line">load data local inpath &apos;/opt/module/hive/tmp_data/test.txt&apos; into table test;</span><br><span class="line">select * from test;</span><br><span class="line">quit;</span><br></pre></td></tr></table></figure><hr><h1 id="常用配置"><a href="#常用配置" class="headerlink" title="常用配置"></a>常用配置</h1><h2 id="Hive-数据仓库位置"><a href="#Hive-数据仓库位置" class="headerlink" title="Hive 数据仓库位置"></a>Hive 数据仓库位置</h2><blockquote><p>默认数据仓库的最原始位置在 HDFS 上，路径为： <code>/user/hive/warehouse/</code><br>在仓库目录下，没有对默认的数据库 default 创建文件夹。如果某张表属于 default 库，则会直接在数据仓库目录下创建一个文件夹<br>修改 default 数据仓库的原始位置（hive-site.xml）</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>默认仓库存储路径<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="查询后信息显示的配置"><a href="#查询后信息显示的配置" class="headerlink" title="查询后信息显示的配置"></a>查询后信息显示的配置</h2><blockquote><p>在 hive-site 中增加配置，显示当前数据库，以及查询表的头信息配置</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>是否打印当前数据库<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>是否打印头信息<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>测试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select * from test;</span><br><span class="line">OK</span><br><span class="line">test.id</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td></tr></table></figure></p><p>可以看到库名：hive(default)。头信息：test.id</p><h2 id="hive-运行日志"><a href="#hive-运行日志" class="headerlink" title="hive 运行日志"></a>hive 运行日志</h2><blockquote><p>Hive 默认的日志存放在 /tmp/[当前用户]/hive.log 下</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hive]# ll /tmp/root/</span><br><span class="line">总用量 3708</span><br><span class="line">-rw-r--r--  1 root root 2292655 12月 27 16:01 hive.log</span><br><span class="line">-rw-r--r--. 1 root root   25667 12月 23 15:18 hive.log.2019-12-23</span><br><span class="line">-rw-r--r--  1 root root 1470990 12月 26 15:27 hive.log.2019-12-26</span><br></pre></td></tr></table></figure><p>将 <code>hive-log4j.properties.template</code> 复制一份为 <code>hive-log4j.properties</code>，修改 log 存储位置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#hive.log.dir=$&#123;java.io.tmpdir&#125;/$&#123;user.name&#125;</span><br><span class="line">hive.log.dir=/opt/logs/hive</span><br></pre></td></tr></table></figure><p>修改后重启 hive 即可。</p><h2 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h2><blockquote><p>查看 mapreduce task 数量</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br><span class="line">mapred.reduce.tasks=-1</span><br></pre></td></tr></table></figure><blockquote><p>设置 mapreduce task 数量</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks=10;</span><br><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br><span class="line">mapred.reduce.tasks=10</span><br></pre></td></tr></table></figure><blockquote><p>另外一种设置方式</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hive]# bin/hive -hiveconf mapred.reduce.tasks=10;</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in file:/opt/module/hive/conf/hive-log4j.properties</span><br><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br><span class="line">mapred.reduce.tasks=10</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 的元数据可以存储在 MySQL 中，解决了 derby 只能单客户端连接的问题。Hive 也可以开启类似 JDBC 的连接查询方式。&lt;/p&gt;
    
    </summary>
    
      <category term="hive" scheme="https://www.laiyy.top/categories/hive/"/>
    
    
      <category term="hive" scheme="https://www.laiyy.top/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive(二) &lt;BR/&gt; 本地文件导入 Hive、MySQL 存储元数据</title>
    <link href="https://www.laiyy.top/hive/hive-2.html"/>
    <id>https://www.laiyy.top/hive/hive-2.html</id>
    <published>2019-12-23T02:42:07.000Z</published>
    <updated>2019-12-23T02:42:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>除了使用命令创建表、插入数据外，也可以将本地文件数据导入 Hive。</p><a id="more"></a><h1 id="本地文件导入-Hive"><a href="#本地文件导入-Hive" class="headerlink" title="本地文件导入 Hive"></a>本地文件导入 Hive</h1><p>准备一个 <a href="/file/hive/student.txt">待导入文件</a>，将文件放在 <code>/opt/module/datas</code> 文件夹下。</p><p>需要注意语法： <code>load data local inpath [local path] into table [table name]</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath &apos;/opt/module/datas/student.txt&apos; into table student;</span><br><span class="line">Loading data to table default.student</span><br><span class="line">Table default.student stats: [numFiles=3, numRows=0, totalSize=66, rawDataSize=0]</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.397 seconds</span><br></pre></td></tr></table></figure><blockquote><p>查看 WebUI</p></blockquote><p><img src="/images/hive/install/load-data.png" alt="load data"></p><blockquote><p>查询</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br><span class="line">OK</span><br><span class="line">1laiyy</span><br><span class="line">2laiyy1</span><br><span class="line">NULLNULL</span><br><span class="line">NULLNULL</span><br><span class="line">NULLNULL</span><br><span class="line">NULLNULL</span><br><span class="line">Time taken: 0.282 seconds, Fetched: 6 row(s)</span><br></pre></td></tr></table></figure><p><strong><em>出现了四条 NULL 记录</em></strong></p><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>在建表的时候，就指定分隔符，且导入时，需要按照建表时指定的分隔符来分隔数据。</p><blockquote><p>新建一张表，以 \t 分隔</p></blockquote><p>注意语法：<code>create table [table name] fow format  delimited fields terminated by [terminated type]</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table stu(id int, name string) row format delimited fields terminated by &apos;\t&apos;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.12 seconds</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">stu</span><br><span class="line">student</span><br><span class="line">Time taken: 0.02 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure><blockquote><p>导入数据到 stu 表，查看表数据</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath &apos;/opt/module/datas/student.txt&apos; into table stu;</span><br><span class="line">Loading data to table default.stu</span><br><span class="line">Table default.stu stats: [numFiles=1, totalSize=49]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.148 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; select * from stu;</span><br><span class="line">OK</span><br><span class="line">1001zhangsan</span><br><span class="line">1002lisi</span><br><span class="line">1003wangwu</span><br><span class="line">1004zhaoliu</span><br><span class="line">Time taken: 0.041 seconds, Fetched: 4 row(s)</span><br></pre></td></tr></table></figure><p><img src="/images/hive/install/inser-data2.png" alt="insert data tu stu"></p><h2 id="将本地文件-put-进表"><a href="#将本地文件-put-进表" class="headerlink" title="将本地文件 put 进表"></a>将本地文件 put 进表</h2><p>再创建一个文件 <a href="/file/hive/stu.txt">stu.txt</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put stu.txt /user/hive/warehouse/stu</span><br></pre></td></tr></table></figure><p><img src="/images/hive/install/put-to-table.png" alt="put tu table"></p><blockquote><p>调用 select * 查看数据</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from stu;</span><br><span class="line">OK</span><br><span class="line">1005haha</span><br><span class="line">1006heihei</span><br><span class="line">NULLNULL</span><br><span class="line">1001zhangsan</span><br><span class="line">1002lisi</span><br><span class="line">1003wangwu</span><br><span class="line">1004zhaoliu</span><br><span class="line">Time taken: 0.052 seconds, Fetched: 7 row(s)</span><br></pre></td></tr></table></figure><h2 id="将-HDFS-上的数据-put-进表"><a href="#将-HDFS-上的数据-put-进表" class="headerlink" title="将 HDFS 上的数据 put 进表"></a>将 HDFS 上的数据 put 进表</h2><p>将 <code>student.txt</code> 复制为 <code>student2.txt</code> 文件，并上传至 HDFS 根目录下，然后将根目录下的数据 put 到 stu 表中</p><p>注意语法：<code>load data inpath [hdfs path] into table [table name]</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 datas]# hadoop fs -put student2.txt /</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; load data inpath &apos;/student2.txt&apos; into table stu;</span><br><span class="line">Loading data to table default.stu</span><br><span class="line">Table default.stu stats: [numFiles=3, totalSize=121]</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.132 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; select * from stu;</span><br><span class="line">OK</span><br><span class="line">1005haha</span><br><span class="line">1006heihei</span><br><span class="line">NULLNULL</span><br><span class="line">1001zhangsan</span><br><span class="line">1002lisi</span><br><span class="line">1003wangwu</span><br><span class="line">1004zhaoliu</span><br><span class="line">1001zhangsan</span><br><span class="line">1002lisi</span><br><span class="line">1003wangwu</span><br><span class="line">1004zhaoliu</span><br></pre></td></tr></table></figure><blockquote><p>此时再查看 WebUI，发现根目录下的 /student2.txt 文件消失了</p></blockquote><p><img src="/images/hive/install/no-student2.png" alt="没有 student2.txt"></p><p>其实，student2.txt 文件并没有删除，是 hive 修改了 student2.txt 文件的元数据，从原本的 <code>/ 目录</code> 修改为 <code>/user/hive/warehouse/stu 目录</code></p><hr><h1 id="元数据存储在-MySQL"><a href="#元数据存储在-MySQL" class="headerlink" title="元数据存储在 MySQL"></a>元数据存储在 MySQL</h1><p>在 hadoop02 上，连接 Hive 后，使用 XShell 再开一个窗口，还是 hadoop02 ，在另外一个窗口没有关闭的情况下，再连接 Hive，会报如下错误</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Caused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------</span><br><span class="line">java.sql.SQLException: Failed to start database &apos;metastore_db&apos; with class loader sun.misc.Launcher$AppClassLoader@214c265e, see the next exception for details.</span><br><span class="line">at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)</span><br><span class="line">at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)</span><br><span class="line">at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)</span><br><span class="line">at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)</span><br><span class="line">at org.apache.derby.impl.jdbc.EmbedConnection.&lt;init&gt;(Unknown Source)</span><br><span class="line">at org.apache.derby.impl.jdbc.EmbedConnection40.&lt;init&gt;(Unknown Source)</span><br><span class="line">at org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)</span><br><span class="line">at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)</span><br><span class="line">at org.apache.derby.jdbc.Driver20.connect(Unknown Source)</span><br></pre></td></tr></table></figure><blockquote><p>错误原因</p></blockquote><p>Hive 默认数据库是 <code>derby</code>，是单用户的，当有两个连接以上时就会报错。为了防止这个错误，可以把元数据信息从 derby 中改为存储在 mysql 中。</p><h2 id="使用-MySQL-存储元数据"><a href="#使用-MySQL-存储元数据" class="headerlink" title="使用 MySQL 存储元数据"></a>使用 MySQL 存储元数据</h2><p><strong><em>注意：本例假设 MySQL 已经成功安装，且 MySQL 已经配置了 root+密码，所有主机访问</em></strong></p><p>本例采用MySQL 57 版本，插件包为 <a href="/file/hive/mysql-connector-java-5.1.34.jar">5.1.34 版本</a></p><p>将插件包拷贝到 <code>%HIVE_HOME%/lib/</code> 下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 mysql-connector-java-5.1.27]# cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive/lib/</span><br></pre></td></tr></table></figure><blockquote><p>修改 Hive 配置信息</p></blockquote><p>在 <code>/opt/module/hive/conf/</code> 下创建一个 <code>hive-site.xml</code> 文件，根据 <a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+3.0+Administration#AdminManualMetastore3.0Administration-Option2:ExternalRDBMS" target="_blank" rel="noopener">官方文档</a>，在 <code>hive-site.xml</code> 中添加数据库配置</p><p><strong><em>注意：数据库要先创建好，表会在启动时自动创建；文件名称必须为 hive-site.xml，否则无效</em></strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop02:3306/hive_meta<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>启动 hive</p></blockquote><p>由于版本问题导致的 SSL 日志忽略即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hive]# bin/hive</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties</span><br><span class="line">Thu Dec 26 15:18:55 CST 2019 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line">Thu Dec 26 15:18:55 CST 2019 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line">Thu Dec 26 15:18:55 CST 2019 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line">Thu Dec 26 15:18:55 CST 2019 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line">Thu Dec 26 15:18:56 CST 2019 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line">Thu Dec 26 15:18:56 CST 2019 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line">Thu Dec 26 15:18:56 CST 2019 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line">Thu Dec 26 15:18:56 CST 2019 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p><img src="/images/hive/install/mysql.png" alt="启动成功后"></p><blockquote><p>执行一些测试</p></blockquote><p>查询不到表，是因为 mysql 只是存储 <code>元数据</code>，而现在还没有在 mysql 做元数据存储的基础上创建 hive 表，所以查询不到。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.016 second</span><br></pre></td></tr></table></figure><p>创建一个表，再查询</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table test(id int);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.208 seconds</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">test</span><br><span class="line">Time taken: 0.018 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><p><img src="/images/hive/install/mysql-metadata.png" alt="MySQL metadata"><br><img src="/images/hive/install/mysql-metadata-1.png" alt="MySQl metadata"><br><img src="/images/hive/install/mysql-metadata-2.png" alt="MySQl metadata"></p><!-- ## 安装 MySQL  --><!-- 使用 rpm 包安装 mysql，本例版本为 `5.6.24`，如已经安装了 MySQL，可跳过此步骤> 解压 mysql-libs，并安装 mysql rpm注意：需要先安装好 `net-tools`、`perl`、`perl-devel` 依赖<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y net-tools perl perl-devel</span><br></pre></td></tr></table></figure><blockquote><p>安装 mysql、启动 mysql</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#yum install wget</span><br><span class="line"></span><br><span class="line">#wget http://repo.mysql.com//mysql57-community-release-el7-7.noarch.rpm</span><br><span class="line"></span><br><span class="line">#yum install mysql57-community-release-el7-7.noarch.rpm</span><br><span class="line"></span><br><span class="line">#yum install mysql-community-server</span><br><span class="line"></span><br><span class="line"># systemctl start mysqld.service</span><br></pre></td></tr></table></figure><blockquote><p>修改 Mysql 密码</p></blockquote><p>启动后查看 <code>/var/log/mysqld.log</code> 文件，查看 mysql 密码，也可能查看不到，查看不到的情况下，修改 <code>/etc/my.cnf</code> 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line">datadir=/var/lib/mysql</span><br><span class="line">socket=/var/lib/mysql/mysql.sock</span><br><span class="line"></span><br><span class="line">symbolic-links=0</span><br><span class="line"></span><br><span class="line">log-error=/var/log/mysqld.log</span><br><span class="line">pid-file=/var/run/mysqld/mysqld.pid</span><br><span class="line"></span><br><span class="line"># mysql 登录跳过密码校验</span><br><span class="line">skip-grant-tables=1</span><br></pre></td></tr></table></figure><p>重启mysql，随便输入密码登录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># systemctl restart mysqld.service</span><br><span class="line"></span><br><span class="line">[root@hadoop02 etc]# mysql -uroot -p</span><br><span class="line">Enter password: </span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br></pre></td></tr></table></figure></p><blockquote><p>修改密码</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use mysql</span><br><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; update user set authentication_string = password(&apos;123456&apos;),password_expired = &apos;N&apos;, password_last_changed = now() where user = &apos;root&apos;;</span><br><span class="line">Query OK, 1 row affected, 1 warning (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; exit</span><br><span class="line">Bye</span><br></pre></td></tr></table></figure><blockquote><p>将 <code>my.cnf</code> 文件的 <code>skip-grant-tables=1</code> 注释掉，重启 mysql， 使用新的登录密码访问。<br>修改允许远程链接</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; update user set `host` = &apos;%&apos; where `user` = &apos;root&apos;;</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure><blockquote><p>重启mysql，使用 Navicat 连接测试</p></blockquote><p><img src="/images/hive/install/mysql-test.png" alt="测试 MySQL"> –&gt;</p>-->]]></content>
    
    <summary type="html">
    
      &lt;p&gt;除了使用命令创建表、插入数据外，也可以将本地文件数据导入 Hive。&lt;/p&gt;
    
    </summary>
    
      <category term="hive" scheme="https://www.laiyy.top/categories/hive/"/>
    
    
      <category term="hive" scheme="https://www.laiyy.top/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive(一) &lt;BR/&gt; 基础概念、安装、基础命令</title>
    <link href="https://www.laiyy.top/hive/hive-1.html"/>
    <id>https://www.laiyy.top/hive/hive-1.html</id>
    <published>2019-12-20T06:51:14.000Z</published>
    <updated>2019-12-20T06:51:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hive 是 Facebook 开源的用于解决 <code>海量结构化日志</code> 的数据统计。<br>Hive 是基于 Hadoop 的数据仓库地址，可以 <code>将结构化的数据文件映射为一张表</code>，并提供 <code>类似 SQL</code> 的查询功能。<br>Hive 的本质是将 HQL(Hive Query Language) 转化为 MapReduce。</p><a id="more"></a><h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><ol><li>数据仓库通过 SQL 进行统计分析</li><li>将 SQL 中常用的操作（select、where、group等）用 MapReduce 写成很多模板</li><li>将所有的 MapReduce 模板封装到 Hive 中</li><li>用户根据业务需求，编写响应的 HQL 语句</li><li>HQL 调用 Hive 中的 MapReduce 模板</li><li>通过模板运行 MapReduce 程序，生成响应的分析结果</li><li>将运行结果返回给客户端</li></ol><p><strong><em>注意点</em></strong></p><blockquote><p>Hive 的数据存储在 HDFS 中<br>Hive 分析数据底层的默认实现是 MapReduce<br>执行程序运行在 Yarn 上</p></blockquote><h2 id="Hive-的优缺点"><a href="#Hive-的优缺点" class="headerlink" title="Hive 的优缺点"></a>Hive 的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><blockquote><p>操作接口采用类 SQL 语法，提供快速开发的能力（简单、容易上手）<br>避免了写 MapReduce，减少学习成本、工作量<br>执行延迟较高，因此常用于数据分析，对实时性要求不高的场合<br>对于处理大数据有优势，处理小数据没有优势<br>支持自定义函数，可以根据自己的需求实现自己的函数</p></blockquote><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><blockquote><p>HQL 表达能力有限</p></blockquote><p>迭代式算法无法表达；<br>数据挖掘方面不擅长；</p><blockquote><p>效率低</p></blockquote><p>Hive 自动生成 MapReduce 作业，通常不够只能<br>Hive 调优困难，粒度较粗</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="/images/hive/hive.png" alt="Hive 架构"></p><h2 id="与数据库的比较"><a href="#与数据库的比较" class="headerlink" title="与数据库的比较"></a>与数据库的比较</h2><p>Hive 采用了类似于 SQL 的查询语言：HQL（Hive Query Language），因此很容易将 Hive 理解为数据库。<br>从结构上看，Hive 和数据库除了拥有类似的查询语言外，再无类似之处。</p><h3 id="查询语言"><a href="#查询语言" class="headerlink" title="查询语言"></a>查询语言</h3><p>由于 SQL 被广泛的应用在数据库中，因此基于 Hive 的特性，设计了类 SQL 的查询语言，熟悉 SQL 的话可以很方便的使用 Hive 做开发</p><h3 id="数据存储位置"><a href="#数据存储位置" class="headerlink" title="数据存储位置"></a>数据存储位置</h3><p>Hive 是建立在 Hadoop 上的，所有 Hive 的数据都是存储在 HDFS 中，而数据库则可以将数据存储在块设备或本地文件系统中</p><h3 id="数据更新"><a href="#数据更新" class="headerlink" title="数据更新"></a>数据更新</h3><p>由于 Hive 是针对数据仓库应用设计的，而数据仓库的内容是 <code>读多写少</code> 的，因此，<strong><em>Hive 不建议修改数据，所有的数据都是加载的时候就确定好的</em></strong>。<br>而数据库中的数据通常是需要进行修改的。</p><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 key 建立索引。<br>Hive 要访问数据中满足条件的特性值，需要 <code>暴力扫描整个数据</code>，因此访问的延迟较高。<br>由于 MapReduce 的引入，Hive 可以并行访问数据，因此即使没有索引，对于大量数据的访问，Hive 依然有优势。</p><p>数据库中，通常会针对一个或几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。</p><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><p>Hive 中大多数查询的执行是通过 Hadoop 的 MapReduce 来实现的，数据库通常有自己的执行引擎</p><h3 id="执行延迟"><a href="#执行延迟" class="headerlink" title="执行延迟"></a>执行延迟</h3><p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。<br>由于 MapReduce 本身具有较高延迟，因此在利用 MapReduce 执行 Hive 查询时，也会有较高的延迟。</p><p>数据库的执行延迟较低（在数据规模小的时候）</p><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><p>Hive 是建立在 Hadoop 上的，因此 Hive 的可扩展性和 Hadoop 的可扩展性是高度一致的。<br>数据库由于 ACID 雨衣的严格限制，扩展非常有限</p><h3 id="数据规模"><a href="#数据规模" class="headerlink" title="数据规模"></a>数据规模</h3><p>Hive 可以利用 MapReduce 进行并行运算，可以支持很大规模的数据<br>数据库可支持的数据规模要比 Hive 小很多</p><hr><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p><strong><em>示例使用 Hadoop-2.7.2、Hive 1.2.1、MySQL 5.6.24 版本</em></strong></p><p><strong><em>省略 Hadoop-2.7.2 安装</em></strong></p><h2 id="安装-hive"><a href="#安装-hive" class="headerlink" title="安装 hive"></a>安装 hive</h2><blockquote><p>将 hive-1.2.1 拷贝到 /opt/software 文件夹，并加压到 /opt/module/hive 下</p></blockquote><blockquote><p>拷贝 conf/hive-env.sh.template 文件为 hive-env.sh</p></blockquote><blockquote><p>修改 hive-env.sh</p></blockquote><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/opt/module/hadoop-2.7.2</span><br><span class="line">HIVE_CONF_DIR=/opt/module/hive/conf</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME HIVE_CONF_DIR</span><br></pre></td></tr></table></figure><blockquote><p>启动 hive</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 hive]# bin/hive</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><h2 id="测试-Hive、创建表"><a href="#测试-Hive、创建表" class="headerlink" title="测试 Hive、创建表"></a>测试 Hive、创建表</h2><blockquote><p>查看数据库</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">default</span><br><span class="line">Time taken: 0.397 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><blockquote><p>进入 default 数据库，创建一张 student 表</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use default;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.028 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; create table student(id int, name string);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.529 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">student</span><br><span class="line">Time taken: 0.018 seconds, Fetched: 1 row(s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive&gt; select * from student;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.291 seconds</span><br></pre></td></tr></table></figure><blockquote><p>插入一条数据</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert into student values(1, &apos;laiyy&apos;);</span><br><span class="line">Query ID = root_20191223102633_3f3c7996-6af7-4718-9634-7b0e13adc979</span><br><span class="line">Total jobs = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is set to 0 since there&apos;s no reduce operator</span><br><span class="line">Starting Job = job_1577067217027_0001, Tracking URL = http://hadoop03:8088/proxy/application_1577067217027_0001/</span><br><span class="line">Kill Command = /opt/module/hadoop-2.7.2/bin/hadoop job  -kill job_1577067217027_0001</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0</span><br><span class="line">2019-12-23 10:26:48,722 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2019-12-23 10:26:57,020 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.4 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 2 seconds 400 msec</span><br><span class="line">Ended Job = job_1577067217027_0001</span><br><span class="line">Stage-4 is selected by condition resolver.</span><br><span class="line">Stage-3 is filtered out by condition resolver.</span><br><span class="line">Stage-5 is filtered out by condition resolver.</span><br><span class="line">Moving data to: hdfs://hadoop02:9000/user/hive/warehouse/student/.hive-staging_hive_2019-12-23_10-26-33_045_4325354072781105943-1/-ext-10000</span><br><span class="line">Loading data to table default.student</span><br><span class="line">Table default.student stats: [numFiles=1, numRows=1, totalSize=8, rawDataSize=7]</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1   Cumulative CPU: 2.4 sec   HDFS Read: 3551 HDFS Write: 79 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 2 seconds 400 msec</span><br><span class="line">OK</span><br><span class="line">Time taken: 26.301 seconds</span><br></pre></td></tr></table></figure><blockquote><p>插入完成后，查看 HDFS Web UI</p></blockquote><p><img src="/images/hive/install/installcreate_table.png" alt="Hive 创建表、插入数据"></p><blockquote><p>再次插入一条数据，查看 WebUI</p></blockquote><p><img src="/images/hive/install/inser-data.png" alt="插入第二条数据"></p><p><strong><em>注意：这个 copy1 并不是真的拷贝的之前 <code>000000_0</code> 的数据！</em></strong></p><p>可以看到，两个文件的大小是不一样的，出现 <code>copy_1</code> 的原因是因为 HDFS 中同一个文件夹下不能出现两个相同名字的文件，所以新插入的数据生成的文件默认拼接了 <code>copy_1</code>，如果再插入数据，则会生成 <code>copy_2</code>、<code>copy_3</code> 以此类推</p><blockquote><p>查看数据</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from student;</span><br><span class="line">OK</span><br><span class="line">1laiyy</span><br><span class="line">2laiyy1</span><br><span class="line">Time taken: 0.039 seconds, Fetched: 2 row(s)</span><br><span class="line"></span><br><span class="line">hive&gt; select name from student;</span><br><span class="line">OK</span><br><span class="line">laiyy</span><br><span class="line">laiyy1</span><br><span class="line">Time taken: 0.069 seconds, Fetched: 2 row(s)</span><br><span class="line"></span><br><span class="line">hive&gt; select count(*) from student;</span><br><span class="line">Query ID = root_20191223103741_2b3573f9-5941-49c7-b4b7-dfd6e402e3d6</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Starting Job = job_1577067217027_0003, Tracking URL = http://hadoop03:8088/proxy/application_1577067217027_0003/</span><br><span class="line">Kill Command = /opt/module/hadoop-2.7.2/bin/hadoop job  -kill job_1577067217027_0003</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">2019-12-23 10:37:53,751 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2019-12-23 10:37:58,136 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.99 sec</span><br><span class="line">2019-12-23 10:38:02,253 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.52 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 2 seconds 520 msec</span><br><span class="line">Ended Job = job_1577067217027_0003</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.52 sec   HDFS Read: 6633 HDFS Write: 2 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 2 seconds 520 msec</span><br><span class="line">OK</span><br><span class="line">2</span><br><span class="line">Time taken: 22.124 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure><blockquote><p>退出 Hive</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; quit;</span><br></pre></td></tr></table></figure><p>此例中，<code>insert</code>、<code>select count(*)</code> 时，会运行 MapReduce 程序，其他不会。因为 <code>count(*)</code> 牵扯到计算；<code>insert</code> 牵扯到创建文件（也可以理解为计算）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hive 是 Facebook 开源的用于解决 &lt;code&gt;海量结构化日志&lt;/code&gt; 的数据统计。&lt;br&gt;Hive 是基于 Hadoop 的数据仓库地址，可以 &lt;code&gt;将结构化的数据文件映射为一张表&lt;/code&gt;，并提供 &lt;code&gt;类似 SQL&lt;/code&gt; 的查询功能。&lt;br&gt;Hive 的本质是将 HQL(Hive Query Language) 转化为 MapReduce。&lt;/p&gt;
    
    </summary>
    
      <category term="hive" scheme="https://www.laiyy.top/categories/hive/"/>
    
    
      <category term="hive" scheme="https://www.laiyy.top/tags/hive/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper（二） &lt;BR/&gt; 原理、Java API 操作</title>
    <link href="https://www.laiyy.top/zookeeper/zookeeper-2.html"/>
    <id>https://www.laiyy.top/zookeeper/zookeeper-2.html</id>
    <published>2019-12-20T02:30:43.000Z</published>
    <updated>2019-12-20T02:30:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;</p><a id="more"></a><h1 id="Zookeeper-内部原理"><a href="#Zookeeper-内部原理" class="headerlink" title="Zookeeper 内部原理"></a>Zookeeper 内部原理</h1><h2 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h2><blockquote><p>半数机制：集群中半数以上的机器存活，集群可用。所以 Zookeeper 适合安装 <strong><em>奇数台服务器</em></strong></p></blockquote><blockquote><p>Zookeeper 虽然在配置文件中没有指定 Master 和 Slave，但是 Zookeeper 工作室，是有一个节点为 Leader，其他为 Follower；Leader 是通过 <code>内部选举机制临时产生</code></p></blockquote><h3 id="选举过程"><a href="#选举过程" class="headerlink" title="选举过程"></a>选举过程</h3><p>假设有舞台服务器组成 Zookeeper 集群，它们的 id 为 <code>1~5</code>，同时，它们都是最新启动的，没有历史数据，所有配置一样。假设服务器依次启动，其选举过程为：</p><ol><li>服务器 A 启动，此时只有一台服务器启动，它发出的报文没有任何响应，选举状态为 <code>LOOKING</code></li><li>服务器 B 启动，与 A 通信，互相交换自己的选举结果。由于两者都没有历史数据，所以 id 较大的 B 胜出；但是，由于没有超过半数以上的服务器同意（5台服务器，半数以上是3)，所以 A、B 继续保持 <code>LOOKING</code></li><li>服务器 C 启动，与 A、B 通信，根据前面的分析，此时 C 是集群中的 Leader。</li><li>服务器 D 启动，与 A、B、C 通信，理论上来说应该是 D 为 Leader，但是 C 已经成为了 Leader， 所以 D 只能成为 Follower</li><li>服务器 E 启动，同样与 D 一样，只能当 Follower</li></ol><h2 id="节点类型"><a href="#节点类型" class="headerlink" title="节点类型"></a>节点类型</h2><blockquote><p>持久型</p></blockquote><p>客户端和服务器端断开链接后，创建的节点不删除；<br>Zookeeper 给该节点名称进行顺序编号</p><p>创建 ZNode 时，设置顺序标识，ZNode 名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护</p><p><strong><em>注意</em></strong>：在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序</p><blockquote><p>短暂型</p></blockquote><p>客户端与服务器端断开链接后，创建的节点自己删除；<br>Zookeeper 给该节点名称进行顺序编码</p><h2 id="Stat-结构体"><a href="#Stat-结构体" class="headerlink" title="Stat 结构体"></a>Stat 结构体</h2><p>使用 <code>stat</code> 命令，查看节点状态，返回值如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] stat /laiyy</span><br><span class="line">cZxid = 0x200000002</span><br><span class="line">ctime = Fri Dec 20 10:19:03 CST 2019</span><br><span class="line">mZxid = 0x200000011</span><br><span class="line">mtime = Fri Dec 20 10:36:33 CST 2019</span><br><span class="line">pZxid = 0x200000018</span><br><span class="line">cversion = 9</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 5</span><br><span class="line">numChildren = 7</span><br></pre></td></tr></table></figure><blockquote><p>cZxid</p></blockquote><p>每次修改 Zookeeper 状态，都会受到一个 zxid 形式的时间戳，也就是 Zookeeper 的 <code>事务 id</code>；<br>事务 id 是 Zookeeper 中所有修改的总的次序，每个修改都有一个 zxid，如果 <code>zxid1 &lt; zxid2</code>，那么说明 zxid1 在 zxid2 之前发生</p><blockquote><p>ctime</p></blockquote><p>ZNode 被创建时的毫秒数（1970开始）</p><blockquote><p>mZxid</p></blockquote><p>ZNode 最后一次修改的事务 id</p><blockquote><p>mtime</p></blockquote><p>最后一次修改的好描述（1970开始）</p><blockquote><p>pZxid</p></blockquote><p>ZNode 最后更新的子节点事务 id</p><blockquote><p>cversion</p></blockquote><p>ZNode 子节点变化数，ZNode 子节点修改次数</p><blockquote><p>dataVersion</p></blockquote><p>ZNode 数据变化号</p><blockquote><p>aclVersion</p></blockquote><p>ZNode 访问控制列表的变化号</p><blockquote><p>ephemeralOwner</p></blockquote><p>如果是临时节点，这个是 ZNode 拥有者的 sessionId；如果不是临时节点，则是 0</p><blockquote><p>dataLength</p></blockquote><p>ZNode 数据长度</p><blockquote><p>numChildren</p></blockquote><p>ZNode 子节点数</p><h2 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h2><blockquote><p>首先，需要一个 main 线程</p></blockquote><blockquote><p>在 main 线程中创建 Zookeeper 客户端，此时会创建两个线程，一个负责网络通信，一个负责监听</p></blockquote><blockquote><p>通过 connect 线程，将注册的监听事件发送到 Zookeeper</p></blockquote><blockquote><p>在 Zookeeper 的注册监听器列表中将注册的监听事件添加到列表中</p></blockquote><blockquote><p>Zookeeper 坚挺到有数据、路径的变化时，就会将这个消息发送给 监听线程</p></blockquote><blockquote><p>监听线程内部调用 process 方法</p></blockquote><blockquote><p>常见的监听：1、监听节点数据变化(get path [watch])；2、监听子节点增减的变化(ls path [watch])</p></blockquote><h2 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h2><blockquote><p>Client 向 Zookeeper 的 <code>Server1</code> 上写数据，发送一个写请求</p></blockquote><blockquote><p>如果 <code>Server1</code> 不是 Leader，那么 Server1 会把接收到的请求转发给 Leader。</p></blockquote><p>Zookeeper 的 Server 中有一个是 Leader，Leader 会将写请求广播给各个 Server，各个 Server 写成功后就会通知 Leader</p><blockquote><p>当 Leader 收到大多数 Server 数据写成功，就说明数据写成功</p></blockquote><p>如果有三个节点的话，只要有两个节点写成功，就认为数据写成功。写成功后，Leader 会告诉 Server1 数据写成功了</p><blockquote><p>Server1 通知 Client 数据写成功，流程结束</p></blockquote><hr><h1 id="API-操作"><a href="#API-操作" class="headerlink" title="API 操作"></a>API 操作</h1><h2 id="创建客户端连接"><a href="#创建客户端连接" class="headerlink" title="创建客户端连接"></a>创建客户端连接</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZOOKEEPER_HOST_URL = <span class="string">"hadoop02:2181,hadoop03:2181,hadoop04:2181"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SESSION_TIMEOUT = <span class="number">2000</span>;</span><br><span class="line"><span class="keyword">private</span> ZooKeeper client;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    client = <span class="keyword">new</span> ZooKeeper(ZOOKEEPER_HOST_URL, SESSION_TIMEOUT, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line">            System.out.println(watchedEvent);       </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@After</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> != client)&#123;</span><br><span class="line">        client.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">connect</span><span class="params">()</span></span>&#123;</span><br><span class="line">    System.out.println(client);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>全日制</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2019-12-20 11:37:46,542 INFO [org.apache.zookeeper.ZooKeeper] - Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT</span><br><span class="line"></span><br><span class="line">State:CONNECTING sessionid:0x0 local:null remoteserver:null lastZxid:0 xid:1 sent:0 recv:0 queuedpkts:0 pendingresp:0 queuedevents:0</span><br><span class="line"></span><br><span class="line">....</span><br></pre></td></tr></table></figure><h2 id="创建子节点"><a href="#创建子节点" class="headerlink" title="创建子节点"></a>创建子节点</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createNode</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    <span class="comment">// 参数解释</span></span><br><span class="line">    <span class="comment">// 参数 1：子节点路径</span></span><br><span class="line">    <span class="comment">// 参数 2：创建子节点时绑定的数据</span></span><br><span class="line">    <span class="comment">// 参数 3：权限</span></span><br><span class="line">    <span class="comment">//       ANYONE_ID_UNSAFE：任何人都可访问</span></span><br><span class="line">    <span class="comment">//       AUTH_IDS：只有创建者才可访问</span></span><br><span class="line">    <span class="comment">//       OPEN_ACL_UNSAFE：开放的 ACL，不安全（常用）</span></span><br><span class="line">    <span class="comment">//       CREATOR_ALL_ACL：授权用户具备权限</span></span><br><span class="line">    <span class="comment">//       READ_ACL_UNSAFE：制度权限</span></span><br><span class="line">    <span class="comment">// 参数 4：节点类型</span></span><br><span class="line">    <span class="comment">//       PERSISTENT：持久型</span></span><br><span class="line">    <span class="comment">//       PERSISTENT_SEQUENTIAL：持久型带节点序号</span></span><br><span class="line">    <span class="comment">//       EPHEMERAL：临时型</span></span><br><span class="line">    <span class="comment">//       EPHEMERAL_SEQUENTIAL：临时型带序号）</span></span><br><span class="line">    String path = client.create(<span class="string">"/laiyy"</span>, <span class="string">"test_api"</span>.getBytes(StandardCharsets.UTF_8), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">    System.out.println(path);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2019-12-20 11:47:27,378 INFO [org.apache.zookeeper.ClientCnxn] - Session establishment complete on server hadoop03/192.168.233.132:2181, sessionid = 0x36f2112be2e0003, negotiated timeout = 4000</span><br><span class="line">/laiyy</span><br><span class="line">2019-12-20 11:47:27,388 INFO [org.apache.zookeeper.ZooKeeper] - Session: 0x36f2112be2e0003 closed</span><br></pre></td></tr></table></figure><blockquote><p>查看服务器</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 8] ls /</span><br><span class="line">[laiyy, zookeeper]</span><br></pre></td></tr></table></figure><h2 id="获取子节点"><a href="#获取子节点" class="headerlink" title="获取子节点"></a>获取子节点</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getChildren</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    List&lt;String&gt; children = client.getChildren(<span class="string">"/"</span>, <span class="keyword">false</span>);</span><br><span class="line">    System.out.println(<span class="string">"子节点包括："</span> + children);</span><br><span class="line">    <span class="comment">// 休眠5秒，防止还没有获取到就已经关闭连接</span></span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">5</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="获取子节点，并监听数据变化"><a href="#获取子节点，并监听数据变化" class="headerlink" title="获取子节点，并监听数据变化"></a>获取子节点，并监听数据变化</h2><p>修改创建连接中的 Watcher，启动成功后查看日志</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">client = <span class="keyword">new</span> ZooKeeper(ZOOKEEPER_HOST_URL, SESSION_TIMEOUT, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line">            List&lt;String&gt; children = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">"------------------- 监控开始 ---------------------"</span>);</span><br><span class="line">                children = client.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</span><br><span class="line">                <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">                    System.out.println(child);</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(<span class="string">"------------------- 监控结束 ----------------------"</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (KeeperException | InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">------------------- 监控开始 ---------------------</span><br><span class="line">laiyy</span><br><span class="line">zookeeper</span><br><span class="line">------------------- 监控结束 ----------------------</span><br></pre></td></tr></table></figure><p>在服务器中进行创建、删除节点操作：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 18] create /test &apos;test&apos;</span><br><span class="line">Created /test</span><br><span class="line">[zk: localhost:2181(CONNECTED) 19] delete /tes</span><br></pre></td></tr></table></figure></p><p>控制台打印：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">------------------- 监控开始 ---------------------</span><br><span class="line">laiyy</span><br><span class="line">zookeeper</span><br><span class="line">test</span><br><span class="line">------------------- 监控结束 ----------------------</span><br><span class="line">------------------- 监控开始 ---------------------</span><br><span class="line">laiyy</span><br><span class="line">zookeeper</span><br><span class="line">------------------- 监控结束 ----------------------</span><br></pre></td></tr></table></figure></p><h2 id="获取节点状态"><a href="#获取节点状态" class="headerlink" title="获取节点状态"></a>获取节点状态</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">exist</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Stat stat = client.exists(<span class="string">"/laiyy"</span>, <span class="keyword">false</span>);</span><br><span class="line">    <span class="comment">// 如果节点不再存，stat 为 null</span></span><br><span class="line">    System.out.println(stat);</span><br><span class="line">    <span class="comment">// 休眠5秒，防止还没有获取到就已经关闭连接</span></span><br><span class="line">    TimeUnit.SECONDS.sleep(<span class="number">5</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h1 id="监听服务器节点动态上下线"><a href="#监听服务器节点动态上下线" class="headerlink" title="监听服务器节点动态上下线"></a>监听服务器节点动态上下线</h1><p><strong><em>需求</em></strong>：主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知主节点服务器的上下线</p><h2 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Server</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZOOKEEPER_HOST_URL = <span class="string">"hadoop02:2181,hadoop03:2181,hadoop04:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SESSION_TIMEOUT = <span class="number">2000</span>;</span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zooKeeper;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        args = <span class="keyword">new</span> String[]&#123;<span class="string">"localhost:001"</span>&#125;;</span><br><span class="line">        Server server = <span class="keyword">new</span> Server();</span><br><span class="line">        <span class="comment">// 连接集群</span></span><br><span class="line">        server.connect();</span><br><span class="line">        <span class="comment">// 注册节点</span></span><br><span class="line">        server.register(args[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">        TimeUnit.MINUTES.sleep(<span class="number">10</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">register</span><span class="params">(String hostname)</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</span><br><span class="line">        zooKeeper.create(<span class="string">"/servers/server"</span>, hostname.getBytes(StandardCharsets.UTF_8), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line">        System.out.println(hostname + <span class="string">" 上线了！"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">connect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        zooKeeper = <span class="keyword">new</span> ZooKeeper(ZOOKEEPER_HOST_URL, SESSION_TIMEOUT, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line">                System.out.println(watchedEvent);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="客户端监听"><a href="#客户端监听" class="headerlink" title="客户端监听"></a>客户端监听</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Client</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZOOKEEPER_HOST_URL = <span class="string">"hadoop02:2181,hadoop03:2181,hadoop04:2181"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SESSION_TIMEOUT = <span class="number">2000</span>;</span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zooKeeper;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Client client = <span class="keyword">new</span> Client();</span><br><span class="line"></span><br><span class="line">        client.connect();</span><br><span class="line"></span><br><span class="line">        client.getChildren();</span><br><span class="line"></span><br><span class="line">        TimeUnit.MINUTES.sleep(<span class="number">5</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">getChildren</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        List&lt;String&gt; children =</span><br><span class="line">                zooKeeper.getChildren(<span class="string">"/servers"</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 存储服务器节点集合名称</span></span><br><span class="line">        List&lt;String&gt; hosts = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">            <span class="keyword">byte</span>[] data = zooKeeper.getData(<span class="string">"/servers/"</span> + child, <span class="keyword">false</span>, <span class="keyword">null</span>);</span><br><span class="line">            hosts.add(<span class="keyword">new</span> String(data));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印主机名称</span></span><br><span class="line">        <span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">            System.out.println(<span class="string">" 监听到 "</span> + host);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">connect</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        zooKeeper = <span class="keyword">new</span> ZooKeeper(ZOOKEEPER_HOST_URL, SESSION_TIMEOUT, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</span><br><span class="line">                System.out.println(watchedEvent);</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    getChildren();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><blockquote><p>先在集群上创建一个 <code>/servers</code> 节点，然后启动 Client<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 20] create /servers &apos;servers&apos;</span><br><span class="line">Created /servers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># java 控制打印</span><br><span class="line">WatchedEvent state:SyncConnected type:None path:null</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>在服务器上创建 <code>/servers/server</code> 节点，查看 client 控制台</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 21] create -e -s /servers/server &apos;server&apos;</span><br><span class="line">Created /servers/server0000000000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># java 控制台打印</span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/servers</span><br><span class="line"> 监听到 server</span><br></pre></td></tr></table></figure><blockquote><p>运行 java 的 server，查看 server 和 client 控制台</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># server</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:None path:null</span><br><span class="line">localhost:001 上线了！</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># client </span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/servers</span><br><span class="line"> 监听到 localhost:001</span><br><span class="line"> 监听到 server2</span><br><span class="line"> 监听到 server</span><br></pre></td></tr></table></figure><blockquote><p>关掉 java 的 server，查看 client 输出</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/servers</span><br><span class="line"> 监听到 server2</span><br><span class="line"> 监听到 server</span><br></pre></td></tr></table></figure><p>由此可以验证，Client 实现了 server 动态上下线监听</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;nbsp;&lt;/p&gt;
    
    </summary>
    
      <category term="zookeeper" scheme="https://www.laiyy.top/categories/zookeeper/"/>
    
    
      <category term="zookeeper" scheme="https://www.laiyy.top/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper（一） &lt;BR/&gt; 安装、命令行操作</title>
    <link href="https://www.laiyy.top/zookeeper/zookeeper-1.html"/>
    <id>https://www.laiyy.top/zookeeper/zookeeper-1.html</id>
    <published>2019-12-19T05:55:16.000Z</published>
    <updated>2019-12-19T05:55:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>Zookeeper 是一个开源的分布式的，为分布式应用提供协调服务器的项目。</p><a id="more"></a><h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><p>Zookeeper 从设计模式角度来理解，是一个基于 <code>观察者模式</code> 设计的分布式服务管理框架，它负责存储和管理数据，然后接受 <code>观察者</code> 的注册，一旦数据的状态发生变化，Zookeeper 就会负责 <code>通知以及在 Zookeeper 上注册的观察者</code>，让其作出相应的反应。</p><p>Zookeeper 可以解释为 <code>文件系统</code> + <code>通知机制</code> 的整合。</p><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><blockquote><p>Zookeeper 是一个 <code>领导者(Leader)</code>、多个 <code>跟随者(Follower)</code> 组成的集群</p></blockquote><blockquote><p>集群中只要有半数以上的节点存活，Zookeeper 集群就能正常服务。(半数=以上是指，如果有 5 台服务器，则最少有 3 台正常；如果有 6 台服务器，最少有 4 台正常)</p></blockquote><blockquote><p>全局数据一致：每个 Server 上保存一份相同的数据备份，Client 无论连接到哪台 Server，数据都是一致的</p></blockquote><blockquote><p>更新请求顺序进行：来自同一个 Client 的更新请求将按照其发送顺序依次执行</p></blockquote><blockquote><p>数据更新的原子性：一次数据更新，要么成功，要么失败</p></blockquote><blockquote><p>实时性：在一定时间范围内，Client 能读取到最新数据</p></blockquote><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><p>Zookeeper 的数据模型结构与 Unix 文件系统类似，整体上可以看做是一棵树，每个节点称作是一个 <code>ZNode</code>。每个 ZNode 默认存储 <code>1MB</code> 数据，每个 ZNode 都可以通过其 <code>路径唯一标识</code>。</p><p><img src="/images/zk/zk.png" alt="Zookeeper 数据结构"></p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>Zookeeper 提供的服务包括：<code>统一命名管理</code>、<code>统一配置管理</code>、<code>统一集群管理</code>、<code>服务器节点动态上下线</code>、<code>软负载均衡</code>等</p><blockquote><p>统一命名管理</p></blockquote><p>在分布式环境下，经常需要对 <code>应用/服务</code> 进行统一命名，便于识别。类似于：ip 不容易记，而域名容易记。</p><blockquote><p>统一配置管理</p></blockquote><ol><li>在分布式环境下，配置文件同步非常重要</li></ol><p>一般要求一个集群中，所有节点的配置信息是一一致的；<br>对配置文件修改后，往往希望能够快速同步到各个节点上</p><ol start="2"><li>配置管理可以交给 Zookeeper 实现</li></ol><p>可以将配置信息写入 Zookeeper 上的一个 ZNode；<br>各个客户端服务器监听这个 ZNode；<br>一旦 ZNode 中数据被修改，Zookeeper 将通知各个客户端服务器</p><blockquote><p>统一集群管理</p></blockquote><ol><li>分布式环境下下，实时掌握各个节点的状态是非常必要的</li></ol><p>可根据节点实时状态做一些调整</p><ol start="2"><li>Zookeeper 可以实现实时监控节点状态变化</li></ol><p>可以将节点信息写入 Zookeeper 上的一个 ZNode；<br>监听这个 ZNode 获取它的实时状态变化</p><blockquote><p>服务器节点动态上下线</p></blockquote><p>客户端能实时洞察到服务器上下线变化</p><blockquote><p>软负载均衡</p></blockquote><p>在 Zookeeper 中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求</p><hr><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>以 <a href="https://archive.apache.org/dist/zookeeper/zookeeper-3.4.10/" target="_blank" rel="noopener">Zookeeper 3.4.10</a> 为例。</p><h2 id="本地模式安装"><a href="#本地模式安装" class="headerlink" title="本地模式安装"></a>本地模式安装</h2><p><strong><em>省略 JDK 安装步骤</em></strong></p><blockquote><p>将 Zookeeper 的压缩包拷贝到服务器上，解压到 /opt/software</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf zookeeper-3.4.10.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure><blockquote><p>修改配置</p></blockquote><ol><li><p>将 <code>/opt/module/zookeeper-3.4.10/conf/</code> 下的 <code>zoo_sample.cfg</code> 复制一份为 <code>zoo.cfg</code></p></li><li><p>修改 dataDir 路径</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 conf]# vim zoo.cfg </span><br><span class="line"># 心跳，2秒一次</span><br><span class="line">tickTime=2000</span><br><span class="line"># Leader 和 Follower 之间刚启动时进行通信的最大延迟时间：10 个心跳。如果超过 10 个心跳都没有通信，则认为 Leader 和 Follower 连接失败</span><br><span class="line">initLimit=10</span><br><span class="line"># 同步频率，5 个心跳；启动成后，5 个心跳没有通信，认为 Leader、Follower 连接失败</span><br><span class="line">syncLimit=5</span><br><span class="line"># 数据存储位置</span><br><span class="line">dataDir=/opt/module/zookeeper-3.4.10/zkData</span><br><span class="line"># 客户端的端口号，即 Client 访问 Zookeeper 时使用哪个端口通信</span><br><span class="line">clientPort=2181</span><br></pre></td></tr></table></figure><ol start="3"><li>在 <code>ZOOKEEPER_HOME</code> 中创建 zkData 文件夹</li></ol><blockquote><p>启动 Zookeeper 服务、查看启动结果</p></blockquote><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动服务</span></span><br><span class="line">[root@hadoop02 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh start</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看启动结果</span></span><br><span class="line">[root@hadoop02 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh status</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: standalone</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看进程</span></span><br><span class="line">[root@hadoop02 zookeeper-3.4.10]<span class="comment"># jps</span></span><br><span class="line">1425 Jps</span><br><span class="line">1369 QuorumPeerMain</span><br></pre></td></tr></table></figure><blockquote><p>启动客户端</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 zookeeper-3.4.10]# bin/zkCli.sh </span><br><span class="line">Connecting to localhost:2181</span><br><span class="line">2019-12-19 14:48:02,837 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT</span><br><span class="line">2019-12-19 14:48:02,839 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=hadoop02</span><br><span class="line">2019-12-19 14:48:02,839 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.8.0_144</span><br><span class="line">2019-12-19 14:48:02,841 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation</span><br><span class="line">2019-12-19 14:48:02,841 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/opt/module/jdk1.8.0_144/jre</span><br><span class="line">2019-12-19 14:48:02,841 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/opt/module/zookeeper-3.4.10/bin/../build/classes:/opt/module/zookeeper-3.4.10/bin/../build/lib/*.jar:/opt/module/zookeeper-3.4.10/bin/../lib/slf4j-log4j12-1.6.1.jar:/opt/module/zookeeper-3.4.10/bin/../lib/slf4j-api-1.6.1.jar:/opt/module/zookeeper-3.4.10/bin/../lib/netty-3.10.5.Final.jar:/opt/module/zookeeper-3.4.10/bin/../lib/log4j-1.2.16.jar:/opt/module/zookeeper-3.4.10/bin/../lib/jline-0.9.94.jar:/opt/module/zookeeper-3.4.10/bin/../zookeeper-3.4.10.jar:/opt/module/zookeeper-3.4.10/bin/../src/java/lib/*.jar:/opt/module/zookeeper-3.4.10/bin/../conf:</span><br><span class="line">2019-12-19 14:48:02,841 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/opt/module/protobuf-2.5.0:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib</span><br><span class="line">2019-12-19 14:48:02,841 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp</span><br><span class="line">2019-12-19 14:48:02,841 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=&lt;NA&gt;</span><br><span class="line">2019-12-19 14:48:02,841 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux</span><br><span class="line">2019-12-19 14:48:02,841 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64</span><br><span class="line">2019-12-19 14:48:02,842 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=3.10.0-693.el7.x86_64</span><br><span class="line">2019-12-19 14:48:02,842 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=root</span><br><span class="line">2019-12-19 14:48:02,842 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/root</span><br><span class="line">2019-12-19 14:48:02,842 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/opt/module/zookeeper-3.4.10</span><br><span class="line">2019-12-19 14:48:02,843 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@5c29bfd</span><br><span class="line">Welcome to ZooKeeper!</span><br><span class="line">2019-12-19 14:48:02,866 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1032] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">JLine support is enabled</span><br><span class="line">2019-12-19 14:48:02,937 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@876] - Socket connection established to localhost/127.0.0.1:2181, initiating session</span><br><span class="line">2019-12-19 14:48:02,955 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1299] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x16f1ce7c5fd0000, negotiated timeout = 30000</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:None path:null</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure><blockquote><p>退出客户端 </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 5] quit</span><br><span class="line">Quitting...</span><br><span class="line">2019-12-19 14:49:22,721 [myid:] - INFO  [main:ZooKeeper@684] - Session: 0x16f1ce7c5fd0000 closed</span><br><span class="line">2019-12-19 14:49:22,722 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@519] - EventThread shut down for session: 0x16f1ce7c5fd0000</span><br></pre></td></tr></table></figure><blockquote><p>关闭服务</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop02 zookeeper-3.4.10]# bin/zkServer.sh stop</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br></pre></td></tr></table></figure><h2 id="分布式安装"><a href="#分布式安装" class="headerlink" title="分布式安装"></a>分布式安装</h2><p>在 hadoop 基础上，在 hadoop02、hadoop03、hadoop04 三个节点上部署 Zookeeper</p><blockquote><p>在 hadoop02 上创建 zkData 文件夹，并在文件夹内创建一个 <code>myid</code> 文件<br>将 zookeeper 文件夹分发到剩余两台机器上</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync /opt/module/zookeeper-3.4.10/</span><br></pre></td></tr></table></figure><blockquote><p>修改三台服务器上的 <code>myid</code> 文件，文件内容分别为 2、3、4<br>修改 hadoop02 上的 zoo.cfg 文件，并分发到其他服务器上</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">####################cluster####################</span><br><span class="line"># 注意，必须以 server. 开头， . 后面的数字是 myid 文件中配置的数字</span><br><span class="line"># hadoop02 是服务器的 ip 地址或 hostname</span><br><span class="line"># 2888：服务器与集群中的 Leader 交换信息的端口</span><br><span class="line"># 3888：如果 Leader 挂了，需要一个端口来重新进行选举 </span><br><span class="line">server.2=hadoop02:2888:3888</span><br><span class="line">server.3=hadoop03:2888:3888</span><br><span class="line">server.4=hadoop04:2888:3888</span><br></pre></td></tr></table></figure><blockquote><p>分别启动 Zookeeper，查看状态</p></blockquote><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hadoop02</span></span><br><span class="line">[root@hadoop02 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh start</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">[root@hadoop02 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh status</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop03</span></span><br><span class="line">[root@hadoop03 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh start</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">[root@hadoop03 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh status</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: leader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop04</span></span><br><span class="line">[root@hadoop04 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh start</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line"></span><br><span class="line">[root@hadoop04 zookeeper-3.4.10]<span class="comment"># bin/zkServer.sh status</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><hr><h1 id="客户端命令行操作"><a href="#客户端命令行操作" class="headerlink" title="客户端命令行操作"></a>客户端命令行操作</h1><h2 id="help"><a href="#help" class="headerlink" title="help"></a>help</h2><p>可以显示所有操作命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] help</span><br><span class="line">ZooKeeper -server host:port cmd args</span><br><span class="line">stat path [watch]</span><br><span class="line">set path data [version]</span><br><span class="line">ls path [watch]</span><br><span class="line">delquota [-n|-b] path</span><br><span class="line">ls2 path [watch]</span><br><span class="line">setAcl path acl</span><br><span class="line">setquota -n|-b val path</span><br><span class="line">history </span><br><span class="line">redo cmdno</span><br><span class="line">printwatches on|off</span><br><span class="line">delete path [version]</span><br><span class="line">sync path</span><br><span class="line">listquota path</span><br><span class="line">rmr path</span><br><span class="line">get path [watch]</span><br><span class="line">create [-s] [-e] path data acl</span><br><span class="line">addauth scheme auth</span><br><span class="line">quit </span><br><span class="line">getAcl path</span><br><span class="line">close </span><br><span class="line">connect host:port</span><br></pre></td></tr></table></figure><h2 id="ls-path-watch"><a href="#ls-path-watch" class="headerlink" title="ls path [watch]"></a>ls path [watch]</h2><p>查看当前 ZNode 中所包含的内容；除根节点外，都不能以 / 结尾</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /</span><br><span class="line">[zookeeper]</span><br></pre></td></tr></table></figure><blockquote><p>节点监听</p></blockquote><p>在 hadoop03、hadoop04 上，监听 <code>/laiyy</code> 节点的变化。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /laiyy watch</span><br><span class="line">[gender0000000002, name, gender0000000001, gender0000000004, gender0000000003]</span><br></pre></td></tr></table></figure><p>在 hadoop02 上修改/创建一个 <code>/laiyy</code> 节点或子节点，查看 hadoop03、hadoop04 的变化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 16] create /laiyy/email &apos;laiyy0728@gmail.com&apos;</span><br><span class="line">Created /laiyy/email</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] </span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/laiyy</span><br></pre></td></tr></table></figure><p><strong><em>注意：监听方式只有一次有效，即监听到一次变化后，后续的就不再监听了</em></strong></p><h2 id="ls2-path-watch"><a href="#ls2-path-watch" class="headerlink" title="ls2 path [watch]"></a>ls2 path [watch]</h2><p>获取 ZNode 总所包含的内容，包括更新时间、次数等；除根节点外，都不能以 / 结尾</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] ls2 /</span><br><span class="line">[zookeeper]</span><br><span class="line">cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x0</span><br><span class="line">cversion = -1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure><h2 id="create"><a href="#create" class="headerlink" title="create"></a>create</h2><p>普通创建。  -s 含有序列， -e 临时（重启或超时即消失）</p><p><strong><em>注意</em></strong>： 创建节点的同时，需要写入数据，否则创建不成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] create /laiyy &quot;test_create&quot;</span><br><span class="line">Created /laiyy</span><br><span class="line">[zk: localhost:2181(CONNECTED) 5] ls /</span><br><span class="line">[laiyy, zookeeper]</span><br></pre></td></tr></table></figure><blockquote><p>短暂节点</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] create -e /laiyy/name &quot;laiyy&quot;</span><br><span class="line">Created /laiyy/name</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /laiyy</span><br><span class="line">[name]</span><br></pre></td></tr></table></figure><p>创建成功后，退出客户端重新连接查看 <code>/laiyy</code> 节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] quit</span><br><span class="line">Quitting...</span><br><span class="line">2019-12-20 10:24:15,052 [myid:] - INFO  [main:ZooKeeper@684] - Session: 0x26f2112be2b0001 closed</span><br><span class="line">2019-12-20 10:24:15,054 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@519] - EventThread shut down for session: 0x26f2112be2b0001</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop02 zookeeper-3.4.10]# bin/zkCli.sh </span><br><span class="line">Connecting to localhost:2181</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /laiyy</span><br><span class="line">[]</span><br></pre></td></tr></table></figure><blockquote><p>带序号的节点（持久型）</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] create -s /laiyy/gender &apos;man&apos;</span><br><span class="line">Created /laiyy/gender0000000001</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] ls /laiyy</span><br><span class="line">[gender0000000001]</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] create -s /laiyy/gender &apos;man&apos;</span><br><span class="line">Created /laiyy/gender0000000002</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] create -s /laiyy/gender &apos;man&apos;</span><br><span class="line">Created /laiyy/gender0000000003</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] create -s /laiyy/gender &apos;man&apos;</span><br><span class="line">Created /laiyy/gender0000000004</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 8] ls /laiyy</span><br><span class="line">[gender0000000002, gender0000000001, gender0000000004, gender0000000003]</span><br></pre></td></tr></table></figure><h2 id="get-path-watch"><a href="#get-path-watch" class="headerlink" title="get path [watch]"></a>get path [watch]</h2><p>获取节点的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 6] get /laiyy</span><br><span class="line">test_create</span><br><span class="line">cZxid = 0x200000002</span><br><span class="line">ctime = Fri Dec 20 10:19:03 CST 2019</span><br><span class="line">mZxid = 0x200000002</span><br><span class="line">mtime = Fri Dec 20 10:19:03 CST 2019</span><br><span class="line">pZxid = 0x200000002</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 11</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure><blockquote><p>监听值的变化</p></blockquote><p>在 hadoop03、hadoop04 上，监听 <code>/laiyy</code> 节点的变化。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] get /laiyy watch</span><br><span class="line">test_create</span><br><span class="line">cZxid = 0x200000002</span><br><span class="line">ctime = Fri Dec 20 10:19:03 CST 2019</span><br><span class="line">mZxid = 0x200000002</span><br><span class="line">mtime = Fri Dec 20 10:19:03 CST 2019</span><br><span class="line">pZxid = 0x20000000d</span><br><span class="line">cversion = 7</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 11</span><br><span class="line">numChildren = 5</span><br></pre></td></tr></table></figure><p>在 hadoop02 上，修改 <code>/laiyy</code> 节点的值，查看 hadoop03、hadoop04 上的输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 15] set /laiyy &apos;emmmm&apos;</span><br><span class="line">cZxid = 0x200000002</span><br><span class="line">ctime = Fri Dec 20 10:19:03 CST 2019</span><br><span class="line">mZxid = 0x200000011</span><br><span class="line">mtime = Fri Dec 20 10:36:33 CST 2019</span><br><span class="line">pZxid = 0x20000000d</span><br><span class="line">cversion = 7</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 5</span><br><span class="line">numChildren = 5</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] </span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDataChanged path:/laiyy</span><br></pre></td></tr></table></figure><p><strong><em>注意：监听方式只有一次有效，即监听到一次变化后，后续的就不再监听了</em></strong></p><h2 id="set"><a href="#set" class="headerlink" title="set"></a>set</h2><p>设置节点的具体值</p><p>创建 <code>/laiyy/name</code> 节点，赋值为 <code>laiyy</code>，使用 set 节点重新赋值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 12] get /laiyy/name</span><br><span class="line">laiyy</span><br><span class="line">cZxid = 0x20000000d</span><br><span class="line">ctime = Fri Dec 20 10:31:18 CST 2019</span><br><span class="line">mZxid = 0x20000000d</span><br><span class="line">mtime = Fri Dec 20 10:31:18 CST 2019</span><br><span class="line">pZxid = 0x20000000d</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 5</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 13] set /laiyy/name &apos;hahaha&apos;</span><br><span class="line">cZxid = 0x20000000d</span><br><span class="line">ctime = Fri Dec 20 10:31:18 CST 2019</span><br><span class="line">mZxid = 0x20000000e</span><br><span class="line">mtime = Fri Dec 20 10:32:50 CST 2019</span><br><span class="line">pZxid = 0x20000000d</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 14] get /laiyy/name         </span><br><span class="line">hahaha</span><br><span class="line">cZxid = 0x20000000d</span><br><span class="line">ctime = Fri Dec 20 10:31:18 CST 2019</span><br><span class="line">mZxid = 0x20000000e</span><br><span class="line">mtime = Fri Dec 20 10:32:50 CST 2019</span><br><span class="line">pZxid = 0x20000000d</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure><h2 id="stat"><a href="#stat" class="headerlink" title="stat"></a>stat</h2><p>查看节点状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] stat /laiyy</span><br><span class="line">cZxid = 0x200000002</span><br><span class="line">ctime = Fri Dec 20 10:19:03 CST 2019</span><br><span class="line">mZxid = 0x200000011</span><br><span class="line">mtime = Fri Dec 20 10:36:33 CST 2019</span><br><span class="line">pZxid = 0x200000018</span><br><span class="line">cversion = 9</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 5</span><br><span class="line">numChildren = 7</span><br></pre></td></tr></table></figure><h2 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h2><p>删除节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /laiyy</span><br><span class="line">[address, gender0000000002, name, gender0000000001, gender0000000004, email, gender0000000003]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] delete /laiyy/address</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /laiyy</span><br><span class="line">[gender0000000002, name, gender0000000001, gender0000000004, email, gender0000000003]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] delete /laiyy</span><br><span class="line">Node not empty: /laiyy</span><br></pre></td></tr></table></figure><h2 id="rmr"><a href="#rmr" class="headerlink" title="rmr"></a>rmr</h2><p>递归删除节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 5] rmr /laiyy</span><br><span class="line">[zk: localhost:2181(CONNECTED) 6] ls /  </span><br><span class="line">[zookeeper]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Zookeeper 是一个开源的分布式的，为分布式应用提供协调服务器的项目。&lt;/p&gt;
    
    </summary>
    
      <category term="zookeeper" scheme="https://www.laiyy.top/categories/zookeeper/"/>
    
    
      <category term="zookeeper" scheme="https://www.laiyy.top/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（19）Map Reduce &lt;BR /&gt; 多 Job 串联、Top N</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/hadoop-19.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/hadoop-19.html</id>
    <published>2019-12-18T01:37:41.000Z</published>
    <updated>2019-12-18T01:37:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>在之前的示例中，都是单个 Job 执行 MapReduce 程序，如何进行多 Job 串联？</p><a id="more"></a><h1 id="多-Job-串联"><a href="#多-Job-串联" class="headerlink" title="多 Job 串联"></a>多 Job 串联</h1><p>在有大量的文本（文档、网页）时，如何建立索引？</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>现有三个文档作为输入数据：<a href="/file/hadoop/map-reduce/a.txt">a.txt</a>、<a href="/file/hadoop/map-reduce/b.txt">b.txt</a>、<a href="/file/hadoop/map-reduce/c.txt">c.txt</a>，期望输出文件中某个字符串在哪个文件中，分别出现几次。</p><h3 id="第一次-MapReduce"><a href="#第一次-MapReduce" class="headerlink" title="第一次 MapReduce"></a>第一次 MapReduce</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Mapper</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FirstMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String fileName;</span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        FileSplit fileSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">        fileName = fileSplit.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String[] fields = value.toString().split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">for</span> (String field : fields) &#123;</span><br><span class="line">            String k = field + <span class="string">"--"</span> + fileName;</span><br><span class="line">            outKey.set(k);</span><br><span class="line">            context.write(outKey, outValue);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reducer</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FirstReduce</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outValue.set(sum);</span><br><span class="line"></span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Driver</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException, IOException, ClassNotFoundException </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">    Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">    job.setJarByClass(FirstDriver.class);</span><br><span class="line"></span><br><span class="line">    job.setMapperClass(FirstMapper.class);</span><br><span class="line">    job.setReducerClass(FirstReduce.class);</span><br><span class="line"></span><br><span class="line">    job.setMapOutputKeyClass(Text.class);</span><br><span class="line">    job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\dev\\hadoop\\job\\*.txt"</span>));</span><br><span class="line"></span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\dev\\hadoop\\job\\output"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    System.exit(succeed ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>运行结果：<br><img src="/images/hadoop/map-reduce/job1.png" alt="第一次运行结果"></p></blockquote><h3 id="第二次-MapReduce"><a href="#第二次-MapReduce" class="headerlink" title="第二次 MapReduce"></a>第二次 MapReduce</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Mapper</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SecondMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> Text outValue = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] split = line.split(<span class="string">"--"</span>);</span><br><span class="line">        outKey.set(split[<span class="number">0</span>]);</span><br><span class="line">        outValue.set(split[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        context.write(outKey, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reducer</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SecondReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outValue = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            builder.append(value.toString().replace(<span class="string">"\t"</span>, <span class="string">"--&gt;"</span>)).append(<span class="string">"\t"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outValue.set(builder.toString());</span><br><span class="line"></span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Driver</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">    Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">    job.setJarByClass(SecondDriver.class);</span><br><span class="line"></span><br><span class="line">    job.setMapperClass(SecondMapper.class);</span><br><span class="line">    job.setReducerClass(SecondReducer.class);</span><br><span class="line"></span><br><span class="line">    job.setMapOutputKeyClass(Text.class);</span><br><span class="line">    job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\dev\\hadoop\\job\\output\\part-r-00000"</span>));</span><br><span class="line"></span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\dev\\hadoop\\job\\output1"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    System.exit(succeed ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>运行结果</p></blockquote><p><img src="/images/hadoop/map-reduce/job2.png" alt="第二次运行结果"></p><h1 id="TopN"><a href="#TopN" class="headerlink" title="TopN"></a>TopN</h1><p>将 <a href="/map-reduce/sort/hadoop-14.html#实现">排序 Demo</a> 的输出结果进行加工，输出总使用量的前 5 位。</p><h2 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h2><p>FlowBean 保持 <a href="/map-reduce/sort/hadoop-14.html#实现">排序 Demo</a> 中的实现不变，修改 Mapper、Reducer</p><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopnMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个 TreeMap，作为存储数据的容器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> TreeMap&lt;FlowBean, Text&gt; flowMap = <span class="keyword">new</span> TreeMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FlowBean flowBean ;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">        String phone = fields[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> upFlow = Long.parseLong(fields[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">long</span> downFlow = Long.parseLong(fields[<span class="number">2</span>]);</span><br><span class="line">        <span class="comment">// 在 map 中创建对象，否则将只会在 flowMap 中存入同一个对象</span></span><br><span class="line">        flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">        flowBean.set(upFlow, downFlow);</span><br><span class="line"></span><br><span class="line">        Text v = <span class="keyword">new</span> Text();</span><br><span class="line">        v.set(phone);</span><br><span class="line"></span><br><span class="line">        flowMap.put(flowBean, v);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 限制 TreeMap 数量</span></span><br><span class="line">        <span class="keyword">if</span> (flowMap.size() &gt; <span class="number">5</span>) &#123;</span><br><span class="line">            flowMap.remove(flowMap.lastKey());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 遍历 map，输出数据</span></span><br><span class="line">        flowMap.forEach((key, value) -&gt; &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                context.write(key, value);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException | InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopnReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个 TreeMap，作为存储数据的容器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> TreeMap&lt;FlowBean, Text&gt; flowMap = <span class="keyword">new</span> TreeMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">            flowBean.set(key.getUpFlow(), key.getDownFlow());</span><br><span class="line"></span><br><span class="line">            flowMap.put(flowBean, <span class="keyword">new</span> Text(value));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果超过 10 条，去掉流量最小的一条</span></span><br><span class="line">            <span class="keyword">if</span> (flowMap.size() &gt; <span class="number">5</span>) &#123;</span><br><span class="line">                flowMap.remove(flowMap.lastKey());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 输出数据</span></span><br><span class="line">        flowMap.forEach((key, value) -&gt; &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                context.write(value, key);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException | InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>省略 Driver，查看结果</p></blockquote><p><img src="/images/hadoop/map-reduce/topn.png" alt="top n"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在之前的示例中，都是单个 Job 执行 MapReduce 程序，如何进行多 Job 串联？&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（18） YARN &lt;BR /&gt; 资源调度器、Hadoop 优化</title>
    <link href="https://www.laiyy.top/hadoop/yarn/hadoop-18.html"/>
    <id>https://www.laiyy.top/hadoop/yarn/hadoop-18.html</id>
    <published>2019-12-16T07:33:32.000Z</published>
    <updated>2019-12-16T07:33:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>Yarn 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的 <code>操作系统</code>，而 MapReduce 等运算程序相当于 <code>操作系统上的应用程序</code></p><a id="more"></a><h1 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h1><p>Yarn 由 <code>ResourceManager</code>、<code>NodeManager</code>、<code>ApplicationMaster</code>、<code>Container</code> 等组件构成。</p><blockquote><p>ResourceManager</p></blockquote><ol><li>处理客户端请求</li><li>监控 NodeManager</li><li>启动或监控 ApplicationMaster</li><li>资源的分配和调度</li></ol><blockquote><p>NodeManager</p></blockquote><ol><li>管理单个节点上的资源</li><li>处理来自 ResourceManager 的命令</li><li>处理来自 ApplicationMaster 的命令</li></ol><blockquote><p>ApplicationMaster</p></blockquote><ol><li>负责数据的切分</li><li>为应用程序申请资源，并分配给内部的任务</li><li>任务的监控、容错</li></ol><blockquote><p>Container</p></blockquote><p>是 YARN 资源的抽象，封装了某个节点上的多维度资源，如：内存、CPU、磁盘、网络等。</p><hr><h1 id="工作机制"><a href="#工作机制" class="headerlink" title="工作机制"></a>工作机制</h1><ol><li>MapReduce 程序提交任务到客户端所在的节点</li><li>申请一个 Application 到 ResourceManager</li><li>ResourceManager 返回 <code>资源提交路径</code> 和 <code>application_id</code></li><li>将任务所需资源提交到 <code>资源提交路径</code> 上，包括切片信息(Job.Split)，任务信息（Job.xml）、所需 jar 包</li><li>资源提交完成后，申请运行 MrAppMaster</li><li>ResourceManager 将用户的请求初始化为一个 Task，进入任务队列</li><li>NodeManager 到 ResourceManager 领取 Task 任务</li><li>NodeManager 创建 Container 容器，分配 CPU、内存等资源，启动对应的 MrAppMaster</li><li>NodeManager 下载 job 资源到本地，MrAppMaster 读取切片信息，决定开启多少个 MapTask</li><li>Container 向 ResourceManager 申请运行 MapTask 容器</li><li>其余 NodeManager 重复 7-10 步骤，等待任务领取完成</li><li>任务领取完成后，MrAppMaster 统一发送程序运行脚本，启动 MapTask</li><li>所有 MapTask 执行完成后，由 MrAppMaster 向 ResourceManager 申请对应切片数量的 ReduceTask，进行 reduce 工作</li><li>ReduceTask 从 MapTask 中获取相应的数据</li><li>ReduceTask 执行完成后，MrAppMaster 想 ResourceManager 申请注销自己</li></ol><p><img src="/images/hadoop/yarn/yarn.png" alt="yarn"></p><hr><h1 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h1><p>资源调度器对应上图中的 <strong><em>FIFO调度队列</em></strong>。</p><p>Hadoop 资源调度器主要有三种：<code>FIFO(队列)</code>、<code>Capacity Scheduler(容量调度器)</code>、<code>Fair Scheduler(公平调度器)</code>， 默认资源调度器为 <code>Capacity Scheduler</code></p><p><img src="/images/hadoop/yarn/yarn-scheduler-class.png" alt="默认资源调度器"></p><h2 id="FIFO"><a href="#FIFO" class="headerlink" title="FIFO"></a>FIFO</h2><p>按照到达时间，先到先服务；单项执行</p><p>当有新的服务器节点资源时，从队列中获取一个任务，从任务重分配一个 Task 给节点进行服务</p><p><img src="/images/hadoop/yarn/fifo.png" alt="FIFO"></p><h2 id="Capacity-Scheduler"><a href="#Capacity-Scheduler" class="headerlink" title="Capacity Scheduler"></a>Capacity Scheduler</h2><p>Hadoop 默认调度器，按照到达时间，先到先服务；并发执行</p><blockquote><p>支持多个队列，每个队列可配置一定的资源量，每个队列采用 FIFO 调度策略<br>为防止同一个用户的作业独占队列中的资源，调度器会对同一个用户提交的作业所占资源量进行限制。<br>如果调度器中有三个队列，可以从三个队列的头部取出三个任务并发执行，相比 FIFO 提高了任务的执行速度。</p></blockquote><p>计算方式：<br>首先，计算每个队列中正在执行的任务数与其应该分得的资源之间的比值，选择一个最小（最闲）的队列；<br>其次，按照作业优先级和提交时间顺序，同时考虑用户资源量限制和内存限制对队列内的任务进项排序</p><h2 id="Fair-Scheduler"><a href="#Fair-Scheduler" class="headerlink" title="Fair Scheduler"></a>Fair Scheduler</h2><p>按照缺额排序，缺额越大越优先；并发度最高</p><blockquote><p>支持多队列、多用户<br>每个队列中的资源量可以配置<br>同一个队列中的作业公平共享队列的所有资源</p></blockquote><p>分配方式：<br>假设有三个队列：QA、QB、QC，每个队列中的任务按照优先级分配资源，优先级越高分配的资源越多。但是每个任务都会分配到资源，以确保 <code>公平</code>。<br>在资源有限的情况下，每个任务理想情况下获得的资源与实际获得的资源可能存在一定的差距，这个差距就称为 <code>缺额</code>。<br>通一个队列中，任务的资源缺额越大，越先获得资源优先执行。作业是按照缺额的高低来先后执行的，且多个作业同时运行。</p><h1 id="任务的推测执行"><a href="#任务的推测执行" class="headerlink" title="任务的推测执行"></a>任务的推测执行</h1><p><strong><em>作业完成时间取决于最慢任务的完成时间</em></strong></p><p>一个作业由若干个 Map 任务和 Reduce 任务构成，因硬件老化、软件 bug 等，某些任务可能运行的非常慢（如：99% 的Map 都完成了，少数的 Mpa 进度很慢完不成）</p><p>解决方案：</p><p>为慢的任务启动一个 <code>备份任务</code>，同时运行，谁先运行完就采用谁的结果。</p><blockquote><p>执行推测任务的前台条件</p></blockquote><ol><li>每个 Task 只能有一个备份任务</li><li>当前 Job 已完成的 Task 必须小于 5%</li><li>在 mapred-site.xml 中开启推测执行（默认是打开的）</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p>不能启动推测任务的情况</p></blockquote><ol><li>任务间存在严重的负载倾斜</li><li>特殊任务（如向数据库中写数据）</li></ol><h2 id="推测方法"><a href="#推测方法" class="headerlink" title="推测方法"></a>推测方法</h2><p>假设某一时刻，任务 T 的执行进度为 progress，则可通过一定的算法来推测出该任务最终完成的时刻 <code>endTime</code>；另一方面，如果此刻为该任务开启一个备份任务，则可以推断出备份任务可能的完成时刻 <code>endTime1</code>。则：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">runTime = (currentTimestamp - taskStartTime) / progress</span><br><span class="line">推测运行时间 = (当前时刻 - 任务启动时刻) / 任务运行比例</span><br><span class="line"></span><br><span class="line">endTime = runTIme + taskStartTime</span><br><span class="line">推测结束时刻 = 运行时间 + 任务启动时刻</span><br><span class="line"></span><br><span class="line">entTime1 = currentTimestamp + avgRunTime</span><br><span class="line">备份任务推测完成时刻 = 当前时刻 + 运行完成任务的平均时间</span><br></pre></td></tr></table></figure><blockquote><p>MR 总是选择 entTime - endTime1 差值最大的任务，并为之启动备份任务<br>为防止大量任务同时启动备份任务造成资源浪费，MR 为每个作业设置了同时启动备份任务数目的上限<br>推测执行机制实际上采用了经典的优化方案：<code>以空间换时间</code>。同时启动多个相同的任务处理相同数据，并让这些任务竞争，以缩短数据处理时间。显然这种方法占用更多的计算资源。在集群资源紧缺的情况下，应合理使用，争取在多用少量资源的情况下，减少作业的计算时间</p></blockquote><hr><h1 id="Hadoop-优化"><a href="#Hadoop-优化" class="headerlink" title="Hadoop 优化"></a>Hadoop 优化</h1><h2 id="MapReduce-速度慢的原因"><a href="#MapReduce-速度慢的原因" class="headerlink" title="MapReduce 速度慢的原因"></a>MapReduce 速度慢的原因</h2><p>MapReduce 效率的瓶颈在于：计算机性能、IO 操作优化</p><p>计算机性能包括：</p><blockquote><p>CPU、内存、磁盘监控、网络</p></blockquote><p>IO 操作优化包括：</p><blockquote><p>数据倾斜<br>Map 和 Reduce 数设置不合理<br>Map 运行时间太长，导致 Reduce 等待太久<br>小文件过多<br>大量不可分块的超大文件<br>溢写次数过多<br>归并次数过多</p></blockquote><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p>可以从六个方面考虑优化：数据输入、Map、Reduce、IO、数据倾斜、参数调优</p><h3 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h3><blockquote><p>合并小文件</p></blockquote><p>在执行 MR 任务之前，将小文件进行合并，大量小文件会产生大量的 Map 任务，增加 Map 任务装载数，而任务的装载比较耗时，从而导致 MR 运行慢。</p><p>解决办法：采用 CombinerTextInputFormat 作为输入，解决输入端大量小文件的问题。</p><h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><blockquote><p>减小溢写次数</p></blockquote><p>通过调整 <strong><em>mapred-site.xml</em></strong> 文件中的 <code>mapreduce.task.io.sort.mb</code> 和 <code>mapreduce.map.sort.spill.percent</code> 参数，增大触发溢写的内存上限，减少溢写次数，从而减小磁盘 IO</p><blockquote><p>减少合并次数</p></blockquote><p>通过调整 <strong><em>mapred-site.xml</em></strong> 文件中的 <code>mapreduce.task.io.sort.factor</code> 参数，增大合并的文件数目，减少合并次数，从而缩短 MR 处理时间</p><blockquote><p>在 Map 之后，不影响业务逻辑的前台下，先进行 Combine 处理，减少 IO（适用于汇总）</p></blockquote><h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h3><blockquote><p>合理设置 Map、Reduce 数</p></blockquote><p>影响 Map 个数的是 <code>切片</code>，影响 reduce 个数的是 <code>setNumReduceTasks</code> 方法。<br>这两个数值都不能设置太小，也不能太大。<br>太小会导致 Task 等待，处理时间长；太大会导致 Map、Reduce 任务间竞争资源，造成处理超时等错误</p><blockquote><p>设置 Map、Reduce 共存</p></blockquote><p>调整 <strong><em>mapred-site.xml</em></strong> 文件中的 <code>mapreduce.job.reduce.slowstart.completedmaps</code> 参数，使 Map 运行到一定程度后，Reduce 也开始运行，减少 Reduce 等待时间。</p><blockquote><p>规避使用 Reduce</p></blockquote><p>由于 Reduce 在用于连接数据集的时候会产生大量的网络消耗，如果不需要使用 Reduce，则可以进行规避，减少大量的 shuffle 时间</p><blockquote><p>合理设置 Reduce 的 buffer</p></blockquote><p>默认情况下，数据达到一个阈值的时候，Buffer 中的数据就会写入磁盘，然后 Reduce 会从磁盘中获得所有数据。<br>Buffer 和 Reduce 是没有直接关联的，中间有多次 <code>写磁盘 -&gt; 读磁盘</code> 的过程，可以通过调整参数来规避，使得 Buffer 中的一部分数据可以直接输送到 Reduce，减少 IO 开销。</p><p>通过调整 <strong><em>mapred-site.xml</em></strong> 文件中的 <code>mapreduce.reduce.input.buffer.percent</code> 配置，默认为 0.0。 当数值大于 0 时，会保留指定比例的内存读 Buffer 中的数据直接交给 Reduce，这样一来，设置 Buffer 需要内存、读取数据需要内存、Reduce 计算也需要内存，如果调整的不合理可能会撑爆服务器，因此需要根据作业运行情况去进行调整。</p><h3 id="IO"><a href="#IO" class="headerlink" title="IO"></a>IO</h3><blockquote><p>数据压缩</p></blockquote><p>安装 Snappy 或 LZO，开启数据压缩</p><blockquote><p>使用 SequenceFile 二进制文件</p></blockquote><h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><p>数据倾斜包含：数据频率倾斜（某一区域内的数据量远远大于其他区域）、数据大小倾斜（部分记录的大小远远大于平均值）</p><p><strong><em>解决方案</em></strong></p><blockquote><p>抽样和范围分区</p></blockquote><p>通过对原始数据进行抽样得到的结果集来预设分区边界值。</p><blockquote><p>自定义分区</p></blockquote><p>使用自定义分区，将某些 key 发送给固定的 Reduce 实例，将剩余 key 发送给剩余的 Reduce 实例</p><blockquote><p>Combine</p></blockquote><p>使用 Combine 可以大量较小数据倾斜，在可能的情况下，Combine 的目的就是聚合并精简数据</p><blockquote><p>采用 MapJoin，避免 ReduceJoin</p></blockquote><h3 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h3><blockquote><p>mapred-default.xml</p></blockquote><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">mapreduce.map.memory.mb</td><td style="text-align:center">一个 MapTask 可以使用的资源上线，默认 1024M。如果 MapTask 实际使用的资源超过该值，将被强制杀死</td></tr><tr><td style="text-align:center">mapreduce.reduce.memory.mb</td><td style="text-align:center">一个 ReduceTask 可以使用的资源上线，默认 1024M。如果 ReduceTask 实际使用的资源超过该值，将被强制杀死</td></tr><tr><td style="text-align:center">mapreduce.map.cpu.vcores</td><td style="text-align:center">一个 MapTask 可使用的最大 CPU 数目，默认 1</td></tr><tr><td style="text-align:center">mapreduce.reduce.cpu.vcores</td><td style="text-align:center">一个 ReduceTask 可使用的最大 CPU 数目，默认 1</td></tr><tr><td style="text-align:center">mapreduce.reduce.shuffle.parallelcopies</td><td style="text-align:center">每个 Reduce 去 Map 中获取数据的并行数，默认 5</td></tr><tr><td style="text-align:center">mapreduce.reduce.shuffle.merge.percent</td><td style="text-align:center">Buffer 中的数据达到多少比例开始写入磁盘，默认 0.66</td></tr><tr><td style="text-align:center">mapreduce.reduce.shuffle.input.buffer.percent</td><td style="text-align:center">Buffer 大小占 Reduce 可用内存的比例，默认 0.7</td></tr><tr><td style="text-align:center">mapreduce.reduce.input.buffer.percent</td><td style="text-align:center">指定多少比例的内存用来存放 Buffer 中的数据，默认 0.0</td></tr><tr><td style="text-align:center">mapreduce.task.io.sort.mb</td><td style="text-align:center">Shuffle 的环形缓冲区大小，默认 100M</td></tr><tr><td style="text-align:center">mapreduce.map.sort.spill.percent</td><td style="text-align:center">环形缓冲区溢写的阈值，默认 0.8</td></tr><tr><td style="text-align:center">mapreduce.map.maxattempts</td><td style="text-align:center">每个 MapTask 最大重试次数，超过该值认为 MapTask 失败，默认 4</td></tr><tr><td style="text-align:center">mapreduce.reduce.maxattempts</td><td style="text-align:center">每个 ReduceTask 最大重试次数，超过该值认为 ReduceTask 失败，默认 4</td></tr><tr><td style="text-align:center">mapreduce.task.timeout</td><td style="text-align:center">Task 超时时间，如果 Task 在一定时间内没有读取新数据，也没有输出数据，则认为 Task 处于 Block 状态，为防止因为用户程序永远 Block 不退出，则强制设置一个超时时间，默认为 10 分钟</td></tr></tbody></table><blockquote><p>yarn-default.xml</p></blockquote><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">yarn.scheduler.minimum-allocation-mb</td><td style="text-align:center">应用程序 Container 分配的最小内存，默认 1024</td></tr><tr><td style="text-align:center">yarn.scheduler.maximum-allocation-mb</td><td style="text-align:center">应用程序给 Container 份分配的最大内存，默认 8192</td></tr><tr><td style="text-align:center">yarn.scheduler.minimum-allocation-vcores</td><td style="text-align:center">每个 Container 申请最小 CPU 核数，默认 1</td></tr><tr><td style="text-align:center">yarn.scheduler.maximum-allocation-vcores</td><td style="text-align:center">每个 Container 申请的最大 CPU 核数，默认 4</td></tr><tr><td style="text-align:center">yarn.nodemanager.resource.memory-mb</td><td style="text-align:center">给 Container 分配的最大物理内存，默认 8192</td></tr></tbody></table><h2 id="小文件优化"><a href="#小文件优化" class="headerlink" title="小文件优化"></a>小文件优化</h2><blockquote><p>小文件弊端</p></blockquote><p>HDFS 上每个文件都要在 NameNode 上建立一个索引，索引大小约 150 byte，当小文件较多时会产生很多索引文件，一方面会大量占用 NameNode 的内存空间，另一方面就是索引文件过大，使得索引速度变慢。</p><blockquote><p>优化方式</p></blockquote><ol><li>在数据采集时，将小文件或小批数据合并为大文件再上传 HDFS</li><li>在业务处理前，在 HDFS 上使用 MapReduce 程序，对小文件进行合并</li><li>在 MapReduce 处理时，使用 CombineTextInputFormat 提高效率</li></ol><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><blockquote><p>Hadoop  Archive</p></blockquote><p>归档是一个高效的将小文件放入 HDFS 块中的文件存档工具，能工将多个小文件打包为一个 HAR 文件，减少 NameNode 内存使用</p><blockquote><p>Sequence File</p></blockquote><p>由一系列二进制 KV 组成，如果 key 为文件名，value 为文件内存，则可以将大批小文件合并成一个大文件</p><blockquote><p>CombineFileInputFormat</p></blockquote><p>新的 InputFormat，用于将多个文件合并为一个单独的 Split，且它会考虑数据的存储位置</p><blockquote><p>开启 JVM 重用</p></blockquote><p>对于大量小文件的任务，可以开启 JVM 重用，会减少大约一半的运行时间。<br>JVM 重用原理：一个 Map 运行在一个 JVM 中，开启后，该 Map 在 JVM 上运行完毕后，JVM 会继续运行其他的 Map。<br>可以通过修改 <strong><em>mapred-site.xml</em></strong> 中的 <code>mapreduce.job.jvm.numtasks</code> 参数，默认为 1，即运行一个 Task 就销毁 JVM</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Yarn 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的 &lt;code&gt;操作系统&lt;/code&gt;，而 MapReduce 等运算程序相当于 &lt;code&gt;操作系统上的应用程序&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="yarn" scheme="https://www.laiyy.top/categories/hadoop/yarn/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="yarn" scheme="https://www.laiyy.top/tags/yarn/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（17） Map Reduce &lt;BR /&gt; 计数器、压缩</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/hadoop-17.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/hadoop-17.html</id>
    <published>2019-12-13T06:45:52.000Z</published>
    <updated>2019-12-13T06:45:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop 为每个作业维护若干个内置计数器，以描述多项指标。<br>如：记录已处理的字节数和记录数，使用户可以监控已处理的输入数据量和已产生的输出数据量</p><a id="more"></a><h1 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h1><p>计数器 API</p><blockquote><p>枚举方式计数<br>采用计数器组、计数器名称的方式计数<br>计数器结果在程序运行后，在控制台上查看</p></blockquote><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>在运行核心业务 MapReduce 之前，往往要先进行数据清洗，清理掉不符合用户要求的数据。清理过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序</p><p>使用一个项目中的日志信息作为输入数据，去除字段长度小于 11 的日志信息</p><p><strong><em>数据信息敏感，不公开</em></strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> result = parseLog(line, context);</span><br><span class="line">        <span class="keyword">if</span> (result)&#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">parseLog</span><span class="params">(String line, Context context)</span> </span>&#123;</span><br><span class="line">        String[] fields = line.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>)&#123;</span><br><span class="line">            context.getCounter(<span class="string">"map"</span>, <span class="string">"true"</span>).increment(<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        context.getCounter(<span class="string">"map"</span>, <span class="string">"false"</span>).increment(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取 job 对象</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">    Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置 jar 存放路径</span></span><br><span class="line">    job.setJarByClass(LogDriver.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关联 Mapper、Reducer 业务类</span></span><br><span class="line">    job.setMapperClass(LogMapper.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定最终输出的数据 KV 类型</span></span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 job 的输入文件所在目录</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\dev\\log\\*"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 job 的输出结果所在目录</span></span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\dev\\log\\output"</span>));</span><br><span class="line"></span><br><span class="line">    job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提交 job</span></span><br><span class="line">    <span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    System.exit(succeed ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>查看结果</p></blockquote><p>数据清洗前：<br><img src="/images/hadoop/shuffle/log-before.png" alt="数据清洗前"></p><p>数据清洗后：<br><img src="/images/hadoop/shuffle/log-after.png" alt="数据清洗后"></p><p><strong><em>具体更复杂的数据清洗，根据输入数据的不同进行高度定制化，此例只做基础概念思想</em></strong></p><hr><h1 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h1><p>压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行 MapReduce 程序时，IO 操作、网络传输、shuffle 和 merge 要花费大量的时间，尤其是数据规模很大、工作负载密集的情况下，因此使用数据压缩非常重要。</p><p>磁盘IO 和网络带宽是 hadoop 的宝贵资源，数据压缩对于节省资源、最小化磁盘 IO、网络传输很有帮助；可以在 <strong><code>任意</code></strong> MapReduce 阶段启用压缩。压缩也是有代价的。</p><p>压缩是提高 hadoop 运行效率的一种优化策略，通过对 Mapper、Reduce 程序运行过程的数据进行压缩，以减少磁盘 IO，提高 MapReduce 程序运行速度。</p><p><strong><em>注意：采用压缩技术，减少了磁盘 IO，但同时增加了 CPU 运算负担，所以，压缩特性运用得当，能提高性能，但是运用不得当也可能降低性能</em></strong></p><blockquote><p>压缩基本原则</p></blockquote><ol><li>运算密集型的任务，少用压缩</li><li>IO 密集型的任务，多用压缩</li></ol><h2 id="压缩编码"><a href="#压缩编码" class="headerlink" title="压缩编码"></a>压缩编码</h2><table><thead><tr><th style="text-align:center">压缩格式</th><th style="text-align:center">是否 hadoop 自带</th><th style="text-align:center">算法</th><th style="text-align:center">文件扩展名</th><th style="text-align:center">是否可切分</th><th style="text-align:center">是否需要修改程序</th><th>对应编解码器</th></tr></thead><tbody><tr><td style="text-align:center">DEFLATE</td><td style="text-align:center">是</td><td style="text-align:center">DEFLATE</td><td style="text-align:center">.deflate</td><td style="text-align:center">否</td><td style="text-align:center">否</td><td>DefaultCodec</td></tr><tr><td style="text-align:center">GZIP</td><td style="text-align:center">是</td><td style="text-align:center">DEFLATE</td><td style="text-align:center">.gz</td><td style="text-align:center">否</td><td style="text-align:center">否</td><td>GzipCodec</td></tr><tr><td style="text-align:center">bzip2</td><td style="text-align:center">是</td><td style="text-align:center">bzip2</td><td style="text-align:center">.bz2</td><td style="text-align:center"><strong><em>是</em></strong></td><td style="text-align:center">否</td><td>BZip2Codec</td></tr><tr><td style="text-align:center">LZO</td><td style="text-align:center"><strong><em>否</em></strong></td><td style="text-align:center">LZO</td><td style="text-align:center">.lzo</td><td style="text-align:center"><strong><em>是</em></strong></td><td style="text-align:center"><strong><em>是，需要建索引、指定输入格式</em></strong></td><td>LzopCodec</td></tr><tr><td style="text-align:center">Snappy</td><td style="text-align:center"><strong><em>否</em></strong></td><td style="text-align:center">Snappy</td><td style="text-align:center">.snappy</td><td style="text-align:center">否</td><td style="text-align:center">否</td><td>SnappyCodec</td></tr></tbody></table><blockquote><p>性能比较</p></blockquote><table><thead><tr><th style="text-align:center">压缩算法</th><th style="text-align:center">原始文件大小</th><th style="text-align:center">压缩文件大小</th><th style="text-align:center">压缩速度</th><th style="text-align:center">解压速度</th></tr></thead><tbody><tr><td style="text-align:center">gzip</td><td style="text-align:center">8.3G</td><td style="text-align:center">1.8G</td><td style="text-align:center">17.5M/S</td><td style="text-align:center">58M/S</td></tr><tr><td style="text-align:center">bzip2</td><td style="text-align:center">8.3G</td><td style="text-align:center">1.1G</td><td style="text-align:center">2.4M/S</td><td style="text-align:center">9.5M/S</td></tr><tr><td style="text-align:center">LZO</td><td style="text-align:center">8.3G</td><td style="text-align:center">2.9G</td><td style="text-align:center">49.3M/S</td><td style="text-align:center">74.6M/S</td></tr></tbody></table><p>Snappy 在单核 i7 64 位处理器上，压缩速度可以达到 250M/S 以上，解压速度可以达到 500M/S 以上；但是压缩完的文件大小还是很大，且不可以切分。</p><h2 id="压缩方式对比"><a href="#压缩方式对比" class="headerlink" title="压缩方式对比"></a>压缩方式对比</h2><h3 id="GZip-压缩"><a href="#GZip-压缩" class="headerlink" title="GZip 压缩"></a>GZip 压缩</h3><blockquote><p>优点：</p></blockquote><ol><li>压缩率较高，压缩、解压速度较快</li><li>hadoop 本身支持，在应用中处理 gzip 文件和直接处理文本一样</li><li>大部分 linux 系统都自带 gzip 命令，使用方便</li></ol><blockquote><p>缺点：</p></blockquote><p>不支持切分</p><blockquote><p>应用场景</p></blockquote><p>当每个文件压缩后，文件大小在 130M 以内（即一个块大小的 1.1 倍以内），都可以考虑用 gzip 压缩。</p><h3 id="BZip2"><a href="#BZip2" class="headerlink" title="BZip2"></a>BZip2</h3><blockquote><p>优点</p></blockquote><ol><li>支持切分</li><li>压缩率高</li><li>hadoop 自带，使用方便</li></ol><blockquote><p>缺点</p></blockquote><p>压缩、解压速度慢</p><blockquote><p>应用场景</p></blockquote><ol><li>对速度要求不高，但是要求较高压缩率；</li><li>输出后的数据比较大，处理后的速度需要压缩存档减少磁盘空间并且以后数据用的比较少</li><li>对单个很大的文本文件向压缩以较少存储空间，同时需要支持切分，而且兼容之前的应用程序</li></ol><h3 id="LZO"><a href="#LZO" class="headerlink" title="LZO"></a>LZO</h3><blockquote><p>优点</p></blockquote><ol><li>压缩、解压速度快</li><li>压缩率合理</li><li>支持切分，hadoop 中最流行的压缩格式</li><li>可以在 linux 中安装 lzop 命令，使用方便</li></ol><blockquote><p>缺点</p></blockquote><ol><li>压缩率比 gzip 低</li><li>hadoop 本身不支持，需要安装</li><li>在应用中需要对 lzo 格式的文件做特殊处理（建索引、指定文件格式）</li></ol><blockquote><p>应用场景</p></blockquote><p>一个很大的文本文件，压缩后还大于 200M 以上的可以考虑。单个文件越大，lzo 优点越明显</p><h3 id="Snappy"><a href="#Snappy" class="headerlink" title="Snappy"></a>Snappy</h3><blockquote><p>优点</p></blockquote><ol><li>高速压缩、解压</li><li>压缩率合理 </li></ol><blockquote><p>缺点</p></blockquote><ol><li>不支持切分</li><li>压缩率比 gzip 低</li><li>hadoop 本身不支持，需要安装</li></ol><blockquote><p>应用场景</p></blockquote><ol><li>当 MapReduce 的 Map 输出数据比较大，作为 Map 到 Reduce 的中间数据压缩格式</li><li>作为 MapReduce 作业的输出和另外一个 MapReduce 作业的输入</li></ol><h2 id="压缩位置的选择"><a href="#压缩位置的选择" class="headerlink" title="压缩位置的选择"></a>压缩位置的选择</h2><blockquote><p>Map 输入之前</p></blockquote><p>在有大量数据并计划重复处理的情况下，应该考虑对输入进行压缩。此时无需指定使用的压缩方式，hadoop 可以检查文件的扩展名，如果扩展名能够匹配，就会使用恰当的编解码方式对文件进行压缩，否则，hadoop 不会使用压缩</p><blockquote><p>Map 处理之后、Reduce 之前</p></blockquote><p>当 Mapper 任务输出的中间数据量很大时，应考虑再此阶段采用压缩技术，能够显著改善内部数据的 shuffle 过程；而 shuffle 过程在 hadoop 处理过程中是消耗资源最多的环节。<br>如果发现数据量大造成网络传输缓慢，应该考虑使用压缩技术，可以用于 Mapper 输出压缩的格式为：LZO、Snappy。</p><p>需要注意：<br>LZO 是提供 hadoop 压缩数据的通用压缩编解码器。设计目的是达到与硬盘读写速度相当的压缩速度，因此速度是优先考虑的因素，而不是压缩率。<br>与 gzip 编解码器相比，它的压缩速度是 gzip 的 5 倍，解压速度是 gzip 的 2 倍。<br>同一个文件用 LZO 压缩后，比用 gzip 压缩后大 50%，但是比压缩前小 25%~50%，这对于改善性能非常有利，Map 阶段完成时间快大概 4 倍。</p><blockquote><p>Reducer 输出</p></blockquote><p>此阶段启用压缩，能够减少要存储的数据量，因此降低所需的磁盘空间。<br>当 MapReduce 作业形成链条时，第二个作业的输入也以压缩，所以启用压缩同样有效。</p><h2 id="压缩参数的配置"><a href="#压缩参数的配置" class="headerlink" title="压缩参数的配置"></a>压缩参数的配置</h2><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">默认值</th><th style="text-align:center">阶段</th><th style="text-align:center">备注</th></tr></thead><tbody><tr><td style="text-align:center"><code>io.compression.codecs</code>（core-site.xml）</td><td style="text-align:center">DefaultCodec、GzipCodec、BZipCodec</td><td style="text-align:center">输入压缩</td><td style="text-align:center">hadoop 使用文件扩展名判断是否支持某种编解码器</td></tr><tr><td style="text-align:center"><code>mapreduce.map.output.compress</code>（mapred-site.xml）</td><td style="text-align:center">false</td><td style="text-align:center">mapper 输出</td><td style="text-align:center">为 true 时启用压缩</td></tr><tr><td style="text-align:center"><code>mapreduce.map.output.compress.codec</code>（mapred-site.xml）</td><td style="text-align:center">DefaultCodec</td><td style="text-align:center">mapper 输出</td><td style="text-align:center">多用 LZO 或 Snappy</td></tr><tr><td style="text-align:center"><code>mapreduce.output.fileoutputformat.compress</code>（mapred-site.xml）</td><td style="text-align:center">false</td><td style="text-align:center">reduce 输出</td><td style="text-align:center">为 true 时启用压缩</td></tr><tr><td style="text-align:center"><code>mapreduce.output.fileoutputformat.compress.type</code>（mapred-site.xml）</td><td style="text-align:center">RECORD</td><td style="text-align:center">reduce 输出</td><td style="text-align:center">SequenceFile 输出使用的压缩类型（BLOCK、NONE、RECORD）</td></tr><tr><td style="text-align:center"><code>mapreduce.output.fileoutputformat.compress.codec</code>（mapred-site.xml）</td><td style="text-align:center">DefaultCodec</td><td style="text-align:center">reduce 输出</td><td style="text-align:center">具体的压缩编码</td></tr></tbody></table><h2 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h2><p>CompressionCodec 有两个方法可以用于轻松的压缩、解压数据。</p><p>想要对正在被写入一个输出流的数据进行压缩，可以使用 <code>createOutputStream</code> 方法创建一个 CompressionOutputStream，将其以压缩格式写入底层的流。<br>想要对输入数据进行嘉业，可以使用 <code>createInputStream</code> 方法床架一个 CompressionInputStream，从而从底层的流读取未压缩的数据</p><h3 id="压缩测试"><a href="#压缩测试" class="headerlink" title="压缩测试"></a>压缩测试</h3><p>输入数据：使用之前 <a href="/hadoop/map-reduce/input-format/hadoop-12.html#案例实操">CombinerTextInputFormat 示例</a>  中的 <a href="/file/hadoop/input-format/d.txt">输入数据 d.txt</a> 作为压缩测试的输入数据，测试不同压缩方式下的压缩率、耗时等。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    String filePath = <span class="string">"d:/dev/wordcount/d.txt"</span>;</span><br><span class="line"><span class="comment">//        String type = "org.apache.hadoop.io.compress.BZip2Codec";</span></span><br><span class="line"><span class="comment">//        String type = "org.apache.hadoop.io.compress.DefaultCodec";</span></span><br><span class="line">    String type = <span class="string">"org.apache.hadoop.io.compress.GzipCodec"</span>;</span><br><span class="line">    compress(filePath, type);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 测试压缩</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@param</span> path 待压缩文件路径</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@param</span> type 压缩方式</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">(String path, String type)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    <span class="comment">// 1. 获取输入流</span></span><br><span class="line">    FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(path));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 设置压缩方式</span></span><br><span class="line">    Class&lt;?&gt; compressClass = Class.forName(type);</span><br><span class="line">    <span class="comment">// 3. 反射获取编解码实例</span></span><br><span class="line">    CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(compressClass, <span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 获取输出流，通过编解码实例获取后缀</span></span><br><span class="line">    FileOutputStream outputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(path + codec.getDefaultExtension()));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5.创建压缩输出流</span></span><br><span class="line">    CompressionOutputStream codecOutputStream = codec.createOutputStream(outputStream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6. 流的对拷</span></span><br><span class="line">    IOUtils.copyBytes(inputStream, codecOutputStream, <span class="number">1024</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7. 关闭资源</span></span><br><span class="line">    IOUtils.closeStream(codecOutputStream);</span><br><span class="line">    IOUtils.closeStream(outputStream);</span><br><span class="line">    IOUtils.closeStream(inputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>测试结果</p></blockquote><p>可见压缩前后数据大小的区别非常明显<br><img src="/images/hadoop/map-reduce/compress.png" alt="压缩结果"></p><h3 id="解压测试"><a href="#解压测试" class="headerlink" title="解压测试"></a>解压测试</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"><span class="comment">//        String path = "d:/dev/wordcount/d.txt.bz2";</span></span><br><span class="line"><span class="comment">//        String path = "d:/dev/wordcount/d.txt.gz";</span></span><br><span class="line">    String path = <span class="string">"d:/dev/wordcount/d.txt.deflate"</span>;</span><br><span class="line">    decompress(path);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">decompress</span><span class="params">(String path)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    <span class="comment">// 1. 校验文件是否可以解压</span></span><br><span class="line">    CompressionCodecFactory factory = <span class="keyword">new</span> CompressionCodecFactory(<span class="keyword">new</span> Configuration());</span><br><span class="line">    CompressionCodec codec = factory.getCodec(<span class="keyword">new</span> Path(path));</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> == codec)&#123;</span><br><span class="line">        System.out.println(<span class="string">"不支持压缩"</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 获取输入流</span></span><br><span class="line">    FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(path);</span><br><span class="line">    CompressionInputStream codecInputStream = codec.createInputStream(inputStream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 获取输出流</span></span><br><span class="line">    FileOutputStream fileOutputStream = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(path + <span class="string">".txt"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 流对拷</span></span><br><span class="line">    IOUtils.copyBytes(codecInputStream, fileOutputStream, <span class="number">1024</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 关闭流</span></span><br><span class="line">    IOUtils.closeStream(fileOutputStream);</span><br><span class="line">    IOUtils.closeStream(codecInputStream);</span><br><span class="line">    IOUtils.closeStream(inputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>测试结果</p></blockquote><p><img src="/images/hadoop/map-reduce/decompress.png" alt="解压测试"></p><h3 id="Map-输出端采用压缩"><a href="#Map-输出端采用压缩" class="headerlink" title="Map 输出端采用压缩"></a>Map 输出端采用压缩</h3><p>即是 MapReduce 的输入、输出文件都是未压缩的文件，仍然可以对 Map 任务的中间结果输出做压缩（原因是 Map 结束后要将文件写入磁盘，并通过网络传输到 Reduce 节点，对其压缩可以提高很多性能）</p><p>Hadoop 源码支持的压缩格式有：<code>BZip2Codec</code>、<code>DefaultCodec</code></p><blockquote><p>基于 <a href="/hadoop/map-reduce/input-format/hadoop-12.html#案例实操">CombinerTextInputFormat 示例</a> 进行操作。</p></blockquote><p>首先执行 wordcount 实例，查看控制台 MapReduce 过程的字节数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Map-Reduce Framework</span><br><span class="line">    Map input records=3792092</span><br><span class="line">    Map output records=5056134</span><br><span class="line">    Map output bytes=50561388</span><br><span class="line">    Map output materialized bytes=60673704</span><br><span class="line">    Input split bytes=778</span><br><span class="line">    Combine input records=0</span><br><span class="line">    Combine output records=0</span><br><span class="line">    Reduce input groups=10</span><br><span class="line">    Reduce shuffle bytes=60673704</span><br><span class="line">    Reduce input records=5056134</span><br><span class="line">    Reduce output records=10</span><br><span class="line">    Spilled Records=10112268</span><br><span class="line">    Shuffled Maps =8</span><br><span class="line">    Failed Shuffles=0</span><br><span class="line">    Merged Map outputs=8</span><br><span class="line">    GC time elapsed (ms)=259</span><br><span class="line">    Total committed heap usage (bytes)=8830582784</span><br><span class="line"></span><br><span class="line">压缩前总耗时：9090 ms</span><br></pre></td></tr></table></figure><p>修改 WordCountDriver，增加压缩配置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启压缩</span></span><br><span class="line">configuration.set(<span class="string">"mapreduce.map.output.compress"</span>, <span class="string">"true"</span>);</span><br><span class="line"><span class="comment">// 指定压缩格式</span></span><br><span class="line">configuration.setClass(<span class="string">"mapreduce.map.output.compress.codec"</span>, BZip2Codec.class, CompressionCodec.class);</span><br></pre></td></tr></table></figure><p>再次运行测试，查看控制台 MapReduce 过程字节数（开启压缩后的程序会比开启压缩前要慢很多）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Map-Reduce Framework</span><br><span class="line">    Map input records=3792092</span><br><span class="line">    Map output records=5056134</span><br><span class="line">    Map output bytes=50561388</span><br><span class="line">    Map output materialized bytes=207808</span><br><span class="line">    Input split bytes=778</span><br><span class="line">    Combine input records=0</span><br><span class="line">    Combine output records=0</span><br><span class="line">    Reduce input groups=10</span><br><span class="line">    Reduce shuffle bytes=207808</span><br><span class="line">    Reduce input records=5056134</span><br><span class="line">    Reduce output records=10</span><br><span class="line">    Spilled Records=10112268</span><br><span class="line">    Shuffled Maps =8</span><br><span class="line">    Failed Shuffles=0</span><br><span class="line">    Merged Map outputs=8</span><br><span class="line">    GC time elapsed (ms)=286</span><br><span class="line">    Total committed heap usage (bytes)=7362576384</span><br><span class="line"></span><br><span class="line">压缩总耗时：312134 ms</span><br></pre></td></tr></table></figure><p><img src="/images/hadoop/map-reduce/map-compress.png" alt="压缩前后字节数对比"></p><h3 id="Reduce-输出压缩"><a href="#Reduce-输出压缩" class="headerlink" title="Reduce 输出压缩"></a>Reduce 输出压缩</h3><p>还是以上例为例，将 map 压缩注释掉（为了快速测试 reduce 压缩，省去 map 压缩的等待时间），增加如下配置，开启 reduce 压缩<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启 reduce 压缩</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 采用 bzip 压缩</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span><br><span class="line"><span class="comment">// 采用gzip压缩</span></span><br><span class="line"><span class="comment">//FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);</span></span><br><span class="line"><span class="comment">// 采用默认压缩</span></span><br><span class="line"><span class="comment">//FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);</span></span><br></pre></td></tr></table></figure></p><p>测试三种压缩方式的执行耗时，由于文件过小，实际总耗时相差无几。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reduce bz2 总耗时：9132</span><br><span class="line">reduce gz 总耗时：9166</span><br><span class="line">reduce default 总耗时：9038</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop 为每个作业维护若干个内置计数器，以描述多项指标。&lt;br&gt;如：记录已处理的字节数和记录数，使用户可以监控已处理的输入数据量和已产生的输出数据量&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（16） Map Reduce &lt;BR /&gt; ReduceJoin、MapJoin</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/map-join/hadoop-16.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/map-join/hadoop-16.html</id>
    <published>2019-12-13T01:43:17.000Z</published>
    <updated>2019-12-13T01:43:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>ReduceJoin 的工作：</p><p>Map 端的主要工作：为来自不同表或者文件的 KV 对，打标签以区别不同来源的记录，然后用连接字段作为 key，其余部分和新加的标志位作为 value，最后进行输出。<br>Reduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，只需要在每个分组中，将那么来源于不同文件的记录分开，最后完成合并即可。</p><a id="more"></a><h1 id="ReduceJoin"><a href="#ReduceJoin" class="headerlink" title="ReduceJoin"></a>ReduceJoin</h1><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>需求：输入数据为两个表：<a href="/file/hadoop/join/order.txt">订单</a>、<a href="/file/hadoop/join/pd.txt">商品信息</a>，将商品信息中的数据，根据商品的 pid，合并到订单数据中。</p><blockquote><p>TableBean</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 订单 id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String id;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 产品 id</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String pid;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 数量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> amount;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 产品名称</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String pname;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 标记位</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> String flag;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(id);</span><br><span class="line">        dataOutput.writeUTF(pid);</span><br><span class="line">        dataOutput.writeInt(amount);</span><br><span class="line">        dataOutput.writeUTF(pname);</span><br><span class="line">        dataOutput.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        id = dataInput.readUTF();</span><br><span class="line">        pid = dataInput.readUTF();</span><br><span class="line">        amount = dataInput.readInt();</span><br><span class="line">        pname = dataInput.readUTF();</span><br><span class="line">        flag = dataInput.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id + <span class="string">"\t"</span> + pname + <span class="string">"\t"</span> + amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略 get、set</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">TableBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> TableBean table = <span class="keyword">new</span> TableBean();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String fileName;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 在 map 执行前，获取文件名称，来判断当前处理的是哪个文件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        FileSplit inputSplit = ((FileSplit) context.getInputSplit());</span><br><span class="line">        fileName = inputSplit.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="keyword">if</span> (fileName.contains(<span class="string">"order"</span>)) &#123;</span><br><span class="line">            <span class="comment">// order.txt</span></span><br><span class="line">            table.setId(fields[<span class="number">0</span>]);</span><br><span class="line">            table.setPid(fields[<span class="number">1</span>]);</span><br><span class="line">            table.setAmount(Integer.parseInt(fields[<span class="number">2</span>]));</span><br><span class="line">            table.setFlag(<span class="string">"order"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// pd.txt</span></span><br><span class="line">            table.setFlag(<span class="string">"pd"</span>);</span><br><span class="line">            table.setPid(fields[<span class="number">0</span>]);</span><br><span class="line">            table.setPname(fields[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        outKey.set(table.getPid());</span><br><span class="line"></span><br><span class="line">        context.write(outKey, table);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">TableBean</span>, <span class="title">TableBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        List&lt;TableBean&gt; tableBeans = Lists.newArrayList();</span><br><span class="line">        <span class="comment">// 注意！！！！！</span></span><br><span class="line">        <span class="comment">// 一定要遍历中重新创建对象，拷贝数据到新创建的对象中，并把新创建的对象放入一个新的 list 中！！</span></span><br><span class="line">        <span class="comment">// 否则 tableBeans 里面的对象都是同一个对象！！！</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean value : values) &#123;</span><br><span class="line">            TableBean tableBean = <span class="keyword">new</span> TableBean();</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// 示例使用的是 commons 包的对象拷贝，阿里规范禁止使用，如果在 spring 项目中使用 hadoop，尽量使用 spring util 的对象拷贝</span></span><br><span class="line">                BeanUtils.copyProperties(tableBean, value);</span><br><span class="line">                tableBeans.add(tableBean);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IllegalAccessException | InvocationTargetException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 获取 orders</span></span><br><span class="line">        List&lt;TableBean&gt; orders = tableBeans.stream().filter(tableBean -&gt; tableBean.getFlag().equals(<span class="string">"order"</span>))</span><br><span class="line">                .collect(Collectors.toList());</span><br><span class="line">        <span class="comment">// 2. 获取商品，并转换为 map，key 为商品id，value 为商品名称</span></span><br><span class="line">        Map&lt;String, String&gt; pd = tableBeans.stream().filter(tableBean -&gt; tableBean.getFlag().equals(<span class="string">"pd"</span>))</span><br><span class="line">                .collect(Collectors.toMap(TableBean::getPid, TableBean::getPname));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 遍历 orders，赋值 pname，并输出</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (TableBean order : orders) &#123;</span><br><span class="line">            order.setPname(pd.get(order.getPid()));</span><br><span class="line"></span><br><span class="line">            context.write(order, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>省略 Driver，查看运行结果</p></blockquote><p><img src="/images/hadoop/map-reduce/reduce-join.png" alt="reduce join "></p><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>合并的操作在 Reduce 阶段完成，Reduce 端的处理压力太大，Map 节点的运算负载很低，资源利用率不高，而且在 Reduce 阶段极易产生数据倾斜。 推荐使用 MapJoin</p><hr><h1 id="MapJoin"><a href="#MapJoin" class="headerlink" title="MapJoin"></a>MapJoin</h1><p>适用场景： 一个张表十分小，一个表十分大</p><blockquote><p>优点</p></blockquote><p>在 Map 端缓存多张表，提前处理业务逻辑，增加了 Map 业务，减少 Reduce 数据压力，尽可能减少数据倾斜</p><blockquote><p>方法</p></blockquote><p>1、在 Mapper 的 setup 阶段，将文件读取到缓存集合中<br>2、在驱动函数中加载缓存</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>依然使用上例中的输入数据，输入结果也应该与上例一致。</p><blockquote><p>Driver</p></blockquote><p><strong><em>由于 MapJoin 不需要 Reduce 端，此时可以将 Driver 中的 MapperKeyClass、MapperValueClass、ReduceClass 去掉</em></strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取 job 对象</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">    Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置 jar 存放路径</span></span><br><span class="line">    job.setJarByClass(MapJoinDriver.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关联 Mapper、Reducer 业务类</span></span><br><span class="line">    job.setMapperClass(MapJoinMapper.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定最终输出的数据 KV 类型</span></span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 job 的输入文件所在目录</span></span><br><span class="line">    <span class="comment">// 只读入 order</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"D:\\dev\\join\\order.txt"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 job 的输出结果所在目录</span></span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"D:\\dev\\join\\output1"</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载缓存数据</span></span><br><span class="line">    job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">"file:///d:/dev/join/pd.txt"</span>));</span><br><span class="line">    <span class="comment">// MapJoin 的逻辑不需要 Reduce，设置 ReduceTask 为 0</span></span><br><span class="line">    job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提交 job</span></span><br><span class="line">    <span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    System.exit(succeed ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, String&gt; fieldMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String path = context.getCacheFiles()[<span class="number">0</span>].getPath();</span><br><span class="line">        BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(path), StandardCharsets.UTF_8));</span><br><span class="line">        String line;</span><br><span class="line">        <span class="keyword">while</span> (StringUtils.isNotBlank(line = reader.readLine()))&#123;</span><br><span class="line">            String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">            fieldMap.put(fields[<span class="number">0</span>], fields[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        IOUtils.closeStream(reader);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">        String pid = fields[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        String pname = fieldMap.get(pid);</span><br><span class="line"></span><br><span class="line">        outKey.set(fields[<span class="number">0</span>] + <span class="string">"\t"</span> + pname + <span class="string">"\t"</span> + fields[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">        context.write(outKey, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>查看结果</p></blockquote><p><img src="/images/hadoop/map-reduce/map-join.png" alt="MapJoin"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ReduceJoin 的工作：&lt;/p&gt;
&lt;p&gt;Map 端的主要工作：为来自不同表或者文件的 KV 对，打标签以区别不同来源的记录，然后用连接字段作为 key，其余部分和新加的标志位作为 value，最后进行输出。&lt;br&gt;Reduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，只需要在每个分组中，将那么来源于不同文件的记录分开，最后完成合并即可。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
      <category term="map-join" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/map-join/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
      <category term="map-join" scheme="https://www.laiyy.top/tags/map-join/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（15） Map Reduce &lt;BR /&gt; 工作流程、OutputFormat</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/hadoop-15.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/hadoop-15.html</id>
    <published>2019-12-10T06:32:03.000Z</published>
    <updated>2019-12-10T06:32:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>MapTask 流程分为：Read 阶段、Map 阶段、Collect 阶段、溢写阶段、Combine 阶段<br>ReduceTask 流程分为：Copy 阶段、Merge 阶段、Sort 阶段、Reduce 阶段</p><a id="more"></a><h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><h2 id="MapTask-流程"><a href="#MapTask-流程" class="headerlink" title="MapTask 流程"></a>MapTask 流程</h2><blockquote><p>Read 阶段</p></blockquote><p>客户端获取待处理文本，提交之前，获取待处理数据的信息，然后根据参数设置，形成一个任务分配的规划；<br>提交信息（Job.split、jar、Job.xml）；<br>计算出 MapTask 数量、开启 MapTask，TextInputFormat 开始读取数据</p><blockquote><p>Map 阶段</p></blockquote><p>读取后，返回对应的 KV 数据，并把数据写入到 Mapper 中</p><blockquote><p>Collect 阶段</p></blockquote><p>Mapper 将数据写入环形缓冲区，并进行分区、排序</p><blockquote><p>溢写阶段</p></blockquote><p>将分区、排序后的数据，溢写到文件（分区且区内有序）</p><blockquote><p>Combine 阶段</p></blockquote><p>将溢写后的数据进行归并排序</p><p><img src="/images/hadoop/shuffle/map-task.png" alt="Map Task"></p><hr><h2 id="ReduceTask-流程"><a href="#ReduceTask-流程" class="headerlink" title="ReduceTask 流程"></a>ReduceTask 流程</h2><blockquote><p>Copy 阶段</p></blockquote><p>将 MapTask 执行结束后，将对应分区的数据，拷贝到 ReduceTask 中</p><blockquote><p>Merge 阶段</p></blockquote><p>将数据进行归并排序</p><blockquote><p>Sort 阶段</p></blockquote><p>合并 ReduceTask 中的文件，进行归并排序</p><blockquote><p>Reduce 阶段</p></blockquote><p>进行数据处理逻辑，通过 TextOutputFormat 输出到指定位置</p><p><img src="/images/hadoop/shuffle/reduce-task.png" alt="reduce-task"></p><h3 id="ReduceTask-数量"><a href="#ReduceTask-数量" class="headerlink" title="ReduceTask 数量"></a>ReduceTask 数量</h3><p>ReduceTask 的并行度，影响着整个 Job 的并发度和执行效率，但是与 <code>MapTask 的并发度，由切片数决定</code> 不同，ReduceTask 的数量是可以手动设置的。<code>job.setNumReduceTasks(2);</code></p><p>ReduceTask 的数量设置，需要根据集群性能去测试调节，并不是一成不变的。</p><blockquote><p>注意事项</p></blockquote><ol><li>ReduceTask=0，表示没有 Reduce 阶段，输出文件的个数和 Map 个数一致。</li><li>ReduceTask=1，输出文件个数为 1 个（默认）</li><li>如果数据分布不均匀，就有可能在 Reduce 阶段产生数据倾斜</li><li>ReduceTask 数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有一个 ReduceTask</li><li>具体需要多少个 ReduceTask，需要根据集群性能而定</li><li>如果分数区不是 1，但是 ReduceTask 为 1，不会执行分区过程！理由：在 MapTask 源码中，执行分区的前提，是判断 ReduceNum 格式是否大于 1，不大于 1 不执行分区</li></ol><hr><h2 id="Shuffle-流程"><a href="#Shuffle-流程" class="headerlink" title="Shuffle 流程"></a>Shuffle 流程</h2><p>Shuffle 运行在 Map 方法之后，Reduce 方法之前；部分与 Map、Reduce 重合。</p><h3 id="Map-阶段"><a href="#Map-阶段" class="headerlink" title="Map 阶段"></a>Map 阶段</h3><p>Map 接收之后，数据写入环形缓冲区，对数据进行分区、排序，数据量到达缓冲区 80% 后，开始溢写数据（可选 Combiner 合并）；<br>溢写到磁盘，生成 2 个文件：spill.index、spill.out；<br>对数据进行归并排序（可选 Combiner 合并）、数据压缩；<br>将文件写入磁盘，分区输出</p><h3 id="Reduce-阶段"><a href="#Reduce-阶段" class="headerlink" title="Reduce 阶段"></a>Reduce 阶段</h3><p>从 MapTask 拷贝数据到内存缓冲区（内存不够时溢写到磁盘）；<br>对每个 Map 来的数据进行归并排序；<br>按照相同的 key 进行分组；<br>执行 reduce 方法</p><p><img src="/images/hadoop/shuffle/shuffle-1.png" alt="shuffle 流程"></p><hr><h1 id="OutputFormat"><a href="#OutputFormat" class="headerlink" title="OutputFormat"></a>OutputFormat</h1><p>OutputFormat 是 MapReduce 输出的基类，所有实现了 MapReduce 输出都实现了 OutputFormat 接口。常见的几个实现类：</p><blockquote><p>TextOutputFormat</p></blockquote><p>默认输出格式，把每条记录斜纹文本行。KV 可以是任意类型，toString 方法可以把它们转为字符串</p><blockquote><p>SequenceFileOutputFormat</p></blockquote><p>可以将将 SequenceFileOutputFormat 的输出，作为后续 MapReduce 任务的输入；格式紧凑，容易压缩</p><blockquote><p>自定义 OutputFormat</p></blockquote><p>自定义实现输出，可定制</p><h2 id="自定义实现"><a href="#自定义实现" class="headerlink" title="自定义实现"></a>自定义实现</h2><p>需求，根据 <a href="/file/hadoop/output-format/log.txt">日志文件</a>，将包含 <code>baidu.com</code> 的日志输出到 <code>baidu.log</code>，其他的输出到 <code>other.log</code></p><blockquote><p>Mapper、Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(key, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>RecordeWriter、OutputFormat</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> LogRecordWriter(job);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FSDataOutputStream baidu, other;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LogRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            FileSystem fileSystem = FileSystem.get(job.getConfiguration());</span><br><span class="line">            baidu = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"d:/dev/opf/baidu.log"</span>));</span><br><span class="line">            other = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"d:/dev/opf/other.log"</span>));</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 获取域名</span></span><br><span class="line">        String domain = key.toString();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (domain.contains(<span class="string">"baidu.com"</span>))&#123;</span><br><span class="line">            baidu.write((domain+<span class="string">"\n"</span>).getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            other.write((domain+<span class="string">"\n"</span>).getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        IOUtils.closeStream(baidu);</span><br><span class="line">        IOUtils.closeStream(other);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Driver</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setOutputFormatClass(CustomerOutputFormat.class);</span><br></pre></td></tr></table></figure><blockquote><p>测试结果</p></blockquote><p><img src="/images/hadoop/shuffle/opf-result.png" alt="自定义outputformat 结果"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MapTask 流程分为：Read 阶段、Map 阶段、Collect 阶段、溢写阶段、Combine 阶段&lt;br&gt;ReduceTask 流程分为：Copy 阶段、Merge 阶段、Sort 阶段、Reduce 阶段&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（14） Map Reduce &lt;BR /&gt; 排序、合并</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/sort/hadoop-14.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/sort/hadoop-14.html</id>
    <published>2019-12-09T08:02:42.000Z</published>
    <updated>2019-12-09T08:02:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>排序，是 MapReduce 中最重要的操作之一。默认的排序方式是 <code>字典排序</code>，且实现此排序的方式是 <code>快速排序</code>。</p><p>MapTask 和 MapReduce 均会对数据按照 <code>key</code> 进行排序，该操作属于 Hadoop 的默认操作。任何程序中的数据均会被排序，不论逻辑上是否需要。</p><a id="more"></a><h1 id="排序概述"><a href="#排序概述" class="headerlink" title="排序概述"></a>排序概述</h1><blockquote><p>对于 MapTask</p></blockquote><p>它会将处理的结果暂时放到 <code>环形缓冲区</code> 中，当缓冲区的使用率达到一定的阈值之后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上的文件进行 <code>归并排序</code>。</p><blockquote><p>对于 ReduceTask</p></blockquote><p>它从 <code>每个 MapTask</code> 上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写到磁盘上，否则存储到内存。<br>如果磁盘上的文件数据达到一定阈值，则进行一次 <code>归并排序</code>，以生成一个更大的文件。<br>如果内存中文件大小或数目，达到一定阈值，则进行一次 <code>合并</code>，将数据溢写到磁盘上。<br>当所有数据拷贝完成后，ReduceTask 统一对内存和磁盘上的数据进行一次 <code>归并排序</code>。</p><h1 id="排序的分类"><a href="#排序的分类" class="headerlink" title="排序的分类"></a>排序的分类</h1><blockquote><p>部分排序</p></blockquote><p>MapReduce 根据输入记录的键，对数据集排序，保证输出的每个文件，内部有序</p><blockquote><p>全排序</p></blockquote><p>最终输出结果只有一个文件，且文件内部有序。实现方式是：只设置一个 ReduceTask。但是该犯法在处理大型文件是，效率极低，完全丧失了 MapReduce 所提供的并行架构。</p><blockquote><p>辅助排序（GroupingComparator 分组）</p></blockquote><p>在 Reduce 端对 key 进行分组。应用于：在接收的 key 为 bean 对象时，想让一个或几个字段相同（并不是全部字段相同）的 key 进入到同一个 Reduce 方法时，可以使用分组排序</p><blockquote><p>二次排序</p></blockquote><p>在自定义排序过程中，如果 compareTo 中的条件为两个，即为二次排序。</p><h2 id="自定义排序（全排序）"><a href="#自定义排序（全排序）" class="headerlink" title="自定义排序（全排序）"></a>自定义排序（全排序）</h2><p>bean 对象作为 key 传输，需要实现 WritableComparable 接口，重写 compareTo 方法。</p><p>需求：使用之前 <a href="/hadoop/map-reduce/hadoop-10.html#序列化-Demo">序列化实例</a> 的结果作为 <a href="/file/hadoop/shuffle/sort.txt">输入数据</a>，期望输出：按照总流量倒序排序。</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><blockquote><p>Bean</p></blockquote><p><a href="/hadoop/map-reduce/hadoop-10.html#序列化-Demo">序列化实例</a> 中的 FlowBean 类，实现 <code>WritableComparable</code> 接口，重写 <code>compareTo</code> 方法<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean flowBean)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 比较</span></span><br><span class="line">    <span class="keyword">if</span> (sumFlow &gt; flowBean.getSumFlow())&#123;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (sumFlow &lt; flowBean.getSumFlow())&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    String line = value.toString();</span><br><span class="line"></span><br><span class="line">    String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line"></span><br><span class="line">    String phone = fields[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> upFlow = Long.parseLong(fields[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">long</span> downFlow = Long.parseLong(fields[<span class="number">2</span>]);</span><br><span class="line">    flowBean.set(upFlow, downFlow);</span><br><span class="line"></span><br><span class="line">    Text text = <span class="keyword">new</span> Text();</span><br><span class="line">    text.set(phone);</span><br><span class="line">    context.write(flowBean, text);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">        context.write(value, key);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Driver 省略，测试结果</p></blockquote><p><img src="/images/hadoop/shuffle/sort.png" alt="排序结果"></p><h2 id="区内排序"><a href="#区内排序" class="headerlink" title="区内排序"></a>区内排序</h2><p>需求：使用 <a href="/file/hadoop/shuffle/sort_1.txt">输入数据</a>，期望输出：根据手机号的前三位不同，分成不同的文件，并在每个文件中，按照总流量倒序输出。</p><p>在上一例全排序的基础上，增加一个分区操作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBeanPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(FlowBean flowBean, Text text, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> partitioner = <span class="number">4</span>;</span><br><span class="line">        String phonePrefix = text.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"134"</span>.equals(phonePrefix))&#123;</span><br><span class="line">            partitioner = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"135"</span>.equals(phonePrefix))&#123;</span><br><span class="line">            partitioner = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"136"</span>.equals(phonePrefix))&#123;</span><br><span class="line">            partitioner = <span class="number">2</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">"137"</span>.equals(phonePrefix))&#123;</span><br><span class="line">            partitioner = <span class="number">3</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> partitioner;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>查看结果</p></blockquote><p><img src="/images/hadoop/shuffle/shuffle-sort.png" alt="分区排序"></p><hr><h1 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h1><blockquote><p>Combiner 是 MapReduce 程序中 Maper 和 Reducer 之外的一种组件<br>Combiner 是父类是 Reducer<br>Combiner 与 Reducer 的区别在于运行的位置：Combiner 是在每个 MapTask 所在的节点运行；Reducer 是接收全局所有 Mapper 的输出结果<br>Combiner 的意义是对每个 MapTask 的数据进行局部汇总，以减小网络的传输量<br>应用前提：不能影响最终的业务逻辑（Combiner 输出的 KV，应该与 Reducer 的输入 KV 类型对应）</p></blockquote><h2 id="自定义-Combiner"><a href="#自定义-Combiner" class="headerlink" title="自定义 Combiner"></a>自定义 Combiner</h2><p>需求： 使用 <a href="/file/hadoop/combiner/combiner.txt">输入数据</a>，进行 <code>局部汇总</code>，以减小网络传输量。</p><p>期望输出：分隔单词，局部汇总每个单词的数量</p><p><strong><em>以 WordCount 实例为例</em></strong></p><p>原始的 WordCount 的控制台输出为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Map-Reduce Framework</span><br><span class="line">    Map input records=8</span><br><span class="line">    Map output records=18</span><br><span class="line">    Map output bytes=168</span><br><span class="line">    Map output materialized bytes=210</span><br><span class="line">    Input split bytes=100</span><br><span class="line">    Combine input records=0</span><br><span class="line">    Combine output records=0</span><br><span class="line">    Reduce input groups=7</span><br><span class="line">    Reduce shuffle bytes=210</span><br><span class="line">    Reduce input records=18</span><br><span class="line">    Reduce output records=7</span><br><span class="line">    Spilled Records=36</span><br><span class="line">    Shuffled Maps =1</span><br><span class="line">    Failed Shuffles=0</span><br><span class="line">    Merged Map outputs=1</span><br><span class="line">    GC time elapsed (ms)=7</span><br><span class="line">    Total committed heap usage (bytes)=514850816</span><br></pre></td></tr></table></figure></p><blockquote><p>方案 1</p></blockquote><p>自实现一个 Combiner，并在 Driver 中注册。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable();</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 1. 累加求和</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        outValue.set(sum);</span><br><span class="line">        context.write(key, outValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordCountCombiner.class);</span><br></pre></td></tr></table></figure><p>测试运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Map-Reduce Framework</span><br><span class="line">    Map input records=8</span><br><span class="line">    Map output records=18</span><br><span class="line">    Map output bytes=168</span><br><span class="line">    Map output materialized bytes=85  //缩小了</span><br><span class="line">    Input split bytes=100</span><br><span class="line">    Combine input records=18    // 相比之前 Combine 有变化</span><br><span class="line">    Combine output records=7</span><br><span class="line">    Reduce input groups=7</span><br><span class="line">    Reduce shuffle bytes=85      // 缩小了</span><br><span class="line">    Reduce input records=7      // 缩小了</span><br><span class="line">    Reduce output records=7</span><br><span class="line">    Spilled Records=14          // 缩小了</span><br><span class="line">    Shuffled Maps =1</span><br><span class="line">    Failed Shuffles=0</span><br><span class="line">    Merged Map outputs=1</span><br><span class="line">    GC time elapsed (ms)=11      // 缩小了</span><br><span class="line">    Total committed heap usage (bytes)=514850816</span><br></pre></td></tr></table></figure></p><blockquote><p>方案 2</p></blockquote><p>直接将之前的 Reducer 作为 Combiner 即可。<code>job.setCombinerClass(WordCountReducer.class);</code></p><hr><h1 id="辅助排序"><a href="#辅助排序" class="headerlink" title="辅助排序"></a>辅助排序</h1><p>GroupingComparator，对 Reducer 阶段的数据根据某一个或几个字段进行分组。</p><p>分组排序步骤：</p><blockquote><p>自定义排序类，继承 WritableComparator<br>从学 compare 方法<br>创建一个构造，将比较对象传给父类</p></blockquote><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p>根据订单 <a href="/file/hadoop/grouping/grouping.txt">输入数据</a>，进行分组，并找出每笔订单中最贵的商品</p><p>实现步骤：</p><ol><li>利用 “订单 id、成交金额” 作为 key，可以将 Map 阶段读取到的订单数据按照 id 进行排序。如果 id 相同，再根据金额降序，然后发送到 Reducer</li><li>在 Reducer 利用 GroupingComparator 将订单相同的 KV 聚合成组，然后取第一个即可。</li></ol><blockquote><p>创建一个 Bean，用于存储订单信息</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">OrderBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> orderId;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span> price;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(OrderBean order)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 先按照 id 升序，id 相同的按照价格降序</span></span><br><span class="line">        <span class="keyword">if</span> (orderId &gt; order.getOrderId())&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (orderId &lt; order.getOrderId())&#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> Double.compare(order.getPrice(), price);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeInt(orderId);</span><br><span class="line">        dataOutput.writeDouble(price);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        orderId = dataInput.readInt();</span><br><span class="line">        price = dataInput.readDouble();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> OrderBean order = <span class="keyword">new</span> OrderBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String[] fields = value.toString().split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="keyword">int</span> orderId = Integer.parseInt(fields[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">double</span> price = Double.parseDouble(fields[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">        order.setOrderId(orderId);</span><br><span class="line">        order.setPrice(price);</span><br><span class="line"></span><br><span class="line">        context.write(order, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">OrderBean</span>, <span class="title">NullWritable</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(key, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Driver 省略</p></blockquote><blockquote><p>结果</p></blockquote><p><img src="/images/hadoop/shuffle/grouping.png" alt="grouping1"></p><h2 id="开始分组排序"><a href="#开始分组排序" class="headerlink" title="开始分组排序"></a>开始分组排序</h2><p>在得到了结果后，可以看到，现在的结果确实是不同订单的，在一起显示，且是按照倒序排列的。只不过，没有进行分组，没有完成只输出第一条。在此基础上，开始进行辅助分组排序。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个构造，调用父方法的构造，第二个参数必须传为 true</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(OrderBean.class, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注意，一定要重写两个参数均为 WritableComparable 的比较方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">        OrderBean order1 = ((OrderBean) a);</span><br><span class="line">        OrderBean order2 = ((OrderBean) b);</span><br><span class="line">        <span class="keyword">int</span> result;</span><br><span class="line">        <span class="keyword">if</span> (order1.getOrderId() &gt; order2.getOrderId())&#123;</span><br><span class="line">            result = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (order1.getOrderId() &lt; order2.getOrderId())&#123;</span><br><span class="line">            result = -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            result = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// WritableComparator 源码</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">WritableComparator</span><span class="params">(Class&lt;? extends WritableComparable&gt; keyClass, <span class="keyword">boolean</span> createInstances)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(keyClass, (Configuration)<span class="keyword">null</span>, createInstances);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">WritableComparator</span><span class="params">(Class&lt;? extends WritableComparable&gt; keyClass, Configuration conf, <span class="keyword">boolean</span> createInstances)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.keyClass = keyClass;</span><br><span class="line">    <span class="keyword">this</span>.conf = conf != <span class="keyword">null</span> ? conf : <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 由此可见，如果 OrderGroupingComparator 的构造参数，在调用父类的构造时，</span></span><br><span class="line">    <span class="comment">// 如果传入的是 false，或者不传入第二个参数，则 key1、buffer 均为 null，此时可能出现空指针异常。</span></span><br><span class="line">    <span class="keyword">if</span> (createInstances) &#123;</span><br><span class="line">        <span class="keyword">this</span>.key1 = <span class="keyword">this</span>.newKey();</span><br><span class="line">        <span class="keyword">this</span>.key2 = <span class="keyword">this</span>.newKey();</span><br><span class="line">        <span class="keyword">this</span>.buffer = <span class="keyword">new</span> DataInputBuffer();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">this</span>.key1 = <span class="keyword">this</span>.key2 = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">this</span>.buffer = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Driver</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setGroupingComparatorClass(OrderGroupingComparator.class);</span><br></pre></td></tr></table></figure><blockquote><p>运行测试</p></blockquote><p><img src="/images/hadoop/shuffle/grouping-result.png" alt="grouping result"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;排序，是 MapReduce 中最重要的操作之一。默认的排序方式是 &lt;code&gt;字典排序&lt;/code&gt;，且实现此排序的方式是 &lt;code&gt;快速排序&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;MapTask 和 MapReduce 均会对数据按照 &lt;code&gt;key&lt;/code&gt; 进行排序，该操作属于 Hadoop 的默认操作。任何程序中的数据均会被排序，不论逻辑上是否需要。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
      <category term="sort" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/sort/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
      <category term="sort" scheme="https://www.laiyy.top/tags/sort/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（13） Map Reduce &lt;BR /&gt; 工作流程、Shuffle</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/shuffle/hadoop-13.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/shuffle/hadoop-13.html</id>
    <published>2019-12-05T07:46:39.000Z</published>
    <updated>2019-12-05T07:46:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>在了解了常见的 InputFormat，及其处理分片的方式后，通过集成 FileInputFormat 自实现了一个自定义的 InputFormat，并通过自实现的 InputFormat，完成了一个对小文件的汇总合并工作。</p><p>那么此时，就需要深入了解一下 MapReduce 的具体工作流程</p><a id="more"></a><h1 id="MapReduce-工作流程"><a href="#MapReduce-工作流程" class="headerlink" title="MapReduce 工作流程"></a>MapReduce 工作流程</h1><ol><li>准备待处理文件</li><li>客户端进行 submit 之前，获取待处理数据的信息，根据参数配置，形成一份任务分配的规划（即 切片信息）</li><li>提交切片信息(job.split) 和 jar(集群模式下提交)、Job.xml</li><li>计算 MapTask 的数量（Yarn 中会先创建一个 MrAppMaster，根据 job.split 决定分配 MapTask 的数量）</li><li>执行 InputFormat 的 initialize 方法，获取文件、分片信息</li><li>执行 Mapper 操作</li><li>向环形缓冲区(默认 100M)中写入 KV 数据（日志中会打印 0%、50% 等信息）；缓冲区右侧是数据，左侧是数据的元数据（索引、位置、k-v 的起始位置等）。当缓冲区达到 80% 时，数据溢写到磁盘，且左右两侧数据清空，并反向写入数据。</li><li>进行分区、排序</li><li>溢出到文件（分区、且区内有序）</li><li>归并排序并合并文件（Reducer）</li><li>所有的 MapTask 任务完成后，启动 ReduceTask（数量由 MapTask 分区数量决定），并告知 ReduceTask 处理数据的范围（数据分区）</li><li>将 MapTask 处理后的数据下载到 ReduceTask 本地磁盘</li><li>将 ReduceTask 的文件进行归并排序，合并为一个文件后，进行 Reduce 操作</li><li>通过 OutputFormat 写出文件</li></ol><p><img src="/images/hadoop/map-reduce/mr-work-1.png" alt="MapReduce 工作流程"><br><img src="/images/hadoop/map-reduce/mr-work-2.png" alt="MapReduce 工作流程"></p><hr><h1 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h1><p>Map 方法之后，Reduce 方法之前的数据处理，称之为 Shuffle。此操作涉及：分区、排序、归并排序、数据压缩等。</p><p><img src="/images/hadoop/shuffle/shuffle.png" alt="shuffle"></p><h2 id="Partition-分区"><a href="#Partition-分区" class="headerlink" title="Partition 分区"></a>Partition 分区</h2><p>分区：将统计的结果，按照不同的条件，输出到不同的文件中。默认分区实现：<code>HashPartitioner</code>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由源码可知，默认的分区是根据 key 的HashCode 对 ReduceTasks 个数取模得到的。用户没有办法控制哪个 key 存储到哪个分区。</p><p>修改 WordCount 实例，增加配置： <code>job.setNumReduceTasks(2);</code>，再次运行。当 Mapper 进入 <code>context.write(key, value);</code> 时，将进行分区操作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// MapTask.java</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// collector 收集器；此处调用的是 MapOutputBuffer 的 collect 方法</span></span><br><span class="line">    collector.collect(key, value,</span><br><span class="line">                    <span class="comment">// 此处调用的 Partitioner 即为 HashPartitioner</span></span><br><span class="line">                    partitioner.getPartition(key, value, partitions));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行后查看输出目录，发现有 2 个文件，证明分区成功。<br><img src="/images/hadoop/shuffle/2-split.png" alt="2 split"><br><img src="/images/hadoop/shuffle/2-split-result.png" alt="2 split"></p><h2 id="自定义分区"><a href="#自定义分区" class="headerlink" title="自定义分区"></a>自定义分区</h2><p>实现步骤：</p><ol><li>自定义类，继承 <code>Partitioner</code>，重写 <code>getPartition</code> 方法</li><li>在 Job 驱动中，设置自定义的 Partitioner</li><li>根据自定义的 Partitioner 逻辑，设置相应数量的 ReduceTask</li></ol><p>需求：使用之前 <a href="/hadoop/map-reduce/hadoop-10.html#序列化-Demo">序列化实例</a> 的输入数据，实现 <code>按照手机号归属地不同，输出的不同的文件中</code>。</p><p>期望输出：如果手机号以 <code>偶数</code> 结尾，输入的一个文件，否则输出到不同文件。 根据文件内容，手机号以 <code>0、3、5、7、8</code> 结尾，则 0、8 输出到一个文件，其余的每个手机号一个文件。</p><hr><p><strong><em>注意：在使用自定义的 Partitioner 时，必须要指定 ReduceTask 的数量（setNumReduceTasks），否则只会输出一个文件，且所有数据都在这一个文件中！</em></strong><br><strong><em>如果指定的 ReduceTask 数量，小于 Partitioner 中的数量，则会出现 IO 异常，原因：无法确定输出结果用哪个 ReduceTask 输出。</em></strong><br><strong><em>如果指定的 ReduceTask 数量，大于 Partitioner 中的数量，不会报错，但是会出现几个空文件</em></strong><br><strong><em>分区号必须从0开始，逐一累加</em></strong></p><hr><blockquote><p>在之前<code>序列化实例</code>的基础上，进行修改</p></blockquote><ol><li>自定义 Partitioner</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBanPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// 注意：getPartition 只能从 0 开始。</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 如果手机号以 `偶数` 结尾，输入的一个文件，否则输出到不同文件</span></span><br><span class="line">        String phone = text.toString();</span><br><span class="line">        String key = phone.substring(<span class="number">9</span>, <span class="number">12</span>);</span><br><span class="line">        <span class="keyword">if</span> (key.equals(<span class="string">"885"</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (key.equals(<span class="string">"889"</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (key.equals(<span class="string">"883"</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (key.equals(<span class="string">"887"</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>修改 Driver</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改 Partitioner</span></span><br><span class="line">job.setPartitionerClass(FlowBeanPartitioner.class);</span><br><span class="line"><span class="comment">// 根据 Partitioner，设置 ReduceTask</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><ol><li>测试运行</li></ol><p><img src="/images/hadoop/shuffle/customer-split.png" alt="自定义分区"><br><img src="/images/hadoop/shuffle/customer-split-result.png" alt="自定义分区"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在了解了常见的 InputFormat，及其处理分片的方式后，通过集成 FileInputFormat 自实现了一个自定义的 InputFormat，并通过自实现的 InputFormat，完成了一个对小文件的汇总合并工作。&lt;/p&gt;
&lt;p&gt;那么此时，就需要深入了解一下 MapReduce 的具体工作流程&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
      <category term="shuffle" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/shuffle/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
      <category term="shuffle" scheme="https://www.laiyy.top/tags/shuffle/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（12） Map Reduce &lt;BR /&gt; MapReduce 框架原理：InputFormat（二）</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/input-format/hadoop-12.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/input-format/hadoop-12.html</id>
    <published>2019-12-05T02:29:37.000Z</published>
    <updated>2019-12-05T02:29:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>此前了解了 InputFormat 运行时，需要参考的 MapTask 并行度决定机制，以及任务提交的流程，那么接下来就需要深入分析 InputFormat 机制。</p><a id="more"></a><h1 id="FileInputFormat-切片机制"><a href="#FileInputFormat-切片机制" class="headerlink" title="FileInputFormat 切片机制"></a>FileInputFormat 切片机制</h1><h2 id="切片机制"><a href="#切片机制" class="headerlink" title="切片机制"></a>切片机制</h2><ol><li>简单的按照文件的内容长度进行切片</li><li>切片大小默认为 Block 大小</li><li>切片时不考虑数据集整体，而是针对每一个文件单独切片</li></ol><h2 id="源码中计算切片大小"><a href="#源码中计算切片大小" class="headerlink" title="源码中计算切片大小"></a>源码中计算切片大小</h2><p>Math.max(minSize, Math.min(maxSize, blockSize));</p><p>mapreduce.input.fileinputformat.split.minsize=1<br>mapreduce.input.fileinputformat.split.maxsize=Long.MAX_VALUE</p><p>基于此，默认情况下，切片大小 等于 blockSize</p><h2 id="切片大小的设置"><a href="#切片大小的设置" class="headerlink" title="切片大小的设置"></a>切片大小的设置</h2><p>maxsize：切片最大值，参数如果调得比 blockSize 小，则会让切片变小，而且就等于配置的这个参数的值。<br>minsize：切片最小值，如果调的比 blockSize 大，则可以让切片变得比 blockSize 还大</p><h2 id="获取切片信息的-API"><a href="#获取切片信息的-API" class="headerlink" title="获取切片信息的 API"></a>获取切片信息的 API</h2><p>inputSplit.gePath.getName()：获取切片的文件名称<br>(FileSplit)content.getInputSplit(); 根据文件类型获取切片信息</p><hr><h1 id="CombineTextInputFormat-切片机制"><a href="#CombineTextInputFormat-切片机制" class="headerlink" title="CombineTextInputFormat 切片机制"></a>CombineTextInputFormat 切片机制</h1><p>Hadoop 默认的 TextInputFormat 切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个 MapTask，这样如果有大量小文件，就会产生大量的 MapTask，处理效率极其低下。</p><p>CombineTextInputFormat 用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样多个小文件就可以交给一个 MapTask 处理。</p><h2 id="最大值设置"><a href="#最大值设置" class="headerlink" title="最大值设置"></a>最大值设置</h2><p>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); // 4 M</p><p>这个最大值的设置最好按照实际的小文件大小情况来设置。</p><h2 id="切片机制-1"><a href="#切片机制-1" class="headerlink" title="切片机制"></a>切片机制</h2><p>生成切片的过程包括：虚拟存储过程、切片过程 两步。</p><p>如：虚拟切片最大值为 4 M，现在有 4 个文件，分别为：a.txt(1.7M)、b.txt(5.1M)、c.txt(3.4M)、d.txt(6.8M)。</p><p>虚拟存储过程：</p><blockquote><p>a：1.7 &lt; 4，划分为 1 块（1.7M）<br>b：4 &lt; 5.1 &lt; 2<em>4；则划分为 2 块（2.55M，2.55M）<br>c：3.4 &lt; 4：划分为 1块（3.4M）<br>d：4 &lt; 6.8 &lt; 2</em>4：划分为 2块（3.4M，3.4M）</p></blockquote><p>切片过程：</p><blockquote><p>如果虚拟存储的文件大小大于设置好的最大值，则单独形成一个切片<br>否则跟下一个虚拟存储文件进行合并，共同形成一个切片</p></blockquote><p>所以最终会形成 3 个切片：(1.7 + 2.55)M、(2.55 + 3.4)M、(3.4 + 3.4)M</p><h2 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h2><p>准备 4 个测试文件：<a href="/file/hadoop/input-format/a.txt">a.txt(1.7M)</a>、<a href="/file/hadoop/input-format/b.txt">b.txt(5.1M)</a>、<a href="/file/hadoop/input-format/c.txt">c.txt(3.4M)</a>、<a href="/file/hadoop/input-format/d.txt">d.txt(6.8M)</a>。</p><p>期望：一个切片，处理4个文件。</p><h3 id="默认处理"><a href="#默认处理" class="headerlink" title="默认处理"></a>默认处理</h3><p>利用这个 4 个文件，运行 WordCount 实例，查看切片个数。</p><p><img src="/images/hadoop/map-reduce/default-split.png" alt="default split"></p><h3 id="设置虚拟存储"><a href="#设置虚拟存储" class="headerlink" title="设置虚拟存储"></a>设置虚拟存储</h3><blockquote><p>设置虚拟存储切片最大值为 4M</p></blockquote><p>在任务提交之前，增加配置：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置任务使用虚拟存储切片</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置虚拟存储切片最大值为 4 M</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br><span class="line"><span class="comment">// 提交 job</span></span><br><span class="line"><span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure></p><p><img src="/images/hadoop/map-reduce/3-split.png" alt="三片"></p><blockquote><p>设置虚拟存储切片最大值为 20M</p></blockquote><p>在任务提交之前，增加配置：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置任务使用虚拟存储切片</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置虚拟存储切片最大值为 20 M</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">20971520</span>);</span><br><span class="line"><span class="comment">// 提交 job</span></span><br><span class="line"><span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure></p><p><img src="/images/hadoop/map-reduce/1-split.png" alt="一片"></p><hr><h1 id="FileInputFormat-实现类"><a href="#FileInputFormat-实现类" class="headerlink" title="FileInputFormat 实现类"></a>FileInputFormat 实现类</h1><p>在运行 MapReduce 程序时，输入的文件包括：基于行的日志文件、二进制格式文件、数据库表等 。针对不同的数据类型，MapReduce 如何读取数据？</p><p>FileInputFormat 常见实现：TextInputFormat(文本文件)，KeyValueTextInputFormat（基于 KV 的文本文件），NLineInputFormat（按行处理）、CombineTextInputFormat（小文件处理）、自定义 InputFormat。</p><p><img src="/images/hadoop/map-reduce/first-sub-class.png" alt="实现类"></p><h2 id="TextInputFormat"><a href="#TextInputFormat" class="headerlink" title="TextInputFormat"></a>TextInputFormat</h2><p>TextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。key 是该行在整个文件中的字节偏移量（LongWritable 类型）；value 是读取到的该行内容（不包括任何终止符，Text 类型）。</p><h2 id="KeyValueTextInputFormat"><a href="#KeyValueTextInputFormat" class="headerlink" title="KeyValueTextInputFormat"></a>KeyValueTextInputFormat</h2><p>每一行为一条记录，被分隔符分隔为 key、value。可以通过 <code>configuration.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, &quot;\t&quot;);</code> 来设定分隔符，默认分隔符是 <code>\t</code>。<br>此时的 key 是每行排在分隔符之前的 Text 序列。</p><h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><p>需求：统计输入文件中，每一行的第一个单词相同的行数。参考文件：<a href="/file/hadoop/input-format/key-value.txt">key-value.txt</a></p><p>如：输入数据格式为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">laiyy#dahe.cn like</span><br><span class="line">liyl#dahe.cn like</span><br><span class="line">laiyy#sina.com.cn hate</span><br><span class="line">laiyy#study hadoop</span><br><span class="line">liyl#study hadoop</span><br></pre></td></tr></table></figure></p><p>期望输出为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">laiyy 3</span><br><span class="line">liyl 2</span><br></pre></td></tr></table></figure></p><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"当前行的 key ："</span> + key + <span class="string">" ---&gt; 当前行的 value："</span> + value);</span><br><span class="line">    context.write(key, outValue);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> IntWritable outValue = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">        sum += value.get();</span><br><span class="line">    &#125;</span><br><span class="line">    outValue.set(sum);</span><br><span class="line">    context.write(key, outValue);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Driver</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取 job 对象</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置分隔符</span></span><br><span class="line">    configuration.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, <span class="string">"#"</span>);</span><br><span class="line"></span><br><span class="line">    Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置使用 KeyValue 形式的 InputFormat</span></span><br><span class="line">    job.setInputFormatClass(KeyValueTextInputFormat.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>检查输出结果</p></blockquote><p><img src="/images/hadoop/map-reduce/kv-result.png" alt="kv 输出结果"></p><h2 id="NLineInputFormat"><a href="#NLineInputFormat" class="headerlink" title="NLineInputFormat"></a>NLineInputFormat</h2><p>如果使用 NLineInputFormat，代表每个 map 进程处理的 InputSplit 不再按照 Block 去划分，而是按照 NLineInputFormat 指定的行数来划分。<br>即：<code>输入文件的总行数/N=切片数</code>。如果不能整除，切片数为 <code>商+1</code>。</p><p>如：使用 hadoop 的 <a href="/file/hadoop/map-reduce/README.txt">README.txt</a> 文件作为输入，如果 N 为 5，则每个输入分片包括 5 行；文件总共 32 行，则应该有 7 个分片。</p><h3 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h3><p>使用 WordCount 实例作为测试代码，对每个单词进行个数统计。根据输入文件的行数来规定输出几个切片。此案例要求每 5 行放入一个切片。</p><p>只需要在 WordCount 的 Driver 中，Job 提交之前，加入下列代码即可。Mapper、Reducer 都不需要变动。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置为 NLineInputFormat</span></span><br><span class="line">job.setInputFormatClass(NLineInputFormat.class);</span><br><span class="line"><span class="comment">// 5 行一个切片</span></span><br><span class="line">NLineInputFormat.setNumLinesPerSplit(job, <span class="number">5</span>);</span><br></pre></td></tr></table></figure></p><p><img src="/images/hadoop/map-reduce/nline.png" alt="NLineInputFormat split"></p><hr><h1 id="自定义-InputFormat"><a href="#自定义-InputFormat" class="headerlink" title="自定义 InputFormat"></a>自定义 InputFormat</h1><p>步骤：</p><blockquote><p>自定义一个类，集成 FileInputFormat<br>改写 RecordReader，实现一次读取一个完整文件，封装为 KV<br>在输出的时候，使用 SequenceFileOutputFormat 输出合并文件</p></blockquote><p>无论是 HDFS 还是 MapReduce，在处理小文件时，效率都非常低，但是又难免面临大量小文件的场景。此时，可以使用自定义 InputFormat 实现小文件的合并。</p><blockquote><p>需求</p></blockquote><p>将多个小文件，合并为一个 SequenceFile 文件（Hadoop 用来存储二进制形式的 k-v 对的文件格式），SequenceFile 中存储着多着文件，存储形式为 <code>文件路径 + 名称 为 key，内容为 value</code></p><p>准备三个小文件：<a href="/file/hadoop/input-format/sf_1.txt">sf_1.txt</a>，<a href="/file/hadoop/input-format/sf_2.txt">sf_2.txt</a>，<a href="/file/hadoop/input-format/sf_3.txt">sf_3.txt</a></p><h2 id="自定义-InputFormat-1"><a href="#自定义-InputFormat-1" class="headerlink" title="自定义 InputFormat"></a>自定义 InputFormat</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Text：key，存储的是文件路径+名称</span></span><br><span class="line"><span class="comment"> * BytesWritable：value，存储的是整个文件的字节流</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerInputFormat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordReader&lt;Text, BytesWritable&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        CustomerRecordReader recordReader = <span class="keyword">new</span> CustomerRecordReader();</span><br><span class="line">        <span class="comment">// 初始化</span></span><br><span class="line">        recordReader.initialize(split, context);</span><br><span class="line">        <span class="keyword">return</span> recordReader;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="实现-RecordReader"><a href="#实现-RecordReader" class="headerlink" title="实现 RecordReader"></a>实现 RecordReader</h2><blockquote><p>自定义实现 RecordReader，实现一次读取一个完整文件，封装为 KV</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 切片</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> FileSplit fileSplit;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 配置信息</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Configuration configuration;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 输出的 key（路径 + 名称）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> Text key = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 输出的 value（整个文件内容）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> BytesWritable value = <span class="keyword">new</span> BytesWritable();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 标识是否正在读取</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> progressing = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> split 切片</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context 上下文</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.fileSplit = (FileSplit) split;</span><br><span class="line">        configuration = context.getConfiguration();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 核心业务逻辑</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (progressing) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取 fs 对象</span></span><br><span class="line">            Path path = fileSplit.getPath();</span><br><span class="line">            FileSystem fileSystem = path.getFileSystem(configuration);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取输入流</span></span><br><span class="line">            FSDataInputStream inputStream = fileSystem.open(path);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 封装 key</span></span><br><span class="line">            key.set(path.toString());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 拷贝，将文件内容拷贝到 buffer 中</span></span><br><span class="line">            <span class="keyword">byte</span>[] buffer = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>) fileSplit.getLength()];</span><br><span class="line">            IOUtils.readFully(inputStream, buffer, <span class="number">0</span>, buffer.length);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 封装 value</span></span><br><span class="line">            value.set(buffer, <span class="number">0</span>, buffer.length);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 关闭资源</span></span><br><span class="line">            IOUtils.closeStream(inputStream);</span><br><span class="line"></span><br><span class="line">            progressing = <span class="keyword">false</span>;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取当前的 key</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> key;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> BytesWritable <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取处理进度</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 关闭资源</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><blockquote><p>Mapper</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, BytesWritable value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(key, value);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Reducer</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;BytesWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 循环写出</span></span><br><span class="line">        <span class="keyword">for</span> (BytesWritable value : values) &#123;</span><br><span class="line">            context.write(key, value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Driver</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 设置 InputFormat、OutputFormat</span></span><br><span class="line">    job.setInputFormatClass(CustomerInputFormat.class);</span><br><span class="line">    job.setOutputFormatClass(SequenceFileOutputFormat.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="执行结果"><a href="#执行结果" class="headerlink" title="执行结果"></a>执行结果</h2><p><img src="/images/hadoop/map-reduce/sf-result.png" alt="执行结果"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此前了解了 InputFormat 运行时，需要参考的 MapTask 并行度决定机制，以及任务提交的流程，那么接下来就需要深入分析 InputFormat 机制。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
      <category term="input-format" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/input-format/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
      <category term="input-format" scheme="https://www.laiyy.top/tags/input-format/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（11） Map Reduce &lt;BR /&gt; MapReduce 框架原理：InputFormat（一）&lt;BR/&gt; MapTask 并行度决定机制</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/input-format/hadoop-11.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/input-format/hadoop-11.html</id>
    <published>2019-12-04T08:17:08.000Z</published>
    <updated>2019-12-04T08:17:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>在了解了 Hadoop 的序列化操作，实现了基本的 Bean 序列化的一个 demo，接下来分析一下 MapReduce 的框架原理。</p><a id="more"></a><h1 id="切片与MapTask-并行度决定机制"><a href="#切片与MapTask-并行度决定机制" class="headerlink" title="切片与MapTask 并行度决定机制"></a>切片与MapTask 并行度决定机制</h1><p>MapTask 的并行度决定 Map 阶段的任务处理并发度，进而影响整个 Job 的处理速度。</p><p>问题：</p><blockquote><p>一个 1G 的数据，启动 8 个MapTask，可以提高集群的并发处理能力。但是如果是一个 1K 的数据，也启动 8 个MapTask，会提高性能吗？<br>MapTask 是否越多越好？<br>什么因素会影响到 MapTask 的并行度？</p></blockquote><hr><h1 id="MapTask并行度决定机制"><a href="#MapTask并行度决定机制" class="headerlink" title="MapTask并行度决定机制"></a>MapTask并行度决定机制</h1><p>前置概念：</p><blockquote><p>数据块：Block 在 HDFS 物理上把数据分成一块一块的。<br>数据切片：在逻辑上对输入进行分片，并不会在磁盘上将其分片存储。</p></blockquote><p>现在，假设有一个 300M 的数据，分别存放在 DataNode 1、2、3 上，DataNode1 上存储 0~128M 数据，DataNode2 上存储 128~256M 数据，DataNode3 上存储 256~300M 数据。<br>如果数据切片大小为 100M，则读取第一个切片没有问题，当读取第2、3个切片时，需要将DataNode1 上的剩余的数据拷贝到 MapTask2 上，将 DataNode2 上剩余的数据拷贝到 MapTask3 上，这样会存在大量的数据 IO，严重影响性能。</p><p><img src="/images/hadoop/map-reduce/100-split.png" alt="切片大小为 100M"></p><p>如果数据切片大小为 128M（即与 Block 大小一致），此时，每个 MapTask 都读取 128M 数据，则可以分别运行在三台 DataNode 上，没有数据拷贝，此时性能最高。</p><blockquote><p>MapTask 并行度决定机制</p></blockquote><ol><li>一个 Job 的 Map 阶段并行度由客户端在提交 Job 时的切片数决定</li><li>每个切片分配一个 MapTask 并行实例处理</li><li>默认情况下，切片大小等于 BlockSize</li><li>切片时不考虑数据集整体，而是逐个针对每个文件单独切片</li></ol><hr><h1 id="Job-提交流程、切片源码"><a href="#Job-提交流程、切片源码" class="headerlink" title="Job 提交流程、切片源码"></a>Job 提交流程、切片源码</h1><p>在 Job 调用 <code>job.waitForCompletion</code> 时，进行任务提交。此方法会调用 <code>submit()</code> 方法进行真正的提交。</p><h2 id="任务提交流程"><a href="#任务提交流程" class="headerlink" title="任务提交流程"></a>任务提交流程</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">waitForCompletion</span><span class="params">(<span class="keyword">boolean</span> verbose</span></span></span><br><span class="line"><span class="function"><span class="params">                                )</span> <span class="keyword">throws</span> IOException, InterruptedException,</span></span><br><span class="line"><span class="function">                                        ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">        <span class="comment">// 调用真正的提交</span></span><br><span class="line">        submit();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (verbose) &#123;</span><br><span class="line">        <span class="comment">// 打印日志</span></span><br><span class="line">        monitorAndPrintJob();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 忽略</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> isSuccessful();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">submit</span><span class="params">()</span> </span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="comment">// 判断任务状态</span></span><br><span class="line">    ensureState(JobState.DEFINE);</span><br><span class="line">    <span class="comment">// 将老旧的 API 转换为新的 API，为兼容性考虑</span></span><br><span class="line">    setUseNewAPI();</span><br><span class="line">    <span class="comment">// 连接</span></span><br><span class="line">    connect();</span><br><span class="line">    <span class="keyword">final</span> JobSubmitter submitter = </span><br><span class="line">        getJobSubmitter(cluster.getFileSystem(), cluster.getClient());</span><br><span class="line">    status = ugi.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> JobStatus <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, </span></span><br><span class="line"><span class="function">        ClassNotFoundException </span>&#123;</span><br><span class="line">            <span class="comment">// 提交任务的详细信息</span></span><br><span class="line">            <span class="keyword">return</span> submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    state = JobState.RUNNING;</span><br><span class="line">    LOG.info(<span class="string">"The url to track the job: "</span> + getTrackingURL());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="connect-连接流程"><a href="#connect-连接流程" class="headerlink" title="connect 连接流程"></a>connect 连接流程</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">connect</span><span class="params">()</span></span></span><br><span class="line"><span class="function">          <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (cluster == <span class="keyword">null</span>) &#123;</span><br><span class="line">      cluster = </span><br><span class="line">        ugi.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;Cluster&gt;() &#123;</span><br><span class="line">                   <span class="function"><span class="keyword">public</span> Cluster <span class="title">run</span><span class="params">()</span></span></span><br><span class="line"><span class="function">                          <span class="keyword">throws</span> IOException, InterruptedException, </span></span><br><span class="line"><span class="function">                                 ClassNotFoundException </span>&#123;</span><br><span class="line">                     <span class="comment">// 创建一个新的 Cluster</span></span><br><span class="line">                     <span class="keyword">return</span> <span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">                   &#125;</span><br><span class="line">                 &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Cluster</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="keyword">null</span>, conf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Cluster</span><span class="params">(InetSocketAddress jobTrackAddr, Configuration conf)</span> </span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.conf = conf;</span><br><span class="line">    <span class="keyword">this</span>.ugi = UserGroupInformation.getCurrentUser();</span><br><span class="line">    <span class="comment">// Cluster 初始化</span></span><br><span class="line">    initialize(jobTrackAddr, conf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InetSocketAddress jobTrackAddr, Configuration conf)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">synchronized</span> (frameworkLoader) &#123;</span><br><span class="line">        <span class="keyword">for</span> (ClientProtocolProvider provider : frameworkLoader) &#123;</span><br><span class="line">        ClientProtocol clientProtocol = <span class="keyword">null</span>; </span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (jobTrackAddr == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// 创建一个 LocalJobRunner（在本地运行时）</span></span><br><span class="line">                clientProtocol = provider.create(conf);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 创建一个 YARNRunner（在集群运行时）</span></span><br><span class="line">                clientProtocol = provider.create(jobTrackAddr, conf);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 省略</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="实际提交流程"><a href="#实际提交流程" class="headerlink" title="实际提交流程"></a>实际提交流程</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">JobStatus <span class="title">submitJobInternal</span><span class="params">(Job job, Cluster cluster)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> ClassNotFoundException, InterruptedException, IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 校验输出路径</span></span><br><span class="line">    checkSpecs(job);</span><br><span class="line"></span><br><span class="line">    Configuration conf = job.getConfiguration();</span><br><span class="line">    <span class="comment">// 缓存处理</span></span><br><span class="line">    addMRFrameworkToDistributedCache(conf);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 任务临时目录， tmp/hadoop-Administrator\mapred\staging，每次运行任务都会在这个目录下创建一个文件夹，将所需数据都保存在内</span></span><br><span class="line">    <span class="comment">// 当任务执行结束后，会删除文件夹内的所有数据</span></span><br><span class="line">    Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 获取网络 ip</span></span><br><span class="line">    InetAddress ip = InetAddress.getLocalHost();</span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成一个唯一的 jobId</span></span><br><span class="line">    JobID jobId = submitClient.getNewJobID();</span><br><span class="line">    job.setJobID(jobId);</span><br><span class="line">    Path submitJobDir = <span class="keyword">new</span> Path(jobStagingArea, jobId.toString());</span><br><span class="line">    JobStatus status = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交文件的信息到之前创建的文件夹下（本机下不会提交）</span></span><br><span class="line">        copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line"></span><br><span class="line">        Path submitJobFile = JobSubmissionFiles.getJobConfPath(submitJobDir);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 写入切片信息到文件夹</span></span><br><span class="line">        <span class="keyword">int</span> maps = writeSplits(job, submitJobDir);</span><br><span class="line">        conf.setInt(MRJobConfig.NUM_MAPS, maps);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写入任务信息到文件夹</span></span><br><span class="line">        writeConf(conf, submitJobFile);</span><br><span class="line">        </span><br><span class="line">        printTokens(jobId, job.getCredentials());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交完成后，删除文件夹内信息</span></span><br><span class="line">        status = submitClient.submitJob(</span><br><span class="line">            jobId, submitJobDir.toString(), job.getCredentials());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 省略</span></span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="comment">// 省略</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/images/hadoop/map-reduce/job-staging.png" alt="Hadoop 任务临时路径"><br><img src="/images/hadoop/map-reduce/split-file.png" alt="hadoop 临时切片文件"><br><img src="/images/hadoop/map-reduce/job-submit.png" alt="hadoop Job 提交流程"></p><h2 id="切片流程"><a href="#切片流程" class="headerlink" title="切片流程"></a>切片流程</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">writeSplits</span><span class="params">(org.apache.hadoop.mapreduce.JobContext job,</span></span></span><br><span class="line"><span class="function"><span class="params">    Path jobSubmitDir)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">    JobConf jConf = (JobConf)job.getConfiguration();</span><br><span class="line">    <span class="keyword">int</span> maps;</span><br><span class="line">    <span class="keyword">if</span> (jConf.getUseNewMapper()) &#123;</span><br><span class="line">        <span class="comment">// 使用新的切片规则</span></span><br><span class="line">        maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 使用旧切片规则</span></span><br><span class="line">        maps = writeOldSplits(jConf, jobSubmitDir);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> maps;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;T extends InputSplit&gt; <span class="function"><span class="keyword">int</span> <span class="title">writeNewSplits</span><span class="params">(JobContext job, Path jobSubmitDir)</span> <span class="keyword">throws</span> IOException,</span></span><br><span class="line"><span class="function">        InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取配置信息</span></span><br><span class="line">    Configuration conf = job.getConfiguration();</span><br><span class="line">    InputFormat&lt;?, ?&gt; input =</span><br><span class="line">        ReflectionUtils.newInstance(job.getInputFormatClass(), conf);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取切片信息</span></span><br><span class="line">    List&lt;InputSplit&gt; splits = input.getSplits(job);</span><br><span class="line">    T[] array = (T[]) splits.toArray(<span class="keyword">new</span> InputSplit[splits.size()]);</span><br><span class="line"></span><br><span class="line">    Arrays.sort(array, <span class="keyword">new</span> SplitComparator());</span><br><span class="line">    JobSplitWriter.createSplitFiles(jobSubmitDir, conf, </span><br><span class="line">        jobSubmitDir.getFileSystem(conf), array);</span><br><span class="line">    <span class="keyword">return</span> array.length;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 此处调用的是 FileInputFormat 中的 getSplits</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    StopWatch sw = <span class="keyword">new</span> StopWatch().start();</span><br><span class="line">    <span class="keyword">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">    <span class="keyword">long</span> maxSize = getMaxSplitSize(job);</span><br><span class="line"></span><br><span class="line">    List&lt;InputSplit&gt; splits = <span class="keyword">new</span> ArrayList&lt;InputSplit&gt;();</span><br><span class="line">    <span class="comment">// 文件信息</span></span><br><span class="line">    List&lt;FileStatus&gt; files = listStatus(job);</span><br><span class="line">    <span class="comment">// 按照文件，一个一个切片</span></span><br><span class="line">    <span class="keyword">for</span> (FileStatus file: files) &#123;</span><br><span class="line">        Path path = file.getPath();</span><br><span class="line">        <span class="keyword">long</span> length = file.getLen();</span><br><span class="line">        <span class="keyword">if</span> (length != <span class="number">0</span>) &#123;</span><br><span class="line">        BlockLocation[] blkLocations;</span><br><span class="line">        <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span><br><span class="line">            blkLocations = ((LocatedFileStatus) file).getBlockLocations();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            FileSystem fs = path.getFileSystem(job.getConfiguration());</span><br><span class="line">            blkLocations = fs.getFileBlockLocations(file, <span class="number">0</span>, length);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 判断是否可切割</span></span><br><span class="line">        <span class="keyword">if</span> (isSplitable(job, path)) &#123;</span><br><span class="line">            <span class="comment">// 获取块大小（如果是 local 运行：2.x 32 M，1.x 64 M，yarn 集群：128M，）</span></span><br><span class="line">            <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">            <span class="comment">// 获取切片大小</span></span><br><span class="line">            <span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">long</span> bytesRemaining = length;</span><br><span class="line">            <span class="comment">// 如果当前文件大小 / 切片大小 &gt; 1.1，进入此方法进行切片</span></span><br><span class="line">            <span class="keyword">while</span> (((<span class="keyword">double</span>) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class="line">                <span class="comment">// 重新计算切片开始位置</span></span><br><span class="line">                <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class="line">                <span class="comment">// 添加切片</span></span><br><span class="line">                splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span><br><span class="line">                            blkLocations[blkIndex].getHosts(),</span><br><span class="line">                            blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">                bytesRemaining -= splitSize;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);</span><br><span class="line">                <span class="comment">// 添加切片</span></span><br><span class="line">                splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,</span><br><span class="line">                            blkLocations[blkIndex].getHosts(),</span><br><span class="line">                            blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">// not splitable</span></span><br><span class="line">            splits.add(makeSplit(path, <span class="number">0</span>, length, blkLocations[<span class="number">0</span>].getHosts(),</span><br><span class="line">                        blkLocations[<span class="number">0</span>].getCachedHosts()));</span><br><span class="line">        &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">            <span class="comment">//Create empty hosts array for zero length files</span></span><br><span class="line">            splits.add(makeSplit(path, <span class="number">0</span>, length, <span class="keyword">new</span> String[<span class="number">0</span>]));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Save the number of input files for metrics/loadgen</span></span><br><span class="line">    job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());</span><br><span class="line">    sw.stop();</span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">        LOG.debug(<span class="string">"Total # of splits generated by getSplits: "</span> + splits.size()</span><br><span class="line">            + <span class="string">", TimeTaken: "</span> + sw.now(TimeUnit.MILLISECONDS));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> splits;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>先创建一个数据存储的临时目录</li><li>开始规划切片，遍历处理目录下的每个文件</li><li>遍历文件：<blockquote><p>获取文件大小<br>计算切片大小，公式： Math.max(minSize, Math.min(maxSize, blockSize))<br>默认情况下，切片大小 = blockSize<br>开始切片：local 运行（第一个切片 0~32M，第二个切片 32~64M …）；Yarn 运行（第一个切片 0~128M，第二个切片 128~256M …）；注意：每次切片时，都需要判断切片完成后剩余部分是否是块大小的 1.1 倍，大于就切片，否则不切<br>将切片信息写入切片规划文件<br>InputSplit 只记录切片的元数据信息（起始位置、长度、所在节点列表等）</p></blockquote></li><li>提交切片规划文件（local 运行时为临时目录，集群运行时为 yarn）；Yarn 上的 MrAppMaster 根据切片规划文件计算开启 MapTask 个数。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在了解了 Hadoop 的序列化操作，实现了基本的 Bean 序列化的一个 demo，接下来分析一下 MapReduce 的框架原理。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
      <category term="input-format" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/input-format/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
      <category term="input-format" scheme="https://www.laiyy.top/tags/input-format/"/>
    
  </entry>
  
  <entry>
    <title>hadoop（10） Map Reduce &lt;BR /&gt;  序列化</title>
    <link href="https://www.laiyy.top/hadoop/map-reduce/hadoop-10.html"/>
    <id>https://www.laiyy.top/hadoop/map-reduce/hadoop-10.html</id>
    <published>2019-12-04T06:17:08.000Z</published>
    <updated>2019-12-04T06:17:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>在 MapReduce 的 <a href="/hadoop/map-reduce/hadoop-9.html#Hadoop-数据序列化类型">数据序列化类型</a> 中，介绍了几种常见的 Hadoop 序列化类，实现了一个基础的 <code>WordCount</code> Demo，使用到了 Long、String、Integer 对应的序列化类，那么接下来就需要了解一下 Hadoop 具体的怎么序列化的。</p><a id="more"></a><h1 id="Hadoop-序列化"><a href="#Hadoop-序列化" class="headerlink" title="Hadoop 序列化"></a>Hadoop 序列化</h1><h2 id="序列化概述"><a href="#序列化概述" class="headerlink" title="序列化概述"></a>序列化概述</h2><blockquote><p>什么是序列化、反序列化</p></blockquote><p>序列化：就是把内存中的对象，转换为<code>字节序列</code> 或其他数据传输协议，以便存储到磁盘或网络传输。<br>反序列化：就是将<code>收到的字节序列</code>或<code>其他数据传输协议</code>或<code>磁盘的持久化数据</code>，转换成内存中的对象。</p><blockquote><p>为什么要序列化？</p></blockquote><p>一般来说，<code>对象</code> 只能生存在内存中，断电即消失。而且，<code>对象</code> 只能由本地进程使用，不能被发送到网络上的另外一台计算机中。<br>然而，<code>序列化</code> 可以存储 <code>对象</code>，且可以将对象 <code>发送到远程计算机</code>。</p><blockquote><p>为什么不用 Java 自身的序列化？</p></blockquote><p>Java 的序列化是一个重量级的框架(Serializable)，一个对象被序列化后，别额外附带很多信息，如：校验信息、Header、继承体系等，不便于在网络中高效传输。基于此，Hadoop 开发了一套属于自己的序列化机制：Writable。</p><blockquote><p>Hadoop 序列化的特点</p></blockquote><ol><li>紧凑：高效使用存储空间</li><li>快速：读写数据的额外开销小</li><li>可扩展：随着通信协议的升级而升级</li><li>互操作：支持多语言交互</li></ol><h2 id="自定义实现序列化"><a href="#自定义实现序列化" class="headerlink" title="自定义实现序列化"></a>自定义实现序列化</h2><p>实现步骤：</p><ol><li>实现 Writable 接口</li><li>反序列化时，需要反射调用空参构造函数</li><li>重写序列化方法</li><li>重写反序列化 方法</li><li>反序列化的顺序和序列化的顺序保持一致</li><li>重写 toString</li><li>实现 Comparable 接口（MapReduce 的 Shuffle 过程要求对 key 必须能排序；当需要排序的时候才做）</li></ol><hr><h1 id="序列化-Demo"><a href="#序列化-Demo" class="headerlink" title="序列化 Demo"></a>序列化 Demo</h1><p>需求：根据 <a href="/file/hadoop/map-reduce/phone.txt">测试文件</a>，统计每个手机号的<code>上行流量</code>、<code>下行流量</code>、<code>总流量</code>。<br>文件中，倒数第三列为上行流量，倒数第二列为下行流量，最后一列为网络请求状态码。</p><h2 id="创建统计流量的-Bean-对象"><a href="#创建统计流量的-Bean-对象" class="headerlink" title="创建统计流量的 Bean 对象"></a>创建统计流量的 Bean 对象</h2><p>创建一个统计流量的 Bean 对象，并实现序列化操作</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 上行流量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 下行流量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 总流量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 空参构造，反射用</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = upFlow + downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">"\t"</span> + downFlow + <span class="string">"\t"</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 省略 get、set</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 序列化</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> dataOutput 输入输出</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException 可能异常</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 反序列化</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> dataInput 输入数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException 可能异常</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">// 必须和序列化方法顺序一致</span></span><br><span class="line">        upFlow = dataInput.readLong();</span><br><span class="line">        downFlow = dataInput.readLong();</span><br><span class="line">        sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="MapReduce-程序"><a href="#MapReduce-程序" class="headerlink" title="MapReduce 程序"></a>MapReduce 程序</h2><h3 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">    <span class="keyword">private</span> Text outKey = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 1   17319758889 192.168.100.1   www.baidu.com 2481    24685   200</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">// 2. 切割</span></span><br><span class="line">        String[] fields = line.split(<span class="string">"\t"</span>);</span><br><span class="line">        <span class="comment">// 3. 封装对象</span></span><br><span class="line">        outKey.set(fields[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> length = fields.length;</span><br><span class="line">        <span class="keyword">long</span> upFlow = Long.parseLong(fields[length - <span class="number">3</span>]);</span><br><span class="line">        <span class="keyword">long</span> downFlow = Long.parseLong(fields[length - <span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">        flowBean.setUpFlow(upFlow);</span><br><span class="line">        flowBean.setDownFlow(downFlow);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 写出</span></span><br><span class="line">        context.write(outKey, flowBean);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Reducer"><a href="#Reducer" class="headerlink" title="Reducer"></a>Reducer</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FlowBean flowBean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 两个相同的手机号的访问记录</span></span><br><span class="line">        <span class="comment">// 11   17319788888 192.168.100.11   www.java1234.com 231    28   200</span></span><br><span class="line">        <span class="comment">// 12   17319788888 192.168.100.12    211    7852   200</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> sumUpFlow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">long</span> sumDownFlow = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FlowBean value : values) &#123;</span><br><span class="line">            sumUpFlow += value.getUpFlow();</span><br><span class="line">            sumDownFlow += value.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        flowBean.set(sumUpFlow, sumDownFlow);</span><br><span class="line">        context.write(key, flowBean);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Deiver"><a href="#Deiver" class="headerlink" title="Deiver"></a>Deiver</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 获取 job 对象</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置 jar 存放路径</span></span><br><span class="line">    job.setJarByClass(FlowCountDriver.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 关联 Mapper、Reducer 业务类</span></span><br><span class="line">    job.setMapperClass(FlowCountMapper.class);</span><br><span class="line">    job.setReducerClass(FlowCountReducer.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 Mapper 输出的 KV 类型</span></span><br><span class="line">    job.setMapOutputKeyClass(Text.class);</span><br><span class="line">    job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定最终输出的数据 KV 类型</span></span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 job 的输入文件所在目录</span></span><br><span class="line">    FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 指定 job 的输出结果所在目录</span></span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 提交 job</span></span><br><span class="line"><span class="comment">//        job.submit();</span></span><br><span class="line">    <span class="keyword">boolean</span> succeed = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    System.exit(succeed ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="运行测试"><a href="#运行测试" class="headerlink" title="运行测试"></a>运行测试</h2><p>设置输入输出路径：<br><img src="/images/hadoop/map-reduce/flow-count-args.png" alt="输入输出路径"></p><p>查看输出结果：<br><img src="/images/hadoop/map-reduce/flow-count-result.png" alt="flow count result"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 MapReduce 的 &lt;a href=&quot;/hadoop/map-reduce/hadoop-9.html#Hadoop-数据序列化类型&quot;&gt;数据序列化类型&lt;/a&gt; 中，介绍了几种常见的 Hadoop 序列化类，实现了一个基础的 &lt;code&gt;WordCount&lt;/code&gt; Demo，使用到了 Long、String、Integer 对应的序列化类，那么接下来就需要了解一下 Hadoop 具体的怎么序列化的。&lt;/p&gt;
    
    </summary>
    
      <category term="hadoop" scheme="https://www.laiyy.top/categories/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/categories/hadoop/map-reduce/"/>
    
    
      <category term="hadoop" scheme="https://www.laiyy.top/tags/hadoop/"/>
    
      <category term="map-reduce" scheme="https://www.laiyy.top/tags/map-reduce/"/>
    
  </entry>
  
</feed>
